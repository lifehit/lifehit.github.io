<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>知识图谱表示学习：评估方法</title>
      <link href="/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-ping-gu-fang-fa/"/>
      <url>/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-ping-gu-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>如何对知识图谱进行评估，其实知识图谱本身并不需要评估，因为其中的实体和关系已经确定，但是考虑到应用时的效率问题等，需要将它们表示成低维向量的形式，因此这种评估是针对表示为低维向量的知识图谱，来探求：低维向量的形式是否能完美代表之前的知识图谱。</p><h1 id="评估数据准备"><a href="#评估数据准备" class="headerlink" title="评估数据准备"></a>评估数据准备</h1><p>那么如何评估这种“完美程度”呢？我们知道在一些tabular数据中使用的模型，比如random forest，GBDT，来一组新的数据，因为有标记，我们预测新数据产生的输出，来与标记进行对比。模型学到的是不同类型数据之间的交互。这种情况下，是通过对新的数据进行prediction来评估模型的学习成果。</p><p>那么在知识图谱表示学习中，模型学到了什么？因为本质上是换了一种表示方法，所以模型学到的是针对同一事务的不同的表示形式，那么就要求在评估时，仍然有同一事务的存在，这么说有点抽象，举个例子：</p><table><thead><tr><th>$x_1$</th><th>$x_2$</th><th>$…$</th><th>$x_{i-1}$</th><th>$x_{i}$</th><th>$y$</th></tr></thead><tbody><tr><td>3</td><td>4</td><td>$…$</td><td>4</td><td>3</td><td>1</td></tr><tr><td>1</td><td>5</td><td>$…$</td><td>9</td><td>0</td><td>0</td></tr><tr><td>2</td><td>1</td><td>$…$</td><td>11</td><td>7</td><td>1</td></tr></tbody></table><p> 假设这是一个tabular数据的预测问题，例如：男生能否脱单。</p><p> 每个维度的向量$x_i$ 代表一种数据，比如：</p><ul><li>$x_1$ 代表你的学历；</li><li>$x_2$ 代表你的家庭背景；</li><li>等等，如此类推…</li></ul><p> 当进行评估时，你所需要的是再出现一个同样格式的数据，每一维的数据的意义和训练集的类似，比如：你不能说在评估时出现一个 $x_{i+1}$ 代表这个人是否会养猪，因为这种类型的数据不在训练集中。</p><p> 同样的，知识图谱表示学习中学到的是一个实体或者关系的低维向量表示，当训练集中出现的是: 高圆圆和Alex, 在评估时，你让我评估 新垣结衣和小约翰(均没有出现在训练集中)的关系。</p><p> 这一点是和之前的评估方法有所区别的。</p><p> 那么如何准备训练集和测试集呢？<strong>在评估数据准备时的一个原则：保证测试集中出现的实体和关系均在训练集中出现。</strong></p><blockquote><p>不同时出现，我评估谁去？谁知道小约翰和新垣结衣合不合适？但我们能评估高圆圆和Alex合不合适… </p></blockquote><h1 id="评估任务"><a href="#评估任务" class="headerlink" title="评估任务"></a>评估任务</h1><p>在KGE评估时，一般采取的是<code>Link prediction</code>任务，在准备时，可以采取的策略：</p><ul><li>在原始的知识图谱上随机删除一些link，然后在评估时预测这些link是否存在，这种情况下，需要计算预测的准确率；</li><li>还有一种方法是，将<code>link prediction</code>视为<code>learn to rank</code>任务，通<strong>过排序的指标进行评估，这种方式目前是主流的方法</strong>，下文主要阐述这种方法；</li></ul><p>对于这种方法，首先确定需要进行评估的数据集 $\mathcal{E}$ ，因为有时候进行评估的数据集可能并不是原始的完整的数据集，然后针对数据集中的三元组，进行两种类型的替换，如下：</p><ul><li><p>$(h, r, ?)$ ：取 $\mathcal{E}$ 中的所有实体，替换？，然后计算 $(h, r, t)$ 在其中的排序，越靠前越好；</p></li><li><p>$(?, r, t)$ ：取 $\mathcal{E}$ 中的所有实体，替换？，然后计算 $(h, r, t)$ 在其中的排序，越靠前越好；</p></li></ul><h1 id="评估指标-Rank-based-method"><a href="#评估指标-Rank-based-method" class="headerlink" title="评估指标(Rank-based method)"></a>评估指标(Rank-based method)</h1><h2 id="Hits-K"><a href="#Hits-K" class="headerlink" title="Hits@K"></a>Hits@K</h2><p>hits@k根据出现在排序列表中的前k个实体中，真实实体排序的位置进行评估。</p><p>具体计算方式如下：</p><p>$$<br>\text{hits@}k = \frac{1}{|\mathcal{I}|} \sum \limits_{r \in \mathcal{I}} \mathbb{I}[r \leq k]<br>$$</p><ul><li>$\mathcal{I}$ 表示一组排序的结果；</li><li>$|\mathcal{I}|$ 表示一组排序的结果的数量；</li><li>$\mathbb{I}$ 为指示函数；</li><li>$k$ 为界定的范围，常为1, 3, 5, 10</li><li>该值越接近1，说明学习的效果越好；</li><li>这里的<code>r</code>指的是<code>rank</code>;</li></ul><p>一个例子说明：</p><p>当有3个正样本用于生成负样本时，同时考虑替换head和tail，可以生成6个负样本，也就可以得到6个排名结果。</p><table><thead><tr><th>正样本</th><th>head负样本</th><th>tail负样本</th><th>head负样本排序</th><th>tail负样本排序</th></tr></thead><tbody><tr><td>$(h_1, r_1, t_1)$</td><td>$(h_1, r_1, ?)$</td><td>$(?, r_1, t_1)$</td><td>8</td><td>4</td></tr><tr><td>$(h_2, r_2, t_2)$</td><td>$(h_2, r_2, ?)$</td><td>$(?, r_2, t_2)$</td><td>2</td><td>1</td></tr><tr><td>$(h_3, r_3, t_3)$</td><td>$(h_3, r_3, ?)$</td><td>$(?, r_3, t_3)$</td><td>90</td><td>1</td></tr></tbody></table><p>则计算如下：</p><p>$$<br>hits@1 = \frac{1}{6} \times (0 + 0 + 0 + 1 + 0 + 1) = 0.3333<br>$$</p><p>$$<br>hits@3 = \frac{1}{6} \times (0 + 0 + 1 + 1 + 0 + 1) = 0.50<br>$$</p><p>$$<br>hits@5 = \frac{1}{6} \times (0 + 1 + 1 + 1 + 0 + 1) = 0.6667<br>$$</p><p>如果你实际计算时会发现一个问题，比如对于 $(h_1, r_1, ?)$，它的排序为8，但是由于比1， 3， 5都大，所以仍然记为0，但是它比 $(h_3, r_3, ?)$ 的排序为90，要好很多，但是对于 $rank &gt; k$ 的，该指标一视同仁，也是一个缺陷。</p><h2 id="Mean-Rank-MR"><a href="#Mean-Rank-MR" class="headerlink" title="Mean Rank(MR)"></a>Mean Rank(MR)</h2><p>正如名字体现的，是triples所有排序的算术平均值，这个值的范围是: [1, 所有负样本的数量]。</p><ul><li>1 表明是最理想的情况，所有的triples的排序都是1；</li><li>所有负样本的数量：最坏的情况，所有的排序都是最后一个；</li><li>这个指标越低越好；</li></ul><p>$$<br>\text{score} =\frac{1}{|\mathcal{I}|} \sum \limits_{r \in \mathcal{I}} r<br>$$</p><p>根据上述的6个负样本，计算如下：</p><p>$$<br>\text{MR} = \frac{1}{6} \times (8 + 4 + 2 + 1 + 90 + 1) = 17.6667<br>$$</p><p>从计算方式上来看，这个评估指标比<code>Hits@K</code>好的地方在于：</p><blockquote><p>it is sensitive to any model performance changes, not only what occurs under a certain cutoff and therefore reflects average performance.</p></blockquote><p>但是它的缺陷也很明显，虽然有一定的解释性，但是由于它的边界取决于负样本的数量，但是MR=10，对于负样本为20或者200000的模型，其性能指示含义完全不同。</p><h2 id="Mean-reciprocal-rank-MRR"><a href="#Mean-reciprocal-rank-MRR" class="headerlink" title="Mean reciprocal rank(MRR)"></a>Mean reciprocal rank(MRR)</h2><p>MRR是所有排名的倒数的算术平均值，具体计算如下：</p><p>$$<br>\text{score} =\frac{1}{|\mathcal{I}|} \sum_{r \in \mathcal{I}} r^{-1}<br>$$</p><p>根据上述的6个负样本，计算如下：</p><p>$$<br>\text{MR} = \frac{1}{6} \times (\frac{1}{8} + \frac{1}{4} + \frac{1}{2} + \frac{1}{1} + \frac{1}{90} + \frac{1}{1}) = 0.4810<br>$$</p><ul><li>这个指标在 $[0, 1]$ 之间;</li><li>是对MR的一种改进，使得可以对异常值不太敏感，这里指对高rank的异常值不敏感，但是去开始对低rank值敏感；</li></ul><p>同样的问题，对于一个模型，如果观察到MRR指标为0.01， 那么这个模型是好还是不好？</p><p>这种结论很难直接下，这个值意味着，当去掉outliers时，平均排名为 100(1/0.01) 左右，这个结果可能是好的，也可能是坏的，这取决于使用的负样本的数量。</p><ul><li>当使用了100万的负样本, 那么这个结果很好，因为在100万中排名100，是很好的；</li><li>当使用了负样本只有100个，说明test triples在跟corruptions一起排名时得到的结果很差；</li></ul><p>在真实的数据集上，应该仔细看看hits@k这个指标，之后再判断模型的好坏。</p><p>其中k的选择，应该取决于针对每个test triple生成的负样本的数量。</p><blockquote><p>其实主要的评估指标，包括：<code>MRR</code>, <code>hits@k</code>, 已经是经常使用的了，例如在：<a href="https://paperswithcode.com/task/link-prediction">paperswithcode</a>和<a href="https://www.jiqizhixin.com/sota/tech-task/90c2aa81-2fb1-4363-a379-53edc69b3898">机器之心</a>的SOTA中都是以这几种组作为benchmark的。</p></blockquote><h2 id="其他的指标"><a href="#其他的指标" class="headerlink" title="其他的指标"></a>其他的指标</h2><p>针对上述常用的指标，也有一些改进的衡量指标，比如：</p><ul><li>Inverse Geometric Mean Rank</li><li>Adjusted Mean Rank</li><li>Adjusted Mean Rank Index</li></ul><p>这些指标可以参考 <a href="https://pykeen.readthedocs.io/en/stable/tutorial/understanding_evaluation.html">其他衡量指标</a></p><h1 id="深入评估细节"><a href="#深入评估细节" class="headerlink" title="深入评估细节"></a>深入评估细节</h1><h2 id="如何计算rank"><a href="#如何计算rank" class="headerlink" title="如何计算rank?"></a>如何计算rank?</h2><p>上边我们默认已经知道rank，但是rank是如何计算的，不就是排序列表的索引吗？</p><p>对于一个模型，输出针对triples的分数如下：</p><table><thead><tr><th>Triples</th><th>Score</th><th>Rank(optimistic)</th><th>Rank(pessimistic)</th><th>Rank(realistic)</th></tr></thead><tbody><tr><td>(高圆圆，喜欢，XXX)</td><td>0.9628</td><td>1</td><td>1</td><td>1</td></tr><tr><td>(高圆圆，喜欢，Alex)</td><td>0.9405</td><td>2</td><td>3</td><td>2.5</td></tr><tr><td>(高圆圆，喜欢，赵又廷)</td><td>0.9405</td><td>3</td><td>2</td><td>2</td></tr><tr><td>(高圆圆，喜欢，小约翰)</td><td>0.0001</td><td>4</td><td>4</td><td>4</td></tr></tbody></table><p>但是会出现一个问题：如果模型针对不同的triples输出同样的分数时，如何计算rank？这时候有三种方式：</p><ul><li><code>optimistic</code>：乐观的方式，当正样本和其他的负样本有同样的分数时，则排名取第一个；</li><li><code>pessimistic</code>：悲观的方式，当正样本和其他的负样本有同样的分数时，则排名取最后一个；</li><li><code>realistic</code>：务实的方式，当正样本和其他的负样本有同样的分数时，则排名取<code>optimistic</code>和<code>pessimistic</code>的平均排名；</li></ul><h2 id="rank选边"><a href="#rank选边" class="headerlink" title="rank选边"></a>rank选边</h2><p>上述我们一直默认，当生成负样本时，是针对head和tail同时进行的，但是其实可以任选一边，即：</p><ul><li>只针对head生成负样本；</li><li>只针对tail生成负样本；</li></ul><p>但是一般情况下，我们都是针对两边同时生成负样本。</p><p>但是指针对以便进行负样本的生成是有意义的，因为这样可以评估模型对于head和tail的预测差异。</p><h2 id="过滤已知triples"><a href="#过滤已知triples" class="headerlink" title="过滤已知triples"></a>过滤已知triples</h2><p>这个很明显，当生成负样本时，我们发现，针对(高圆圆，喜欢，Alex)，替换head，得到新的负样本的head可以任意替换，那么，恰巧生成一个负样本 (孙艺珍，喜欢，Alex)，这就尴尬了，因为这是一个正样本，当评估时必然会得到很高的分数，导致得到排名很前的rank，这就导致(高圆圆，喜欢，Alex)的排名靠后了，因此为了正确地衡量模型的性能，因此需要过滤掉这些正确的样本，因为它会忽略这些一直的triples带来的负面影响。</p><p>但是这种filtering的使用时机是很重要的：<strong>一个原则是评估的结果不会影响到模型的训练过程</strong>。根据这个原则，不可以使用filtering：</p><ul><li>Early stopping：不使用test data进行正样本的过滤，以免泄露信息；</li><li>hyperparameter optimization：不使用test data进行正样本的过滤，以免泄露信息；</li></ul><h2 id="实体和关系限定"><a href="#实体和关系限定" class="headerlink" title="实体和关系限定"></a>实体和关系限定</h2><p>这个问题需要阐述的是，上边根据LCWA或者sLCWA，可以生成针对所有实体或者关系的负样本，但是依赖于具体的任务，我们不想这么做，比如：在医疗知识图谱中，每个实体会有额外的实体类型，比如:</p><p>(氯吡格雷，治疗，过敏)，氯吡格雷是一种药物，可以治疗过敏，但是实体中有二甲双胍，这也是一种药品，但是根据上述的假设，则可以生成负样本：(氯吡格雷，治疗，二甲双胍)，这显然是没有意义的，因为在不符合实际，因为这种情况下，当生成负样本时，我们一般希望规定，生成的负样本时符合实际的，比如：(氯吡格雷，治疗，过敏性鼻炎)。</p><p>这种情况下，需要完成两步工作：</p><ul><li>重新生成评估数据；</li><li>针对重新生成的评估数据，来计算衡量指标；</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法研究 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱表示学习：训练方法</title>
      <link href="/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-xun-lian-fang-fa/"/>
      <url>/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-xun-lian-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="前提假设"><a href="#前提假设" class="headerlink" title="前提假设"></a>前提假设</h1><h2 id="Open-World-Assumption-OWA"><a href="#Open-World-Assumption-OWA" class="headerlink" title="Open World Assumption(OWA)"></a>Open World Assumption(OWA)</h2><p>开放世界假设，其含义在于暗含的假定为是未知的事实而不是假的，这也是“开放”这个词的含义，这么理解起来有点抽象，一个例子：宇宙内有除了人类的外星人，这个描述在Open World Assumption条件下，不是假的，你只能说它是未知的，这个问题的答案是“开放”的。</p><h2 id="Closed-World-Assumption-CWA"><a href="#Closed-World-Assumption-CWA" class="headerlink" title="Closed World Assumption(CWA)"></a>Closed World Assumption(CWA)</h2><p>封闭世界假设，与开放对立，即：所有未知的都是假的。同样上述的例子：宇宙内有除了人类的外星人，这个描述在Closed World Assumption条件下，我们均认为它是错误的。</p><h2 id="Local-Closed-World-Assumption-LCWA"><a href="#Local-Closed-World-Assumption-LCWA" class="headerlink" title="Local Closed World Assumption(LCWA)"></a>Local Closed World Assumption(LCWA)</h2><p>这个是什么意思呢？与Closed World Assumption相关，这涉及到局部的情况。同样上述的例子：宇宙内有除了人类的外星人，比如：对于不同的人类，这个描述在Local Closed World Assumption条件下，不同的人可能有不同的回答，对于NASA或者国家航天局来说，他们可能知道但是不说，因此他们对这个的回答可能是正确的，而对于我们来说，一般来说是错误的，因为你肯定不知道。这种情况导致不同的人群对于同样的问题出现认知的不同，所以Closed World Assumption假设过于绝对了，因此引入了Local Closed World Assumption，即：对于所有未知的知识中的一部分认为是错误的（因为有人认为那是对的）。</p><h2 id="Stochastic-Local-Closed-World-Assumption-sLCWA"><a href="#Stochastic-Local-Closed-World-Assumption-sLCWA" class="headerlink" title="Stochastic Local Closed World Assumption(sLCWA)"></a>Stochastic Local Closed World Assumption(sLCWA)</h2><p>这个假设，是在Local Closed World Assumption基础之上构建的。其实这个词相比于上述三者使用的较少，<strong>它需要和具体的采样策略结合来看</strong>。因为人类的知识没有办法具体的统计出来，因此日常接触到的知识均可以视为Local Knowledge, 而基于Local Closed World Assumption，所有不属于Local Knowledge的知识都是错误的。Stochastic Local Closed World Assumption则告诉我们，如果在这些“错误”的知识中进行采样，来帮助训练。</p><h1 id="不同假设对于训练的影响"><a href="#不同假设对于训练的影响" class="headerlink" title="不同假设对于训练的影响"></a>不同假设对于训练的影响</h1><p>这种假设条件对于知识图谱嵌入的影响如下：</p><table><thead><tr><th>Assumptions</th><th>影响</th><th>采用情况</th><th>说明</th></tr></thead><tbody><tr><td>Open World Assumption</td><td>会导致欠拟合under-fitting，也即over-generalization)</td><td>一般不用</td><td>直观理解，当一个模型对于一个自己认知之外事务，不明确表达态度时，其实是一种“缺乏自信”的表现，也会对所有已知和未知的事务同样保持“中庸”。</td></tr><tr><td>Closed World Assumption</td><td>会导致over-fitting，即泛化程度很低</td><td>一般不用</td><td>这种假设太过绝对，只要我不知道，就是错误的，有点过于“自负”，这种模型除了自己知道的事务，对其他均不认可。</td></tr><tr><td>Local Closed World Assumption</td><td>根据已有的知识，生成一部分“假”知识</td><td>可用</td><td></td></tr><tr><td>Stochastic Local Closed World Assumption</td><td>根据已有的知识，生成一部分“假”知识，从这些假知识中进行采样</td><td>可用</td><td></td></tr></tbody></table><h1 id="如何生成“假”知识？"><a href="#如何生成“假”知识？" class="headerlink" title="如何生成“假”知识？"></a>如何生成“假”知识？</h1><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>来自于 <a href="https://pykeen.readthedocs.io/en/stable/reference/training.html">source</a>。</p><blockquote><p>Throughout the following explanations of training loops, we will assume the set of entities $\mathcal{E}$ , set of relations $\mathcal{R}$ , set of possible triples $\mathcal{T} = \mathcal{E} \times \mathcal{R} \times \mathcal{E}$. We stratify $\mathcal{T}$ into the disjoint union of positive triples $\mathcal{T^{+}} \subseteq \mathcal{T}$ and negative triples $\mathcal{T^{-}} \subseteq \mathcal{T}$ such that $\mathcal{T^{+}} \cap \mathcal{T^{-}} = \emptyset$ and $\mathcal{T^{+}} \cup \mathcal{T^{-}} = \mathcal{T}$ .</p></blockquote><h2 id="“朴素”的方法"><a href="#“朴素”的方法" class="headerlink" title="“朴素”的方法"></a>“朴素”的方法</h2><p>对于一个已有的知识图谱而言，根据Closed World Assumption，任何不在该知识图谱内的知识或者三元组都是假知识。</p><p>一种“朴素”的负样本的生成办法是：</p><p>对于三元组，可能的组合有以下几种：</p><ul><li>N(head): 代表head实体的种类数量；</li><li>N(rel): 代表relation的种类数量；</li><li>N(tail)：代表tail实体的种类数量；</li></ul><p>则可能的三元组的数量为：</p><p>$$<br>N(head) \times N(rel) \times N(tail)<br>$$</p><p>当删除知识图谱内的三元组时，就可以认为是所有的负样本。这种负样本的数量是巨大的。</p><p>一种可以解决方法是：采样。具体的采样方法有很多种，这里不赘述了。</p><p>但是有一种情况，我们的模型需要<strong>更容易地对真知识进行判断</strong>，举个例子：</p><p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTM15gYJOEGxQqr5jBsWUzpAw6RJjvCzus8MA&usqp=CAU" alt="高圆圆"></p><p>对比，高圆圆比娜扎漂亮，我们假设它是对的，当出现凤姐时，模型已经认识高圆圆，它能得出高圆圆比凤姐漂亮。但是当得到的负样本为：梅根福克斯与克劳馥时，你觉得模型对：</p><ul><li>高圆圆与凤姐</li><li>梅根福克斯与克劳馥</li></ul><p>哪个容易判断。</p><h2 id="LCWA下的负样本生成"><a href="#LCWA下的负样本生成" class="headerlink" title="LCWA下的负样本生成"></a>LCWA下的负样本生成</h2><p>基于上述的情况，在Local Closed World Assumption下，<strong>负样本的生成基于以下原则：任意替换三元组的任意一个位置，得到的负样本不属于知识图谱本身，即认为是负样本。</strong></p><p>根据这个原则，可以有三种类型的操作：</p><ul><li><p>head generation: $(h, r, t)$ -&gt; $(h, r, t_i)$;</p><p>   In this setting, for any triple $(h, r, t) \in \mathcal{K}$ that has been observed, a set<br>   $\mathcal{T^{-}} (h, r) $ of negative examples is created by considering all triples $(h, r, t_i) \notin \mathcal{K}$ as false.</p></li><li><p>relation generation: $(h, r, t)$ -&gt; $(h, r_i, t)$; </p><p>   In this setting, for any triple $(h, r, t) \in \mathcal{K}$ that has been observed, a set<br>   $\mathcal{T^{-}} (h, t) $ of negative examples is created by considering all triples $(h, r_i, t) \notin \mathcal{K}$ as false.</p></li><li><p>tail generation: $(h, r, t)$ -&gt; $(h_i, r, t)$;</p><p>   In this setting, for any triple $(h, r, t) \in \mathcal{K}$ that has been observed, a set<br>   $\mathcal{T^{-}} (r, t) $ of negative examples is created by considering all triples $(h_i, r, t) \notin \mathcal{K}$ as false.</p></li></ul><p>一般情况下，在Local Closed World Assumption下，很多的实现都只会考虑head generation和relation generation，不会考虑tail generation。</p><h2 id="sLCWA下的负样本生成"><a href="#sLCWA下的负样本生成" class="headerlink" title="sLCWA下的负样本生成"></a>sLCWA下的负样本生成</h2><p>从Local Closed World Assumption中的三种情况下进行的集合进行采样，即：</p><ul><li>$(h, r, t)$ -&gt; $(h, r, t_i)$</li><li>$(h, r, t)$ -&gt; $(h, r_i, t)$</li><li>$(h, r, t)$ -&gt; $(h_i, r, t)$</li></ul><p>但从实际来看，有时候只需要$(h, r, t)$ -&gt; $(h, r, t_i)$和$(h, r, t)$ -&gt; $(h_i, r, t)$，而没有考虑关系的替换，然后从这两者的集合中进行采样。</p><table><thead><tr><th>Assumptions</th><th>Local Closed World Assumption</th><th>Stochastic Local Closed World Assumption</th></tr></thead><tbody><tr><td>负样本来源</td><td>$(h, r, t)$ -&gt; $(h, r, t_i)$， $(h, r, t)$ -&gt; $(h, r_i, t)$</td><td>$(h, r, t)$ -&gt; $(h, r, t_i)$，$(h, r, t)$ -&gt; $(h_i, r, t)$</td></tr><tr><td>是否要采样</td><td>不需要</td><td>需要</td></tr><tr><td>过滤正样本</td><td>需要</td><td>需要</td></tr></tbody></table><h2 id="例子说明"><a href="#例子说明" class="headerlink" title="例子说明"></a>例子说明</h2><p>这个具体的帮助理解的例子来自于:<br><a href="https://arxiv.org/abs/2006.13365">Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework</a></p><p><img src="https://pykeen.readthedocs.io/en/stable/_images/training_approaches.png"></p><p>该图对比了负样本在：</p><ul><li>Local Closed World Assumption</li><li>Stochastic Local Closed World Assumption<br>两种生成策略。</li></ul><p>对于同一种关系 <code>works_at</code>, 红色部分是true triples。</p><ul><li>在LCWA生成的是深蓝色对应的负样本，它们都没在原始的知识图谱上。</li><li>在sLCWA生成的是浅蓝色对应的负样本，并从其中采样，它们都没在原始的知识图谱上。</li><li>黄色部分则不在负样本的考虑之列。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法研究 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读 - Convolutional 2D Knowledge Graph Embeddings</title>
      <link href="/2021/11/14/convkb/"/>
      <url>/2021/11/14/convkb/</url>
      
        <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>Link prediction是在知识图谱中预测实体之间的关系的任务，在查询扩展和语义关系预测中需要解决的，也是本论文致力于解决的。</p><p>在解决Link prediction, 其核心考虑因素在于：</p><blockquote><p>link predictors should scale in a manageable way with respect to <strong>both the number of parameters and computational costs</strong> to be applicable in real-world scenarios.</p></blockquote><p>同时需要考虑计算成本，还是考虑模型本身的预测性能，而参数数量是模型性能的一个指标。</p><p>之前方法的问题在于：</p><blockquote><p>Previous work on link prediction has <strong>focused on shallow, fast models</strong> which can scale to large knowledge graphs. However, these models <strong>learn less expressive features</strong> than deep, multi-layer models which potentially limits performance.</p></blockquote><p>简单来讲，之前的方法过于关注在大型知识图谱上进行快速的学习，这需要网络不太复杂。但是其架构学习到的特征不够丰富，性能不够强。</p><p>为了提高知识图谱上的预测性能，从而提高表现力，在不使用深层模型的前提下，只能提高embedding size，但是难以应用到大型知识图谱上。</p><p>换一种思路，使用深度模型，就可以减少embedding size，因为可以学到高层特征，但是在过往的模型中，深度模型中的使用的架构都是全连接层，因此会造成过拟合问题。</p><p>以上问题的一种解决办法是：</p><blockquote><p>use parameter efﬁcient, fast operators which can be composed into deep networks.</p></blockquote><p>因此，自然而然，卷积的操作就被引入了，因为以下特点：</p><ul><li>parameter efﬁcient</li><li>fast to compute: highly optimised GPU implementations</li></ul><p>本文提出来的模型：ConvE, a multi-layer convolutional network model for link prediction，这是一个基于卷积的多层架构。它的优点如下：</p><ul><li><strong>highly parameter efﬁcient</strong>: yielding the same performance as <strong>DistMult</strong> and <strong>R-GCN</strong> with 8x and 17x fewer parameters;</li><li><strong>particularly effective at modelling nodes with high indegree</strong>: common in highlyconnected, complex knowledge graphs such as Freebase and YAGO3;</li></ul><h1 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h1><h2 id="2-1-1D-vs-2D-Convolutions"><a href="#2-1-1D-vs-2D-Convolutions" class="headerlink" title="2.1 1D vs 2D Convolutions"></a>2.1 1D vs 2D Convolutions</h2><p>NLP任务中，多数使用1D卷积进行操作，即在文本的序列方向上进行卷积操作，本文使用2D卷积，不仅在文本序列方向进行操作，同时还在embedding的纵向上进行操作，这使用卷积操作捕捉到的交互信息更丰富。</p><p>这种操作的优点在于： increases the expressiveness of our model through additional points of interaction between embeddings.</p><p>这种操作如何理解？</p><p>当把1D的embedding 进行拼接时，你仍然需要得到1D的embedding，那么拼接的方法只能是在一维上，如下例：</p><p><img src="/images/conve/lex0.png"></p><p>其中假如你的卷积核k=3, 那么能捕捉到的交互只有最临近的a和b, 除非你讲卷积核的尺寸增加，这样才能捕捉到更多的交互。</p><p>当换到2D时，由于是在二维的方向上进行拼接和堆叠，因此其方式可以有多种，因此当卷积进行操作时，可以捕捉的信息可以是左右方向的，也可以是上下方向的，如下图：</p><p><img src="/images/conve/lex1.png"></p><p>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的提升交互次数:</p><p><img src="/images/conve/lex2.png"></p><p>因此在2D条件下，捕捉到的交互信息数量是不单单与卷积核有关，而且还与矩阵的尺寸有关。</p><h2 id="2-2-问题形式化"><a href="#2-2-问题形式化" class="headerlink" title="2.2 问题形式化"></a>2.2 问题形式化</h2><p>link prediction 可以认为是 a pointwise learning to rank problem。具体而言，对每个输入的三元组 triples: $x = (s, r, o)$, 目标是 learning a scoring function $\psi(x)$, 其结果正比于x为true的likelihood。</p><h2 id="2-3-Neural-Link-Predictors"><a href="#2-3-Neural-Link-Predictors" class="headerlink" title="2.3 Neural Link Predictors"></a>2.3 Neural Link Predictors</h2><p>这个组件是干嘛的，深度学习中总有一些为了唬人提出的名词，其实这个predictors就是一个多层神经网络，包括：</p><ul><li>encoding component: 对 $x = (s, r, o)$, 该部分将subject和object映射为embeddings, $e_s$, $e_o$; </li><li>scoring component: 使用 $\psi_r$ 对embeddings评分，即: $\psi(s, r, o) = \psi_r(e_s, e_o)$ </li></ul><p>一些经常使用的典型predictors如下: </p><p><img src="/images/conve/1.png" alt="经常使用的典型predictors"></p><h2 id="2-4-模型主要内容"><a href="#2-4-模型主要内容" class="headerlink" title="2.4 模型主要内容"></a>2.4 模型主要内容</h2><h3 id="2-4-1-Scoring-function"><a href="#2-4-1-Scoring-function" class="headerlink" title="2.4.1 Scoring function"></a>2.4.1 Scoring function</h3><p>scoring function定义如下:<br>$$<br>\psi_{r}\left(\mathbf{e}_s, \mathbf{e}_o\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_s} ; \overline{\mathbf{r}_r}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_o<br>$$</p><p>一些符号如下:</p><ul><li><p>$\mathbf{e}_s, \mathbf{e}_o$ 分别代表头实体和尾实体的Embedding；</p></li><li><p>$\overline{\mathbf{e}_s}, \overline{\mathbf{r}_r}$ 分别代表Reshape后的头实体和关系向量，这种操作如下：如果 $\mathbf{e}_s, \mathbf{r}_r \in \mathbb{R}^k$, 那么 $\overline{\mathbf{e}_s}, \overline{\mathbf{r}_r} \in \mathbb{R}^{k_w \times k_h} $, 则 $k = k_w  k_h$</p></li><li><p>$\omega$ 代表卷积核；</p></li><li><p>$\mathbf{W}$ 代表投影矩阵；</p></li></ul><h3 id="2-4-2-模型架构"><a href="#2-4-2-模型架构" class="headerlink" title="2.4.2 模型架构"></a>2.4.2 模型架构</h3><p><img src="/images/conve/2.png" alt="ConvE"></p><p><code>ConvE</code>的整个训练过程如下.</p><ul><li>先通过Embedding的方式分别获得头实体表示 $\mathbf{e}_s$ 和关系表示 $\mathbf{r}_r$ ；</li><li>将头实体和关系表示先<code>Concat</code>起来, 然后将其<code>Reshape</code>到某一种尺寸, 此时头实体和关系的表示记为 $\left[ \overline{\mathbf{e}_s} ; \overline{\mathbf{r}_r} \right]$；</li><li>接着利用卷积抽取Reshape后的二维向量, 也就是对头实体和关系的交互信息进行捕捉；</li><li>利用卷积(可以是任意数量的卷积核)抽取完信息后, 将所有的特征打平成一个一维向量；</li><li>通过投影矩阵 $\mathbf{W}$ 投影到一个中间层中，输出的尺寸与embedding size相同，以便与尾实体表示 $\mathbf{e}_o$ 做内积, 获得相似度, 即Logits；</li><li>这种方式通过内积来比较所获向量与尾实体的相似度, 越相似得分越高.</li><li>然后将Logits经过 $\sigma$ 函数, 得到每个实体的概率：  $$p=\sigma(\psi_r\left(\mathbf{e}_s, \mathbf{e}_o \right))$$</li></ul><p>优化时的损失函数采用BCE(binary cross-entropy loss)：</p><p>$$\mathcal{L}(p, t)=-\frac{1}{N} \sum_i\left(t_i \cdot \log \left(p_i \right)+\left(1-t_i \right) \cdot \log \left(1-p_i \right)\right)$$</p><p>$t$ 是尾实体的one-hot vector. 对于和输入的 $(s, r, ?)$ 匹配的位置为1，其余为0.</p><h3 id="2-4-3-训练tips"><a href="#2-4-3-训练tips" class="headerlink" title="2.4.3 训练tips"></a>2.4.3 训练tips</h3><h4 id="2-4-3-1-基本tips"><a href="#2-4-3-1-基本tips" class="headerlink" title="2.4.3.1 基本tips"></a>2.4.3.1 基本tips</h4><ul><li>rectiﬁed linear units: as the non-linearity $f$, 加快训练；</li><li>batch normalization: after each layer to stabilise</li><li>regularise: dropout</li><li>optimiser: Adam</li><li>label smoothing: to lessen overﬁtting due to saturation of output non-linearities at the labels</li></ul><h4 id="2-4-3-2-加速评估tips"><a href="#2-4-3-2-加速评估tips" class="headerlink" title="2.4.3.2 加速评估tips"></a>2.4.3.2 加速评估tips</h4><p>卷积操作消耗大量时间</p><blockquote><p>convolution consumes about 75-90% of the total computation time, thus it is important to minimise the number of convolution operations to speed up computation</p></blockquote><p>思路1：增加batch size加速，但是CNN会使得GPU的内存超过限制；</p><p>解决办法：1-N scoring</p><p>ConvE最后的输出, 能获得对所有实体相关的Logits, 这样就能<strong>对所有的尾实体同时打分</strong>, <strong>而不用考虑采样的问题</strong>. 在原文中这种打分方式被称为<strong>1-N Scoring</strong>.</p><p>过去评估时，需要采样负样本，进行1-1评估。现在这种方式能极大地加快Evaluation的速度, 因为负采样只能对单一的三元组打分, 而这种方式能同时对所有的尾实体同时打分。这种思想能够应用于所有的1-1 scoring Model.</p><p>这种方式其实本质上利用GPU并行执行的特点，在架构上将训练和评估同时考虑，通过将平衡计算性能和收敛速度，来使得评估过程加快。</p><h1 id="3-实验评估"><a href="#3-实验评估" class="headerlink" title="3. 实验评估"></a>3. 实验评估</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><table><thead><tr><th>数据集</th><th>来源</th><th>关系</th><th>实体</th><th>三元组</th><th>说明</th></tr></thead><tbody><tr><td>WN18</td><td>a subset of WordNet</td><td>18</td><td>40943</td><td>151442</td><td>consist of hyponym and hypernym relations and, for such a reason, WN18 tends to follow a strictly hierarchical structure. 用WN18RR替代。</td></tr><tr><td>FB15k</td><td>a subset of Freebase</td><td>1345</td><td>14951</td><td>——</td><td>A large fraction of content in this knowledge graph describes facts about movies, actors, awards, sports, and sport teams. 用FB15k-237替代。</td></tr><tr><td>YAGO3-10</td><td>a subset of YAGO3</td><td>37</td><td>123182</td><td>entities which have a minimum of 10 relations each</td><td>Most of the triples deal with descriptive attributes of people, such as citizenship, gender, and profession.</td></tr><tr><td>Countries</td><td></td><td></td><td></td><td></td><td>a benchmark dataset that is useful to evaluate a model’s ability to learn long-range dependencies between entities and relations. It consists of three sub-tasks which increase in difﬁculty in a step-wise fashion, where the minimum pathlength to ﬁnd a solution increases from 2 to 4.</td></tr></tbody></table><h2 id="3-2-超参"><a href="#3-2-超参" class="headerlink" title="3.2 超参"></a>3.2 超参</h2><ul><li><p>选择方法: <code>grid search</code> according to the <code>mean reciprocal rank (MRR)</code> on the validation set</p></li><li><p>选定范围:</p><ul><li><code>embedding dropout</code>: 0.0, 0.1, 0.2</li><li><code>feature map dropout</code>: 0.0, 0.1, 0.2, 0.3</li><li><code>projection layer dropout</code>: 0.0, 0.1, 0.3, 0.5</li><li><code>embedding size</code>: 100, 200</li><li><code>batch size</code>: 64, 128, 256</li><li><code>learning rate</code>: 0.001, 0.003</li><li><code>label smoothing</code>: 0.0, 0.1, 0.2, 0.3</li></ul></li><li><p>最佳参数:</p><ul><li>WN18, YAGO3-10 and FB15k<ul><li><code>embedding dropout</code>: 0.2</li><li><code>feature map dropout</code>: 0.2</li><li><code>projection layer dropout</code>: 0.3</li><li><code>embedding size</code>: 200</li><li><code>batch size</code>: 128</li><li><code>learning rate</code>: 0.001</li><li><code>label smoothing</code>: 0.1</li></ul></li><li>Countries dataset<ul><li><code>embedding dropout</code>: 0.3</li><li><code>hidden dropout</code>: 0.5</li><li><code>label smoothing</code>: 0</li></ul></li><li><code>early stopping</code> according to the <code>mean reciprocal rank</code> (WN18, FB15k, YAGO3-10) and <code>AUC-PR</code> (Countries) statistics on the validation set</li></ul></li></ul><h2 id="3-3-结果"><a href="#3-3-结果" class="headerlink" title="3.3 结果"></a>3.3 结果</h2><p>实验中进行评估时，有几件注意事项：</p><ul><li><p>由于数据出现leakage，因此使用了rule-based method来识别逆向关系作为对照，同时在数据集包括有无逆向关系)中进行评估；</p></li><li><p>使用了<code>filtered setting</code>;</p><blockquote><p>Rank test triples against all other candidate triples not appearing in the training, validation, or test set.</p></blockquote><blockquote><p>Candidates are obtained by permuting either the subject or the object of a test triple with all entities in the knowledge graph.</p></blockquote></li></ul><h3 id="3-3-1-从参数效率看"><a href="#3-3-1-从参数效率看" class="headerlink" title="3.3.1 从参数效率看"></a>3.3.1 从参数效率看</h3><p><img src="/images/conve/3.png" alt="参数数量和性能(ConvE vs. DistMult)"></p><h3 id="3-3-2-结果-含有逆向关系"><a href="#3-3-2-结果-含有逆向关系" class="headerlink" title="3.3.2 结果(含有逆向关系)"></a>3.3.2 结果(含有逆向关系)</h3><p><img src="/images/conve/4.png" alt="结果(含有逆向关系)"></p><h3 id="3-3-3-结果-不含逆向关系"><a href="#3-3-3-结果-不含逆向关系" class="headerlink" title="3.3.3 结果(不含逆向关系)"></a>3.3.3 结果(不含逆向关系)</h3><p>这里将数据集中存在逆向关系的三元组全部删除了，来避免leakage造成的负面影响。</p><p><img src="/images/conve/5.png" alt="结果(不含逆向关系)1"></p><p><img src="/images/conve/6.png" alt="结果(不含逆向关系)2"></p><h2 id="3-4-分析"><a href="#3-4-分析" class="headerlink" title="3.4 分析"></a>3.4 分析</h2><h3 id="3-4-1-消融实验"><a href="#3-4-1-消融实验" class="headerlink" title="3.4.1 消融实验"></a>3.4.1 消融实验</h3><p>为了查看，哪部分组件在整个架构中的作用最重要，消融实验显示：</p><p><img src="/images/conve/7.png" alt="消融实验结果"></p><ul><li>hidden dropout的影响最大;</li><li>但是label smoothing的影响几乎可以忽略；</li></ul><h3 id="3-4-2-从图的结构分析优点"><a href="#3-4-2-从图的结构分析优点" class="headerlink" title="3.4.2 从图的结构分析优点"></a>3.4.2 从图的结构分析优点</h3><h4 id="3-4-2-1-假设1"><a href="#3-4-2-1-假设1" class="headerlink" title="3.4.2.1 假设1"></a>3.4.2.1 假设1</h4><p>datasets contain nodes with very high relation-speciﬁc indegree时，<code>ConvE</code>效果更好，而indegree较小时，一些模型足以应对<code>DistMult</code>。</p><blockquote><p>Our hypothesis is that deeper models, that is, models that learn multiple layers of features, like ConvE, have an advantage over shallow models, like DistMult, to capture all these constraints.</p></blockquote><ul><li>验证：通过将数据中indegree中的过大或者过小的node删除，然后分别使用ConvE和DistMult进行实验，验证了上述假设。</li></ul><h4 id="3-4-2-2-假设2"><a href="#3-4-2-2-假设2" class="headerlink" title="3.4.2.2 假设2"></a>3.4.2.2 假设2</h4><p>平均PageRank越高的graph，ConvE的性能相比于<code>DistMult</code>越好；</p><blockquote><p>This gives additional evidence that models that are deeper have an advantage when modelling nodes with high (recursive) indegree.</p></blockquote><ul><li>验证：通过计算各个数据集的平均pagerank值，然后计算pagerank值与(convE - DistMult)差值计算相关性，验证了上述假设，这个假设2与假设1其实可以看成等价。</li></ul><h1 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4. 代码实现"></a>4. 代码实现</h1><ul><li><p><a href="https://github.com/TimDettmers/ConvE">原文实现</a></p></li><li><p><a href="https://github.com/Accenture/AmpliGraph">AmpliGraph</a></p></li></ul><h1 id="5-问题"><a href="#5-问题" class="headerlink" title="5. 问题"></a>5. 问题</h1><ol><li>与CV中CNN的应用对比，模型仍是浅层，未来会增加深度的卷积模型。</li><li>2D卷积的解释；</li><li>如何更多地捕捉embedding之间的交互，例如通过增加大型结构；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《美国陷阱》中阿尔斯通被肢解背后的经济战</title>
      <link href="/2021/11/13/mei-guo-xian-jing-zhong-a-er-si-tong-bei-zhi-jie-bei-hou-de-jing-ji-zhan/"/>
      <url>/2021/11/13/mei-guo-xian-jing-zhong-a-er-si-tong-bei-zhi-jie-bei-hou-de-jing-ji-zhan/</url>
      
        <content type="html"><![CDATA[<p>疫情在家，除了完成正常的工作学习外，由于没啥好的娱乐项目，读书是一种廉价的消遣方式。正巧遇到一本书《美国陷阱》，其中的情节和阴谋论恐怕电影都拍不出来，甚至可以当成悬疑小说来读。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/1.png" alt="《美国陷阱》"></p><h1 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1. 背景介绍"></a>1. 背景介绍</h1><p>最近几年，美国加大对中国的打压力度，尤其是科技领域，其中以华为最为大家所熟悉。那么最为人所知伎俩即，扣押孟晚舟（任正非的女儿和华为的高管）。大家肯定跌破眼镜，这种流氓式的手段，对于被称为“世界最大的民主国家”的美国，怎么干得出来？</p><p>当然，理由还是要有的，不然无法说服世界舆论，其实估计也没人信。因为这种套路也不是第一次了，本书讲得就是同样的套路，应用到另一个公司的另一个人身上。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/2.png" alt="两起事件的对比"></p><p>而本书，讲得就是阿尔斯通的故事，而且是由当事人弗雷德里克-皮耶鲁齐自己陈述的。因此对于我们具有极大的参考价值。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/3.png" alt="这位老哥比孟晚舟惨得多"></p><h1 id="2-阿尔斯通与中国"><a href="#2-阿尔斯通与中国" class="headerlink" title="2. 阿尔斯通与中国"></a>2. 阿尔斯通与中国</h1><p>阿尔斯通是法国一家有战略意义的工业巨头，关于它的内容，大家可以在网上搜索，我这里不过多阐述，大家认识到这是一家工业巨头就可以了，法国的核电站都是它来维护的，舰艇的燃气轮机也是它提供的。</p><p>而他与中国的渊源也是颇深，三峡水利工程、向家坝水利工程等好多项目，都有它的参与，<strong>甚至书中还披露，当然是阿尔斯通的认罪协议中，阿尔斯通曾经为参与台北地铁项目，而贿赂相关负责人。</strong></p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/4.png" alt="阿尔斯通"></p><h1 id="3-书中主要内容"><a href="#3-书中主要内容" class="headerlink" title="3. 书中主要内容"></a>3. 书中主要内容</h1><p>为了大家能够明白书中的主要线索，我首先用图的方式表达其中涉及到的参与人和机构。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/5.png" alt="当事人和相关关系"></p><ol><li>首先，美国司法部质控阿尔斯通违反《反腐败法》，逮捕作者，也就是皮耶鲁齐，希望他认罪，在阿尔斯通内部充当“间谍”，以便掌控该企业的内部信息，作者拒绝，被监禁；</li><li>陆续有其他阿尔斯通的高管被逮捕，公司CEO惶恐，希望摆脱诉讼和入狱；</li><li>通用电气介入，摆出了可以帮助阿尔斯通摆脱当前困局的慈善面目，公司CEO为脱罪，达成秘密协议（推测，当事人都矢口否认，但作者强烈暗示这种行为），将阿尔斯通能源部门出售给通用电气（是该公司最有价值的资产）；</li><li>法国政府对这种行为保持暧昧态度，后来知晓，通用电气和阿尔斯通花费将近3亿欧元进行公关，还有一层原因，马克龙（现任法国总统，时任经济部长）相信阿尔斯通CEO与美国司法部、通用电气有交易，但是没有证据；同时受限于大西洋主义以及对美国强大实力的深深恐惧；</li><li>“肢解”交易达成，作者被判入狱30个月。</li></ol><p>从上述过程中，从头到尾，<strong>作者的角色和所处的地位，相较于整个事件，简直微不足道！所以，实际上他当了一个背锅侠和替罪羊！</strong></p><h1 id="4-带给我们的思考和启示"><a href="#4-带给我们的思考和启示" class="headerlink" title="4. 带给我们的思考和启示"></a>4. 带给我们的思考和启示</h1><p><strong>第一点</strong>，战争的形式已经发生变化了，从之前枪炮相加，血肉横飞到网络世界的攻防，可能并没有人死亡，但是损失可能更大。<strong>这本书中提到的有两点很值得注意，一个是舆论公关战，一个就是法律战。</strong></p><p>通用电气和阿尔斯通为促成收购，在媒体和网络上，大肆说明该收购带来的好处，比如增加就业岗位，帮助阿尔斯通摆脱诉讼，同时游说法国政界和企业界的精英。结果，一个曾经在戴高乐将军领导下致力于独立自主的法国居然在外人收购自己的核心资产时，居然沉默是“金”，简直可怕！想想我们这个社会，这样的人会在少数吗？</p><p>法律战，也即本事件的起因，违反《反海外腐败法》。这个法是个什么东西呢？简单来说，美国之外国家的公司，如果使用了与美国相关的工具，则全部收受到该法律的管辖。是不是，有点治外法权的意味呢！比如，在我们国家犯罪，却用美国的法律诉讼，是不是有点滑稽，这种域外法的行使，使得美国的“全球警察”地位坐实了。而且，还让人产生错觉，好像是我错了（虽然在本书中提到的阿尔斯通确实有腐败行为）！</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/6.png" alt="《反海外腐败法》"></p><p><strong>第二点</strong>，我想引述书中的一句话，</p><blockquote><p>我们不能上当受骗。不管谁当美国总统，无论他是民主党人还是共和党人，华盛顿都会维护少数工业巨头的利益</p></blockquote><p>有人曾经，觉得美国民主党上台，对我们可能会温和一点（可能法国人也是这么想的），还是放弃这种幻想吧！这是体制决定的，不是简单的人事变动能够改变的，美国政府和企业巨头的纠缠不断的联系导致“旋转门”的不断上演，所以我们还是做好准备吧！</p><p><strong>第三点</strong>，“人权”和“自由”这种过于抽象的东西，每个人都有自己的理解，比如说那个只喝矿泉水的归国人员，它的解释就为“特权”。<strong>而且每个人的理解都会倾向于自己的利益</strong>，比如作者在狱中的遭遇，美国的监狱的确是没有“人权”，你能相信监狱是私人的，所以当然是需要盈利的，“人权”多费钱啊，与资本主义的核心目标——攫取利益不符啊，当事情与他们的利益不符时，就会有这样的论调。当人权在敌对国家遭到威胁时，这时候美国这个道德楷模就出来了。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/7.png" alt="你品，你细品！"></p><p>第四点，爱国主义，这种东西在人们的脑海中是正面的东西，我也倾向于这样认为。但是有一点在书中提及，声称“自己生于斯，长于斯”的人，向大家彰显自己的爱国举动，真是是所谓的“爱国人士”吗，或许阿尔斯通CEO柏珂龙的话有助于我们思考，</p><blockquote><p>我是法国择优选拔人才体制的产物。借此机会，我想说，我的父母都是移民。自从在阿尔斯通任职后，我在法国创造了将近1.5万个就业岗位，对此我感到非常自豪。每个人都必须为法国就业做出一份贡献。我已经努力做出了微薄的贡献。</p></blockquote><p>不要忘记，就是他私下与通用电气密谋，肢解了自己服务的“工业明珠”！</p><p>在这本书中，还有很多值得人们思考的东西，包括友情、爱情、劳资关系、糖衣炮弹、爱国主义等等，建议大家去看一下，会收获不一样的东西！其中，对个人来说，最想提及的一点是，找一个好妻子真的是很重要的事情，尤其是像这位老哥遭遇监禁这样的遭遇时，更能体现这一点。</p><h1 id="5-Q-amp-A"><a href="#5-Q-amp-A" class="headerlink" title="5. Q&amp;A"></a>5. Q&amp;A</h1><p>看本书时，我一直不明白的有几个问题，后来弄明白了，在这里列出来同大家分享。</p><p><strong>1) 美国为啥可以管理国外的犯罪？怎么管？</strong></p><p>这与美国在国际上巨大的影响力和超强的实力有关，比如：做国际贸易需要美元结算，没有美元的话没办法进行交易，大家伙都不认，这也是我们一直推进人民币国际化的原因之一。因此，你只要涉及到美元交易，就归美国管，因为美元是美国发行的，具体怎么管，国际贸易会有清算，主要的清算中心在美国，通俗点讲，不听话的话，你就没办法收到货款，也没办法进口。这里，顺便提一句，你觉得Facebook的Libra能成功吗？</p><p>还有一点，除了中国外，世界主要的信息技术公司都是美国公司，所以只要你用了Gmail、Facebook等就都和美国有关系了。</p><p>其实，立什么法不重要，重要的是要有执行能力，这是美国厉害的地方。比如，有一天，你说全世界都得听我的，人家以为你有病呢，但是有人不听时，你有办法治人家，这时大家才服你，最起码表面上服你！</p><p><strong>2) 大西洋主义 VS 亲欧洲主义</strong></p><p>简单来讲，大西洋主义，就是西欧和北美互相合作，保护共同的安全和价值观，但是被认为是美国操纵欧洲的工具，尤其是对于盟友，美国的小动作也在不断的情况下。亲欧洲主义，强调欧洲一体化，摆脱美国的影响。</p>]]></content>
      
      
      <categories>
          
          <category> 读书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济 </tag>
            
            <tag> 政治 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
