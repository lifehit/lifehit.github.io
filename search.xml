<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>论文阅读 - Convolutional 2D Knowledge Graph Embeddings</title>
      <link href="/2021/11/14/convkb/"/>
      <url>/2021/11/14/convkb/</url>
      
        <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>Link prediction是在知识图谱中预测实体之间的关系的任务，在查询扩展和语义关系预测中需要解决的，也是本论文致力于解决的。</p><p>在解决Link prediction, 其核心考虑因素在于：</p><blockquote><p>link predictors should scale in a manageable way with respect to <strong>both the number of parameters and computational costs</strong> to be applicable in real-world scenarios.</p></blockquote><p>同时需要考虑计算成本，还是考虑模型本身的预测性能，而参数数量是模型性能的一个指标。</p><p>之前方法的问题在于：</p><blockquote><p>Previous work on link prediction has <strong>focused on shallow, fast models</strong> which can scale to large knowledge graphs. However, these models <strong>learn less expressive features</strong> than deep, multi-layer models which potentially limits performance.</p></blockquote><p>简单来讲，之前的方法过于关注在大型知识图谱上进行快速的学习，这需要网络不太复杂。但是其架构学习到的特征不够丰富，性能不够强。</p><p>为了提高知识图谱上的预测性能，从而提高表现力，在不使用深层模型的前提下，只能提高embedding size，但是难以应用到大型知识图谱上。</p><p>换一种思路，使用深度模型，就可以减少embedding size，因为可以学到高层特征，但是在过往的模型中，深度模型中的使用的架构都是全连接层，因此会造成过拟合问题。</p><p>以上问题的一种解决办法是：</p><blockquote><p>use parameter efﬁcient, fast operators which can be composed into deep networks.</p></blockquote><p>因此，自然而然，卷积的操作就被引入了，因为以下特点：</p><ul><li>parameter efﬁcient</li><li>fast to compute: highly optimised GPU implementations</li></ul><p>本文提出来的模型：ConvE, a multi-layer convolutional network model for link prediction，这是一个基于卷积的多层架构。它的优点如下：</p><ul><li><strong>highly parameter efﬁcient</strong>: yielding the same performance as <strong>DistMult</strong> and <strong>R-GCN</strong> with 8x and 17x fewer parameters;</li><li><strong>particularly effective at modelling nodes with high indegree</strong>: common in highlyconnected, complex knowledge graphs such as Freebase and YAGO3;</li></ul><h1 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h1><h2 id="2-1-1D-vs-2D-Convolutions"><a href="#2-1-1D-vs-2D-Convolutions" class="headerlink" title="2.1 1D vs 2D Convolutions"></a>2.1 1D vs 2D Convolutions</h2><p>NLP任务中，多数使用1D卷积进行操作，即在文本的序列方向上进行卷积操作，本文使用2D卷积，不仅在文本序列方向进行操作，同时还在embedding的纵向上进行操作，这使用卷积操作捕捉到的交互信息更丰富。</p><p>这种操作的优点在于： increases the expressiveness of our model through additional points of interaction between embeddings.</p><p>这种操作如何理解？</p><p>当把1D的embedding 进行拼接时，你仍然需要得到1D的embedding，那么拼接的方法只能是在一维上，如下例：</p><p><img src="/images/conve/lex0.png"></p><p>其中假如你的卷积核k=3, 那么能捕捉到的交互只有最临近的a和b, 除非你讲卷积核的尺寸增加，这样才能捕捉到更多的交互。</p><p>当换到2D时，由于是在二维的方向上进行拼接和堆叠，因此其方式可以有多种，因此当卷积进行操作时，可以捕捉的信息可以是左右方向的，也可以是上下方向的，如下图：</p><p><img src="/images/conve/lex1.png"></p><p>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的提升交互次数:</p><p><img src="/images/conve/lex2.png"></p><p>因此在2D条件下，捕捉到的交互信息数量是不单单与卷积核有关，而且还与矩阵的尺寸有关。</p><h2 id="2-2-问题形式化"><a href="#2-2-问题形式化" class="headerlink" title="2.2 问题形式化"></a>2.2 问题形式化</h2><p>link prediction 可以认为是 a pointwise learning to rank problem。具体而言，对每个输入的三元组 triples: $x = (s, r, o)$, 目标是 learning a scoring function $\psi(x)$, 其结果正比于x为true的likelihood。</p><h2 id="2-3-Neural-Link-Predictors"><a href="#2-3-Neural-Link-Predictors" class="headerlink" title="2.3 Neural Link Predictors"></a>2.3 Neural Link Predictors</h2><p>这个组件是干嘛的，深度学习中总有一些为了唬人提出的名词，其实这个predictors就是一个多层神经网络，包括：</p><ul><li>encoding component: 对 $x = (s, r, o)$, 该部分将subject和object映射为embeddings, $e_s$, $e_o$; </li><li>scoring component: 使用 $\psi_r$ 对embeddings评分，即: $\psi(s, r, o) = \psi_r(e_s, e_o)$ </li></ul><p>一些经常使用的典型predictors如下: </p><p><img src="/images/conve/1.png" alt="经常使用的典型predictors"></p><h2 id="2-4-模型主要内容"><a href="#2-4-模型主要内容" class="headerlink" title="2.4 模型主要内容"></a>2.4 模型主要内容</h2><h3 id="2-4-1-Scoring-function"><a href="#2-4-1-Scoring-function" class="headerlink" title="2.4.1 Scoring function"></a>2.4.1 Scoring function</h3><p>scoring function定义如下:<br>$$<br>\psi_{r}\left(\mathbf{e}_s, \mathbf{e}_o\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_s} ; \overline{\mathbf{r}_r}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_o<br>$$</p><p>一些符号如下:</p><ul><li><p>$\mathbf{e}_s, \mathbf{e}_o$ 分别代表头实体和尾实体的Embedding；</p></li><li><p>$\overline{\mathbf{e}_s}, \overline{\mathbf{r}_r}$ 分别代表Reshape后的头实体和关系向量，这种操作如下：如果 $\mathbf{e}_s, \mathbf{r}_r \in \mathbb{R}^k$, 那么 $\overline{\mathbf{e}_s}, \overline{\mathbf{r}_r} \in \mathbb{R}^{k_w \times k_h} $, 则 $k = k_w  k_h$</p></li><li><p>$\omega$ 代表卷积核；</p></li><li><p>$\mathbf{W}$ 代表投影矩阵；</p></li></ul><h3 id="2-4-2-模型架构"><a href="#2-4-2-模型架构" class="headerlink" title="2.4.2 模型架构"></a>2.4.2 模型架构</h3><p><img src="/images/conve/2.png" alt="ConvE"></p><p><code>ConvE</code>的整个训练过程如下.</p><ul><li>先通过Embedding的方式分别获得头实体表示 $\mathbf{e}_s$ 和关系表示 $\mathbf{r}_r$ ；</li><li>将头实体和关系表示先<code>Concat</code>起来, 然后将其<code>Reshape</code>到某一种尺寸, 此时头实体和关系的表示记为 $\left[ \overline{\mathbf{e}_s} ; \overline{\mathbf{r}_r} \right]$；</li><li>接着利用卷积抽取Reshape后的二维向量, 也就是对头实体和关系的交互信息进行捕捉；</li><li>利用卷积(可以是任意数量的卷积核)抽取完信息后, 将所有的特征打平成一个一维向量；</li><li>通过投影矩阵 $\mathbf{W}$ 投影到一个中间层中，输出的尺寸与embedding size相同，以便与尾实体表示 $\mathbf{e}_o$ 做内积, 获得相似度, 即Logits；</li><li>这种方式通过内积来比较所获向量与尾实体的相似度, 越相似得分越高.</li><li>然后将Logits经过 $\sigma$ 函数, 得到每个实体的概率：  $$p=\sigma(\psi_r\left(\mathbf{e}_s, \mathbf{e}_o \right))$$</li></ul><p>优化时的损失函数采用BCE(binary cross-entropy loss)：</p><p>$$\mathcal{L}(p, t)=-\frac{1}{N} \sum_i\left(t_i \cdot \log \left(p_i \right)+\left(1-t_i \right) \cdot \log \left(1-p_i \right)\right)$$</p><p>$t$ 是尾实体的one-hot vector. 对于和输入的 $(s, r, ?)$ 匹配的位置为1，其余为0.</p><h3 id="2-4-3-训练tips"><a href="#2-4-3-训练tips" class="headerlink" title="2.4.3 训练tips"></a>2.4.3 训练tips</h3><h4 id="2-4-3-1-基本tips"><a href="#2-4-3-1-基本tips" class="headerlink" title="2.4.3.1 基本tips"></a>2.4.3.1 基本tips</h4><ul><li>rectiﬁed linear units: as the non-linearity $f$, 加快训练；</li><li>batch normalization: after each layer to stabilise</li><li>regularise: dropout</li><li>optimiser: Adam</li><li>label smoothing: to lessen overﬁtting due to saturation of output non-linearities at the labels</li></ul><h4 id="2-4-3-2-加速评估tips"><a href="#2-4-3-2-加速评估tips" class="headerlink" title="2.4.3.2 加速评估tips"></a>2.4.3.2 加速评估tips</h4><p>卷积操作消耗大量时间</p><blockquote><p>convolution consumes about 75-90% of the total computation time, thus it is important to minimise the number of convolution operations to speed up computation</p></blockquote><p>思路1：增加batch size加速，但是CNN会使得GPU的内存超过限制；</p><p>解决办法：1-N scoring</p><p>ConvE最后的输出, 能获得对所有实体相关的Logits, 这样就能<strong>对所有的尾实体同时打分</strong>, <strong>而不用考虑采样的问题</strong>. 在原文中这种打分方式被称为<strong>1-N Scoring</strong>.</p><p>过去评估时，需要采样负样本，进行1-1评估。现在这种方式能极大地加快Evaluation的速度, 因为负采样只能对单一的三元组打分, 而这种方式能同时对所有的尾实体同时打分。这种思想能够应用于所有的1-1 scoring Model.</p><p>这种方式其实本质上利用GPU并行执行的特点，在架构上将训练和评估同时考虑，通过将平衡计算性能和收敛速度，来使得评估过程加快。</p><h1 id="3-实验评估"><a href="#3-实验评估" class="headerlink" title="3. 实验评估"></a>3. 实验评估</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><table><thead><tr><th>数据集</th><th>来源</th><th>关系</th><th>实体</th><th>三元组</th><th>说明</th></tr></thead><tbody><tr><td>WN18</td><td>a subset of WordNet</td><td>18</td><td>40943</td><td>151442</td><td>consist of hyponym and hypernym relations and, for such a reason, WN18 tends to follow a strictly hierarchical structure. 用WN18RR替代。</td></tr><tr><td>FB15k</td><td>a subset of Freebase</td><td>1345</td><td>14951</td><td>——</td><td>A large fraction of content in this knowledge graph describes facts about movies, actors, awards, sports, and sport teams. 用FB15k-237替代。</td></tr><tr><td>YAGO3-10</td><td>a subset of YAGO3</td><td>37</td><td>123182</td><td>entities which have a minimum of 10 relations each</td><td>Most of the triples deal with descriptive attributes of people, such as citizenship, gender, and profession.</td></tr><tr><td>Countries</td><td></td><td></td><td></td><td></td><td>a benchmark dataset that is useful to evaluate a model’s ability to learn long-range dependencies between entities and relations. It consists of three sub-tasks which increase in difﬁculty in a step-wise fashion, where the minimum pathlength to ﬁnd a solution increases from 2 to 4.</td></tr></tbody></table><h2 id="3-2-超参"><a href="#3-2-超参" class="headerlink" title="3.2 超参"></a>3.2 超参</h2><ul><li><p>选择方法: <code>grid search</code> according to the <code>mean reciprocal rank (MRR)</code> on the validation set</p></li><li><p>选定范围:</p><ul><li><code>embedding dropout</code>: 0.0, 0.1, 0.2</li><li><code>feature map dropout</code>: 0.0, 0.1, 0.2, 0.3</li><li><code>projection layer dropout</code>: 0.0, 0.1, 0.3, 0.5</li><li><code>embedding size</code>: 100, 200</li><li><code>batch size</code>: 64, 128, 256</li><li><code>learning rate</code>: 0.001, 0.003</li><li><code>label smoothing</code>: 0.0, 0.1, 0.2, 0.3</li></ul></li><li><p>最佳参数:</p><ul><li>WN18, YAGO3-10 and FB15k<ul><li><code>embedding dropout</code>: 0.2</li><li><code>feature map dropout</code>: 0.2</li><li><code>projection layer dropout</code>: 0.3</li><li><code>embedding size</code>: 200</li><li><code>batch size</code>: 128</li><li><code>learning rate</code>: 0.001</li><li><code>label smoothing</code>: 0.1</li></ul></li><li>Countries dataset<ul><li><code>embedding dropout</code>: 0.3</li><li><code>hidden dropout</code>: 0.5</li><li><code>label smoothing</code>: 0</li></ul></li><li><code>early stopping</code> according to the <code>mean reciprocal rank</code> (WN18, FB15k, YAGO3-10) and <code>AUC-PR</code> (Countries) statistics on the validation set</li></ul></li></ul><h2 id="3-3-结果"><a href="#3-3-结果" class="headerlink" title="3.3 结果"></a>3.3 结果</h2><p>实验中进行评估时，有几件注意事项：</p><ul><li><p>由于数据出现leakage，因此使用了rule-based method来识别逆向关系作为对照，同时在数据集包括有无逆向关系)中进行评估；</p></li><li><p>使用了<code>filtered setting</code>;</p><blockquote><p>Rank test triples against all other candidate triples not appearing in the training, validation, or test set.</p></blockquote><blockquote><p>Candidates are obtained by permuting either the subject or the object of a test triple with all entities in the knowledge graph.</p></blockquote></li></ul><h3 id="3-3-1-从参数效率看"><a href="#3-3-1-从参数效率看" class="headerlink" title="3.3.1 从参数效率看"></a>3.3.1 从参数效率看</h3><p><img src="/images/conve/3.png" alt="参数数量和性能(ConvE vs. DistMult)"></p><h3 id="3-3-2-结果-含有逆向关系"><a href="#3-3-2-结果-含有逆向关系" class="headerlink" title="3.3.2 结果(含有逆向关系)"></a>3.3.2 结果(含有逆向关系)</h3><p><img src="/images/conve/4.png" alt="结果(含有逆向关系)"></p><h3 id="3-3-3-结果-不含逆向关系"><a href="#3-3-3-结果-不含逆向关系" class="headerlink" title="3.3.3 结果(不含逆向关系)"></a>3.3.3 结果(不含逆向关系)</h3><p>这里将数据集中存在逆向关系的三元组全部删除了，来避免leakage造成的负面影响。</p><p><img src="/images/conve/5.png" alt="结果(不含逆向关系)1"></p><p><img src="/images/conve/6.png" alt="结果(不含逆向关系)2"></p><h2 id="3-4-分析"><a href="#3-4-分析" class="headerlink" title="3.4 分析"></a>3.4 分析</h2><h3 id="3-4-1-消融实验"><a href="#3-4-1-消融实验" class="headerlink" title="3.4.1 消融实验"></a>3.4.1 消融实验</h3><p>为了查看，哪部分组件在整个架构中的作用最重要，消融实验显示：</p><p><img src="/images/conve/7.png" alt="消融实验结果"></p><ul><li>hidden dropout的影响最大;</li><li>但是label smoothing的影响几乎可以忽略；</li></ul><h3 id="3-4-2-从图的结构分析优点"><a href="#3-4-2-从图的结构分析优点" class="headerlink" title="3.4.2 从图的结构分析优点"></a>3.4.2 从图的结构分析优点</h3><h4 id="3-4-2-1-假设1"><a href="#3-4-2-1-假设1" class="headerlink" title="3.4.2.1 假设1"></a>3.4.2.1 假设1</h4><p>datasets contain nodes with very high relation-speciﬁc indegree时，<code>ConvE</code>效果更好，而indegree较小时，一些模型足以应对<code>DistMult</code>。</p><blockquote><p>Our hypothesis is that deeper models, that is, models that learn multiple layers of features, like ConvE, have an advantage over shallow models, like DistMult, to capture all these constraints.</p></blockquote><ul><li>验证：通过将数据中indegree中的过大或者过小的node删除，然后分别使用ConvE和DistMult进行实验，验证了上述假设。</li></ul><h4 id="3-4-2-2-假设2"><a href="#3-4-2-2-假设2" class="headerlink" title="3.4.2.2 假设2"></a>3.4.2.2 假设2</h4><p>平均PageRank越高的graph，ConvE的性能相比于<code>DistMult</code>越好；</p><blockquote><p>This gives additional evidence that models that are deeper have an advantage when modelling nodes with high (recursive) indegree.</p></blockquote><ul><li>验证：通过计算各个数据集的平均pagerank值，然后计算pagerank值与(convE - DistMult)差值计算相关性，验证了上述假设，这个假设2与假设1其实可以看成等价。</li></ul><h1 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4. 代码实现"></a>4. 代码实现</h1><ul><li><p><a href="https://github.com/TimDettmers/ConvE">原文实现</a></p></li><li><p><a href="https://github.com/Accenture/AmpliGraph">AmpliGraph</a></p></li></ul><h1 id="5-问题"><a href="#5-问题" class="headerlink" title="5. 问题"></a>5. 问题</h1><ol><li>与CV中CNN的应用对比，模型仍是浅层，未来会增加深度的卷积模型。</li><li>2D卷积的解释；</li><li>如何更多地捕捉embedding之间的交互，例如通过增加大型结构；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《美国陷阱》中阿尔斯通被肢解背后的经济战</title>
      <link href="/2021/11/13/mei-guo-xian-jing-zhong-a-er-si-tong-bei-zhi-jie-bei-hou-de-jing-ji-zhan/"/>
      <url>/2021/11/13/mei-guo-xian-jing-zhong-a-er-si-tong-bei-zhi-jie-bei-hou-de-jing-ji-zhan/</url>
      
        <content type="html"><![CDATA[<p>疫情在家，除了完成正常的工作学习外，由于没啥好的娱乐项目，读书是一种廉价的消遣方式。正巧遇到一本书《美国陷阱》，其中的情节和阴谋论恐怕电影都拍不出来，甚至可以当成悬疑小说来读。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/1.png" alt="《美国陷阱》"></p><h1 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1. 背景介绍"></a>1. 背景介绍</h1><p>最近几年，美国加大对中国的打压力度，尤其是科技领域，其中以华为最为大家所熟悉。那么最为人所知伎俩即，扣押孟晚舟（任正非的女儿和华为的高管）。大家肯定跌破眼镜，这种流氓式的手段，对于被称为“世界最大的民主国家”的美国，怎么干得出来？</p><p>当然，理由还是要有的，不然无法说服世界舆论，其实估计也没人信。因为这种套路也不是第一次了，本书讲得就是同样的套路，应用到另一个公司的另一个人身上。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/2.png" alt="两起事件的对比"></p><p>而本书，讲得就是阿尔斯通的故事，而且是由当事人弗雷德里克-皮耶鲁齐自己陈述的。因此对于我们具有极大的参考价值。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/3.png" alt="这位老哥比孟晚舟惨得多"></p><h1 id="2-阿尔斯通与中国"><a href="#2-阿尔斯通与中国" class="headerlink" title="2. 阿尔斯通与中国"></a>2. 阿尔斯通与中国</h1><p>阿尔斯通是法国一家有战略意义的工业巨头，关于它的内容，大家可以在网上搜索，我这里不过多阐述，大家认识到这是一家工业巨头就可以了，法国的核电站都是它来维护的，舰艇的燃气轮机也是它提供的。</p><p>而他与中国的渊源也是颇深，三峡水利工程、向家坝水利工程等好多项目，都有它的参与，<strong>甚至书中还披露，当然是阿尔斯通的认罪协议中，阿尔斯通曾经为参与台北地铁项目，而贿赂相关负责人。</strong></p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/4.png" alt="阿尔斯通"></p><h1 id="3-书中主要内容"><a href="#3-书中主要内容" class="headerlink" title="3. 书中主要内容"></a>3. 书中主要内容</h1><p>为了大家能够明白书中的主要线索，我首先用图的方式表达其中涉及到的参与人和机构。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/5.png" alt="当事人和相关关系"></p><ol><li>首先，美国司法部质控阿尔斯通违反《反腐败法》，逮捕作者，也就是皮耶鲁齐，希望他认罪，在阿尔斯通内部充当“间谍”，以便掌控该企业的内部信息，作者拒绝，被监禁；</li><li>陆续有其他阿尔斯通的高管被逮捕，公司CEO惶恐，希望摆脱诉讼和入狱；</li><li>通用电气介入，摆出了可以帮助阿尔斯通摆脱当前困局的慈善面目，公司CEO为脱罪，达成秘密协议（推测，当事人都矢口否认，但作者强烈暗示这种行为），将阿尔斯通能源部门出售给通用电气（是该公司最有价值的资产）；</li><li>法国政府对这种行为保持暧昧态度，后来知晓，通用电气和阿尔斯通花费将近3亿欧元进行公关，还有一层原因，马克龙（现任法国总统，时任经济部长）相信阿尔斯通CEO与美国司法部、通用电气有交易，但是没有证据；同时受限于大西洋主义以及对美国强大实力的深深恐惧；</li><li>“肢解”交易达成，作者被判入狱30个月。</li></ol><p>从上述过程中，从头到尾，<strong>作者的角色和所处的地位，相较于整个事件，简直微不足道！所以，实际上他当了一个背锅侠和替罪羊！</strong></p><h1 id="4-带给我们的思考和启示"><a href="#4-带给我们的思考和启示" class="headerlink" title="4. 带给我们的思考和启示"></a>4. 带给我们的思考和启示</h1><p><strong>第一点</strong>，战争的形式已经发生变化了，从之前枪炮相加，血肉横飞到网络世界的攻防，可能并没有人死亡，但是损失可能更大。<strong>这本书中提到的有两点很值得注意，一个是舆论公关战，一个就是法律战。</strong></p><p>通用电气和阿尔斯通为促成收购，在媒体和网络上，大肆说明该收购带来的好处，比如增加就业岗位，帮助阿尔斯通摆脱诉讼，同时游说法国政界和企业界的精英。结果，一个曾经在戴高乐将军领导下致力于独立自主的法国居然在外人收购自己的核心资产时，居然沉默是“金”，简直可怕！想想我们这个社会，这样的人会在少数吗？</p><p>法律战，也即本事件的起因，违反《反海外腐败法》。这个法是个什么东西呢？简单来说，美国之外国家的公司，如果使用了与美国相关的工具，则全部收受到该法律的管辖。是不是，有点治外法权的意味呢！比如，在我们国家犯罪，却用美国的法律诉讼，是不是有点滑稽，这种域外法的行使，使得美国的“全球警察”地位坐实了。而且，还让人产生错觉，好像是我错了（虽然在本书中提到的阿尔斯通确实有腐败行为）！</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/6.png" alt="《反海外腐败法》"></p><p><strong>第二点</strong>，我想引述书中的一句话，</p><blockquote><p>我们不能上当受骗。不管谁当美国总统，无论他是民主党人还是共和党人，华盛顿都会维护少数工业巨头的利益</p></blockquote><p>有人曾经，觉得美国民主党上台，对我们可能会温和一点（可能法国人也是这么想的），还是放弃这种幻想吧！这是体制决定的，不是简单的人事变动能够改变的，美国政府和企业巨头的纠缠不断的联系导致“旋转门”的不断上演，所以我们还是做好准备吧！</p><p><strong>第三点</strong>，“人权”和“自由”这种过于抽象的东西，每个人都有自己的理解，比如说那个只喝矿泉水的归国人员，它的解释就为“特权”。<strong>而且每个人的理解都会倾向于自己的利益</strong>，比如作者在狱中的遭遇，美国的监狱的确是没有“人权”，你能相信监狱是私人的，所以当然是需要盈利的，“人权”多费钱啊，与资本主义的核心目标——攫取利益不符啊，当事情与他们的利益不符时，就会有这样的论调。当人权在敌对国家遭到威胁时，这时候美国这个道德楷模就出来了。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/7.png" alt="你品，你细品！"></p><p>第四点，爱国主义，这种东西在人们的脑海中是正面的东西，我也倾向于这样认为。但是有一点在书中提及，声称“自己生于斯，长于斯”的人，向大家彰显自己的爱国举动，真是是所谓的“爱国人士”吗，或许阿尔斯通CEO柏珂龙的话有助于我们思考，</p><blockquote><p>我是法国择优选拔人才体制的产物。借此机会，我想说，我的父母都是移民。自从在阿尔斯通任职后，我在法国创造了将近1.5万个就业岗位，对此我感到非常自豪。每个人都必须为法国就业做出一份贡献。我已经努力做出了微薄的贡献。</p></blockquote><p>不要忘记，就是他私下与通用电气密谋，肢解了自己服务的“工业明珠”！</p><p>在这本书中，还有很多值得人们思考的东西，包括友情、爱情、劳资关系、糖衣炮弹、爱国主义等等，建议大家去看一下，会收获不一样的东西！其中，对个人来说，最想提及的一点是，找一个好妻子真的是很重要的事情，尤其是像这位老哥遭遇监禁这样的遭遇时，更能体现这一点。</p><h1 id="5-Q-amp-A"><a href="#5-Q-amp-A" class="headerlink" title="5. Q&amp;A"></a>5. Q&amp;A</h1><p>看本书时，我一直不明白的有几个问题，后来弄明白了，在这里列出来同大家分享。</p><p><strong>1) 美国为啥可以管理国外的犯罪？怎么管？</strong></p><p>这与美国在国际上巨大的影响力和超强的实力有关，比如：做国际贸易需要美元结算，没有美元的话没办法进行交易，大家伙都不认，这也是我们一直推进人民币国际化的原因之一。因此，你只要涉及到美元交易，就归美国管，因为美元是美国发行的，具体怎么管，国际贸易会有清算，主要的清算中心在美国，通俗点讲，不听话的话，你就没办法收到货款，也没办法进口。这里，顺便提一句，你觉得Facebook的Libra能成功吗？</p><p>还有一点，除了中国外，世界主要的信息技术公司都是美国公司，所以只要你用了Gmail、Facebook等就都和美国有关系了。</p><p>其实，立什么法不重要，重要的是要有执行能力，这是美国厉害的地方。比如，有一天，你说全世界都得听我的，人家以为你有病呢，但是有人不听时，你有办法治人家，这时大家才服你，最起码表面上服你！</p><p><strong>2) 大西洋主义 VS 亲欧洲主义</strong></p><p>简单来讲，大西洋主义，就是西欧和北美互相合作，保护共同的安全和价值观，但是被认为是美国操纵欧洲的工具，尤其是对于盟友，美国的小动作也在不断的情况下。亲欧洲主义，强调欧洲一体化，摆脱美国的影响。</p>]]></content>
      
      
      <categories>
          
          <category> 读书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济 </tag>
            
            <tag> 政治 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
