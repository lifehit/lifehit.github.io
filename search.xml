<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>GAMES101-03：2D Transformation</title>
      <link href="/2022/04/24/games101-03-2d-transformation/"/>
      <url>/2022/04/24/games101-03-2d-transformation/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Transforms-Matrices"><a href="#Linear-Transforms-Matrices" class="headerlink" title="Linear Transforms = Matrices"></a>Linear Transforms = Matrices</h1><h2 id="Scale-Transform"><a href="#Scale-Transform" class="headerlink" title="Scale Transform"></a>Scale Transform</h2><p>变换操作为：</p><p>$$<br>\text{scale}(s_x, s_y) = \left(\begin{array}{ll}s_x &amp; 0 \\ 0 &amp; s_y \end{array}\right)<br>$$</p><p>在点$(x, y)$经过变换后变为$(x^{\prime}, y^{\prime})$，如下：</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=\left(\begin{array}{ll}<br>s_x &amp; 0 \\<br>0 &amp; s_y<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right) = \left(\begin{array}{l}<br>s_xx \\<br>s_yy<br>\end{array}\right)<br>$$</p><h2 id="Reflection-Transform"><a href="#Reflection-Transform" class="headerlink" title="Reflection Transform"></a>Reflection Transform</h2><p>在水平方向上，其实是沿y轴翻转。</p><p>$$<br>x^{\prime} = -x \\<br>y^{\prime} = y<br>$$</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=\left(\begin{array}{ll}<br>-1 &amp; 0 \\<br>0 &amp; 1<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right) = \left(\begin{array}{l}<br>-x \\<br>y<br>\end{array}\right)<br>$$</p><p>在竖直方向上，其实是沿x轴翻转。</p><p>$$<br>x^{\prime} = x \\<br>y^{\prime} = -y<br>$$</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=\left(\begin{array}{ll}<br>1 &amp; 0 \\<br>0 &amp; -1<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right) = \left(\begin{array}{l}<br>x \\<br>-y<br>\end{array}\right)<br>$$</p><h2 id="Shear-Transform"><a href="#Shear-Transform" class="headerlink" title="Shear Transform"></a>Shear Transform</h2><p><img src="/images/games101/03/shear.png" alt="Shear Transform"></p><p>在水平方向上，纵坐标不变化。</p><p>$$<br>x^{\prime} = x + ay \\<br>y^{\prime} = y<br>$$</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=\left(\begin{array}{ll}<br>1 &amp; a \\<br>0 &amp; 1<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right) = \left(\begin{array}{l}<br>x + ay \\<br>y<br>\end{array}\right)<br>$$</p><p>在竖直方向上，横坐标不变化。</p><p>$$<br>x^{\prime} = x \\<br>y^{\prime} = y + ax<br>$$</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=<br>\left(\begin{array}{ll}<br>1 &amp; 0 \\<br>a &amp; 1<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right) = \left(\begin{array}{l}<br>x \\<br>y + ax<br>\end{array}\right)<br>$$</p><h2 id="Rotate-Transform"><a href="#Rotate-Transform" class="headerlink" title="Rotate Transform"></a>Rotate Transform</h2><p>默认规则：</p><ul><li>about the origin (0, 0)：围绕原点旋转；</li><li>CCW by default：默认是逆时针旋转；</li></ul><p><img src="/images/games101/03/rotate.png" alt="Rotate Transform"></p><p>取图中两个特殊点计算旋转矩阵，得到：</p><p>$$<br>\mathbf{R}_{\theta} = \left(\begin{array}{ll}<br>\cos\theta &amp; -\sin\theta \\<br>\sin\theta &amp; \cos\theta<br>\end{array}\right)<br>$$</p><h2 id="Linear-Transform总结"><a href="#Linear-Transform总结" class="headerlink" title="Linear Transform总结"></a>Linear Transform总结</h2><p>其中对应矩阵的维度与点的维度保持一致。</p><p>$$<br>x^{\prime} = ax + by \\<br>y^{\prime} = cx + dy<br>$$</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=\left(\begin{array}{ll}<br>a &amp; b \\<br>c &amp; d<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right) = \left(\begin{array}{l}<br>ax + by \\<br>cx + dy<br>\end{array}\right)<br>$$</p><p>$$<br>x^{\prime} = Mx<br>$$</p><h1 id="Homogeneous-coordinates"><a href="#Homogeneous-coordinates" class="headerlink" title="Homogeneous coordinates"></a>Homogeneous coordinates</h1><h2 id="为啥要用齐次坐标？"><a href="#为啥要用齐次坐标？" class="headerlink" title="为啥要用齐次坐标？"></a>为啥要用齐次坐标？</h2><p>要解决平移变换的问题，将之与之前的变换统一表示，因为不能将平移变换写成矩阵乘法的形式。</p><h2 id="Translation-Transform"><a href="#Translation-Transform" class="headerlink" title="Translation Transform"></a>Translation Transform</h2><p><img src="/images/games101/03/translation.png" alt="Translation Transform"></p><p>$$<br>x^{\prime} = x + t_x \\<br>y^{\prime} = y + t_y<br>$$</p><p>Translation cannot be represented in matrix form. 不是线性变换。</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)=<br>\left(\begin{array}{ll}<br>a &amp; b \\<br>c &amp; d<br>\end{array}\right)<br>\left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right)<br>+<br>\left(\begin{array}{l}<br>t_x \\<br>t_y<br>\end{array}\right)<br>$$</p><p>由于不想将该变换特殊处理，为了统一表示，因此需要齐次坐标。</p><h2 id="Solution-Homogenous-Coordinates"><a href="#Solution-Homogenous-Coordinates" class="headerlink" title="Solution: Homogenous Coordinates"></a>Solution: Homogenous Coordinates</h2><p>Add a third coordinate (w-coordinate)</p><ul><li>2D point = $(x, y, 1)^T$</li><li>2D vector = $(x, y, 0)^T$</li></ul><blockquote><p>为啥点和向量在增加的坐标上的表示分别为1和0，而不是相同呢？因为向量具有平移不变性，而点经过平移则会发生变化。</p></blockquote><p>Valid operation if w-coordinate of result is 1 or 0：</p><ul><li>vector + vector = vector</li><li>point – point = vector</li><li>point + vector = point</li><li>point + point = ??</li></ul><p>In homogeneous coordinates,</p><p>$$<br>\left(\begin{array}{l}<br>x \\<br>y \\<br>w<br>\end{array}\right)<br>$$<br>is the 2D point<br>$$<br>\left(\begin{array}{l}<br>x/w \\<br>y/w \\<br>1<br>\end{array}\right)<br>$$</p><p>$$w \ne 0. $$</p><p>在该扩充定义下，齐次坐标中，两点之间的加和结果是两个点的中点，还是一个点。</p><blockquote><p>齐次坐标的代价：引入了多余的数字，理论上会增加存储，但是实际中可以不管。</p></blockquote><h2 id="Translation在齐次坐标下的表示"><a href="#Translation在齐次坐标下的表示" class="headerlink" title="Translation在齐次坐标下的表示"></a>Translation在齐次坐标下的表示</h2><p>使用该方法表示Translation得到：</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime} \\<br>w^{\prime}<br>\end{array}\right)=<br>\left(\begin{array}{lll}<br>1 &amp; 0 &amp; t_x \\<br>0 &amp; 1 &amp; t_y \\<br>0 &amp; 0 &amp; 1<br>\end{array}\right) \left(\begin{array}{l}<br>x \\<br>y \\<br>1<br>\end{array}\right)<br>= \left(\begin{array}{l}<br>x + t_x \\<br>y + t_y \\<br>1<br>\end{array}\right)<br>$$</p><p>从而实现转化为矩阵乘法的形式。</p><h2 id="Affine-Transform仿射变换"><a href="#Affine-Transform仿射变换" class="headerlink" title="Affine Transform仿射变换"></a>Affine Transform仿射变换</h2><p>Affine map = linear map + translation</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime}<br>\end{array}\right)<br>=<br>\left(\begin{array}{ll}<br>a &amp; b \\<br>c &amp; d<br>\end{array}\right)<br>\left(\begin{array}{l}<br>x \\<br>y<br>\end{array}\right)<br>+<br>\left(\begin{array}{l}<br>t_x \\<br>t_y<br>\end{array}\right)<br>$$</p><p>在齐次坐标下表示如下：</p><p>$$<br>\left(\begin{array}{l}<br>x^{\prime} \\<br>y^{\prime} \\<br>1<br>\end{array}\right)<br>=<br>\left(\begin{array}{lll}<br>a &amp; b &amp; t_x \\<br>c &amp; d &amp; t_y \\<br>0 &amp; 0 &amp; 1<br>\end{array}\right) \cdot<br>\left(\begin{array}{l}<br>x \\<br>y \\<br>1<br>\end{array}\right)<br>$$</p><h2 id="齐次坐标下统一表示"><a href="#齐次坐标下统一表示" class="headerlink" title="齐次坐标下统一表示"></a>齐次坐标下统一表示</h2><h3 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h3><p>$$<br>\mathbf{S}(s_x, s_y) =<br>\left(\begin{array}{ll}<br>s_x &amp; 0 &amp; 0 \\<br>0 &amp; s_y &amp; 0 \\<br>0&amp;0&amp; 1<br>\end{array}\right)<br>$$</p><h3 id="Rotation"><a href="#Rotation" class="headerlink" title="Rotation"></a>Rotation</h3><p>$$<br>\mathbf{R}(\theta) = \left(\begin{array}{ll}<br>\cos\theta &amp; -\sin\theta &amp; 0 \\<br>\sin\theta &amp; \cos\theta &amp; 0 \\<br>0 &amp; 0 &amp; 1<br>\end{array}\right)<br>$$</p><h3 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h3><p>$$<br>\mathbf{T}(t_x, t_y) =<br>\left(\begin{array}{lll}<br>1 &amp; 0 &amp; t_x \\<br>0 &amp; 1 &amp; t_y \\<br>0 &amp; 0 &amp; 1<br>\end{array}\right)<br>$$</p><h1 id="Inverse-Transform"><a href="#Inverse-Transform" class="headerlink" title="Inverse Transform"></a>Inverse Transform</h1><p>逆变换 $M$ 在数学上等价于乘上逆矩阵。$M^{-1}$ is the inverse of transform in both a matrix and geometric sense。</p><p><img src="/images/games101/03/inverse.png" alt="Inverse Transform"></p><h1 id="Composite-Transform"><a href="#Composite-Transform" class="headerlink" title="Composite Transform"></a>Composite Transform</h1><h2 id="具体实践"><a href="#具体实践" class="headerlink" title="具体实践"></a>具体实践</h2><p>在组合不同的变换时，注意：Transform Ordering Matters。</p><p><img src="/images/games101/03/compose_ex.png" alt="Composite Example"></p><p>如何理解？</p><p>Matrix multiplication is not commutative.</p><p>$$<br>R_{45} \cdot T_{(1,0)} \ne T_{(1,0)} \cdot  R_{45}<br>$$</p><p>Note that matrices are applied right to left:</p><p>$$<br>T_{(1,0)} \cdot R_{45}<br>\left(\begin{array}{l}x \\ y<br>\\<br>1<br>\end{array}<br>\right)<br>=<br>\left(\begin{array}{lll}1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}<br>\right)<br>\left(<br>\begin{array}{ccc}\cos 45^{\circ} &amp; -\sin 45^{\circ} &amp; 0<br>\\<br>\sin 45^{\circ} &amp; \cos 45^{\circ} &amp; 0 \\<br>0 &amp; 0 &amp; 1\end{array}<br>\right)<br>\left(\begin{array}{l}x \\ y \\ 1<br>\end{array}\right)<br>$$</p><p>对于矩阵乘法来说，当考虑到变换时，是<strong>从右到左</strong>依次起作用的。</p><h2 id="推广"><a href="#推广" class="headerlink" title="推广"></a>推广</h2><p>Sequence of affine transforms $A_1, A_2, A_3,$ </p><p>Compose by matrix multiplication将所有的变换结合起来形成一个变换矩阵然后再统一与原始点相乘。</p><p><img src="/images/games101/03/compose.png" alt="Compose Transform"></p><p>这一技巧对于性能计算很重要，Very important for performance。</p><h1 id="Decomposing-Complex-Transforms"><a href="#Decomposing-Complex-Transforms" class="headerlink" title="Decomposing Complex Transforms"></a>Decomposing Complex Transforms</h1><h2 id="例子：沿着特定点旋转"><a href="#例子：沿着特定点旋转" class="headerlink" title="例子：沿着特定点旋转"></a>例子：沿着特定点旋转</h2><p>How to rotate around a given point c?</p><ol><li>Translate center to origin</li><li>Rotate</li><li>Translate back</li></ol><p><img src="/images/games101/03/decompose.png" alt="Decompose Transforms"></p><p>矩阵形式如下：（从右到左依次写）</p><p>$$<br>\mathbf{T}(c) \cdot \mathbf{R}(\alpha) \cdot \mathbf{T}(-c)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-02：Review of Linear Algebra</title>
      <link href="/2022/04/24/games101-02-review-of-linear-algebra/"/>
      <url>/2022/04/24/games101-02-review-of-linear-algebra/</url>
      
        <content type="html"><![CDATA[<h1 id="计算机图形学的预备知识"><a href="#计算机图形学的预备知识" class="headerlink" title="计算机图形学的预备知识"></a>计算机图形学的预备知识</h1><p>本节主要是复习了计算机图形学中可能用到的线性代数知识，其实CG中也会用到其他一些知识，包括但不限于：</p><h2 id="Basic-mathematics"><a href="#Basic-mathematics" class="headerlink" title="Basic mathematics"></a>Basic mathematics</h2><ul><li>Linear algebra</li><li>Calculus</li><li>Statistics</li></ul><h2 id="Basic-physics"><a href="#Basic-physics" class="headerlink" title="Basic physics"></a>Basic physics</h2><ul><li>Optics</li><li>Mechanics</li></ul><h2 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h2><ul><li>Signal processing</li><li>Numerical analysis</li></ul><h2 id="And-a-bit-of-aesthetics"><a href="#And-a-bit-of-aesthetics" class="headerlink" title="And a bit of aesthetics"></a>And a bit of aesthetics</h2><h1 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li>Usually written as $\vec{a}$ or in bold $\textbf{a}$</li><li>Or using start and end points</li><li>Direction and length</li><li>No absolute starting position</li></ul><h2 id="Vector-Normalization"><a href="#Vector-Normalization" class="headerlink" title="Vector Normalization"></a>Vector Normalization</h2><ul><li>Magnitude (length) of a vector written as $|\vec{a}|$</li><li>Unit vector<ul><li>A vector with magnitude of 1</li><li>Finding the unit vector of a vector (normalization): $\hat{a}=\vec{a} /|\vec{a}|$</li><li>Used to represent directions</li></ul></li></ul><h2 id="Vector-Addition"><a href="#Vector-Addition" class="headerlink" title="Vector Addition"></a>Vector Addition</h2><ul><li>Geometrically: Parallelogram law &amp; Triangle law</li><li>Algebraically: Simply add coordinates</li></ul><h2 id="Cartesian-Coordinates"><a href="#Cartesian-Coordinates" class="headerlink" title="Cartesian Coordinates"></a>Cartesian Coordinates</h2><ul><li>X and Y can be any (usually orthogonal unit) vectors  $$<br>  \mathbf{A}=\left(\begin{array}{l}x \\ y \end{array}\right) \quad \mathbf{A}^{T}=(x, y) \quad|\mathbf{A}|=\sqrt{x^{2}+y^{2}}<br>  $$  默认向量是列向量。  <img src="/images/games101/02/cc.png" alt="Cartesian Coordinates"></li></ul><h2 id="Vector-Multiplication"><a href="#Vector-Multiplication" class="headerlink" title="Vector Multiplication"></a>Vector Multiplication</h2><h3 id="Dot-scalar-Product"><a href="#Dot-scalar-Product" class="headerlink" title="Dot (scalar) Product"></a>Dot (scalar) Product</h3><p><img src="/images/games101/02/dp.png" alt="Dot Product"></p><ul><li>定义  $$<br>  \vec{a} \cdot \vec{b}=|\vec{a}||\vec{b}| \cos \theta<br>  $$  $$<br>  \cos \theta=\frac{\vec{a} \cdot \vec{b}}{|\vec{a}||\vec{b}|}<br>  $$</li><li>运算法则：  $$<br>  \begin{array}{l} \vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a} \\<br>  \vec{a} \cdot(\vec{b}+\vec{c})=\vec{a} \cdot \vec{b}+\vec{a} \cdot \vec{c} \\  (k \vec{a}) \cdot \vec{b}=\vec{a} \cdot(k \vec{b})=k(\vec{a} \cdot \vec{b})\end{array}<br>  $$</li><li>Dot Product in Cartesian Coordinates  Component-wise multiplication, then adding up<ul><li>In 2D  $$<br>  \vec{a} \cdot \vec{b}=\left(\begin{array}{l}x_{a} \\ y_{a}\end{array}\right) \cdot\left(\begin{array}{l}x_{b} \\ y_{b}\end{array}\right)=x_{a} x_{b}+y_{a} y_{b}<br>  $$</li><li>In 3D  $$<br>  \vec{a} \cdot \vec{b}=\left(\begin{array}{c}x_{a} \\ y_{a} \\ z_{a}\end{array}\right) \cdot\left(\begin{array}{c}x_{b} \\ y_{b} \\ z_{b}\end{array}\right)=x_{a} x_{b}+y_{a} y_{b}+z_{a} z_{b}<br>  $$</li></ul></li></ul><h3 id="Dot-Product-in-Graphics"><a href="#Dot-Product-in-Graphics" class="headerlink" title="Dot Product in Graphics"></a>Dot Product in Graphics</h3><ul><li>Find angle between two vectors, (e.g. cosine of angle between light source and surface)</li><li>Finding projection of one vector on another  <img src="/images/games101/02/project.png" alt="投影">  $\vec{b}_{\perp}$:  projection of  $\vec{b}$  onto  $\vec{a}$<ul><li>$\vec{b}_{\perp}$ must be along $\vec{a}$ (or along $\hat{a}$ )</li><li>$\vec{b}_{\perp}=k \hat{a}$</li><li>What’s its magnitude $k$?</li><li>$k=|\vec{b}_{\perp}|=|\vec{b}| \cos \theta$</li></ul></li><li>投影可以用来：<ul><li>Measure how close two directions are</li><li>Decompose a vector</li><li>Determine forward / backward：明确两个向量的方向性，同向、反向、垂直；  <img src="/images/games101/02/fb.png" alt="判断forward/backward"></li></ul></li></ul><h3 id="Cross-vector-Product"><a href="#Cross-vector-Product" class="headerlink" title="Cross (vector) Product"></a>Cross (vector) Product</h3><p><img src="/images/games101/02/cp.png" alt="Cross Product"></p><ul><li>Cross product is orthogonal to two initial vectors：结果为向量，不是数字；</li><li>Direction determined by right-hand rule</li><li>Useful in constructing coordinate systems：构建的称为右手坐标系；</li></ul><p>$$<br>\begin{array}{l}\vec{x} \times \vec{y}=+\vec{z} \\<br>\vec{y} \times \vec{x}=-\vec{z} \\<br>\vec{y} \times \vec{z}=+\vec{x} \\<br>\vec{z} \times \vec{y}=-\vec{x} \\<br>\vec{z} \times \vec{x}=+\vec{y} \\<br>\vec{x} \times \vec{z}=-\vec{y}\end{array}<br>$$</p><ul><li>运算性质  $$<br>  \begin{array}{c}\vec{a} \times \vec{b}=-\vec{b} \times \vec{a} \\<br>  \vec{a} \times \vec{a}=\overrightarrow{0} \\<br>  \vec{a} \times(\vec{b}+\vec{c})=\vec{a} \times \vec{b}+\vec{a} \times \vec{c} \\<br>  \vec{a} \times(k \vec{b})=k(\vec{a} \times \vec{b})\end{array}<br>  $$</li><li>Cross Product: Cartesian Formula  $$<br>  \vec{a} \times \vec{b}=\left(\begin{array}{l}y_{a} z_{b}-y_{b} z_{a} \\<br>  z_{a} x_{b}-x_{a} z_{b} \\<br>  x_{a} y_{b}-y_{a} x_{b}\end{array}\right)<br>  $$</li></ul><h3 id="Cross-Product-in-Graphics"><a href="#Cross-Product-in-Graphics" class="headerlink" title="Cross Product in Graphics"></a>Cross Product in Graphics</h3><ul><li>Determine left / right：$\vec{a} \times \vec{b} &gt; 0$，说明 $\vec{b}$ 在 $\vec{a}$ 的左边；反之在右边；  <img src="/images/games101/02/lr.png" alt="判断左/右"></li><li>Determine inside / outside：当$p$点，与三角形各个顶点的连线，$\overrightarrow{AP}$、$\overrightarrow{BP}$、$\overrightarrow{CP}$分别与$\overrightarrow{AB}$、$\overrightarrow{BC}$、$\overrightarrow{CA}$计算Cross Product，如果p点均在三个向量的左侧或者右侧，则该点在三角形内部，若落在边界上可自由处理。  <img src="/images/games101/02/io.png" alt="判断内/外"></li></ul><h3 id="Orthonormal-bases-and-coordinate-frames"><a href="#Orthonormal-bases-and-coordinate-frames" class="headerlink" title="Orthonormal bases and coordinate frames"></a>Orthonormal bases and coordinate frames</h3><ul><li>Important for representing points, positions, locations</li><li>Often, many sets of coordinate systems<ul><li>Global, local, world, model, parts of model (head, hands, …)</li></ul></li><li>Critical issue is transforming between these systems/bases</li></ul><p>有助于将向量进行分解，例如对于右手坐标系：</p><p>Any set of 3 vectors (in 3D) that</p><p>$$<br>\begin{array}{l}|\vec{u}|=|\vec{v}|=|\vec{w}|=1 \\<br>\vec{u} \cdot \vec{v}=\vec{v} \cdot \vec{w}=\vec{u} \cdot \vec{w}=0 \\<br>\vec{w}=\vec{u} \times \vec{v} \quad \text { (right-handed) } \\<br>\vec{p}=(\vec{p} \cdot \vec{u}) \vec{u}+(\vec{p} \cdot \vec{v}) \vec{v}+(\vec{p} \cdot \vec{w}) \vec{w} \\<br>\text { (projection) }\end{array}<br>$$</p><h1 id="Matrices"><a href="#Matrices" class="headerlink" title="Matrices"></a>Matrices</h1><ul><li>In Graphics, pervasively used to represent transformations</li><li>Array of numbers (m × n = m rows, n columns)  $$<br>  \left(\begin{array}{ll}1 &amp; 3 \\ 5 &amp; 2 \\ 0 &amp; 4 \end{array}\right)<br>  $$</li><li>Addition and multiplication by a scalar are trivial: element by element</li></ul><h2 id="Matrix-Matrix-Multiplication"><a href="#Matrix-Matrix-Multiplication" class="headerlink" title="Matrix-Matrix Multiplication"></a>Matrix-Matrix Multiplication</h2><ul><li>(number of) columns in A must = # rows in B</li><li>(M x N) (N x P) = (M x P)  $$<br>  \left(\begin{array}{ll}1 &amp; 3 \\ 5 &amp; 2 \\ 0 &amp; 4\end{array}\right)\left(\begin{array}{llll}3 &amp; 6 &amp; 9 &amp; 4 \\ 2 &amp; 7 &amp; 8 &amp; 3\end{array}\right)<br>  $$</li><li>Element (i, j) in the product is the dot product of row i from A and column j from B</li><li>运算规则<ul><li>Non-commutative: (AB and BA are different in general)</li><li>Associative and distributive<ul><li>(AB)C=A(BC)</li><li>A(B+C) = AB + AC</li><li>(A+B)C = AC + BC</li></ul></li></ul></li></ul><h2 id="Matrix-Vector-Multiplication"><a href="#Matrix-Vector-Multiplication" class="headerlink" title="Matrix-Vector Multiplication"></a>Matrix-Vector Multiplication</h2><p>Treat vector as a column matrix (m×1)</p><p>$$<br>\left(\begin{array}{cc}-1 &amp; 0 \\ 0 &amp; 1\end{array}\right)\left(\begin{array}{l}x \\ y \end{array}\right)=\left(\begin{array}{c}-x \\ y \end{array}\right)<br>$$</p><h2 id="Transpose-of-a-Matrix"><a href="#Transpose-of-a-Matrix" class="headerlink" title="Transpose of a Matrix"></a>Transpose of a Matrix</h2><p>Switch rows and columns (ij -&gt; ji)</p><p>$$<br>\left(\begin{array}{ll}1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6\end{array}\right)^{T}=\left(\begin{array}{lll}1 &amp; 3 &amp; 5 \\ 2 &amp; 4 &amp; 6\end{array}\right)<br>$$</p><p>运算性质：</p><p>$$<br>(A B)^{T}=B^{T} A^{T}<br>$$</p><h2 id="Identity-Matrix-and-Inverses"><a href="#Identity-Matrix-and-Inverses" class="headerlink" title="Identity Matrix and Inverses"></a>Identity Matrix and Inverses</h2><p>$$<br>I_{3 \times 3}=\left(\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)<br>$$</p><p>$$<br>\begin{array}{l}A A^{-1}=A^{-1} A=I \\ (A B)^{-1}=B^{-1} A^{-1}\end{array}<br>$$</p><h2 id="Vector-multiplication-in-Matrix-form"><a href="#Vector-multiplication-in-Matrix-form" class="headerlink" title="Vector multiplication in Matrix form"></a>Vector multiplication in Matrix form</h2><h3 id="Dot-product"><a href="#Dot-product" class="headerlink" title="Dot product"></a>Dot product</h3><p>$$<br>\begin{aligned}\vec{a} \cdot \vec{b} \\ =&amp;\vec{a}^{T} \vec{b} \\ =&amp;\left(\begin{array}{lll}x_{a} &amp; y_{a} &amp; z_{a}\end{array}\right)\left(\begin{array}{c}x_{b} \\ y_{b} \\ z_{b}\end{array}\right)=\left(x_{a} x_{b}+y_{a} y_{b}+z_{a} z_{b}\right)\end{aligned}<br>$$</p><h3 id="Cross-product"><a href="#Cross-product" class="headerlink" title="Cross product"></a>Cross product</h3><p>$$<br>\vec{a} \times \vec{b}=A^{*} b=\left(\begin{array}{ccc}0 &amp; -z_{a} &amp; y_{a} \\ z_{a} &amp; 0 &amp; -x_{a} \\ -y_{a} &amp; x_{a} &amp; 0\end{array}\right)\left(\begin{array}{l}x_{b} \\ y_{b} \\ z_{b}\end{array}\right)<br>$$</p><p>A: dual matrix of vector a</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-01：Overview of Computer Graphics</title>
      <link href="/2022/04/24/games101-01-overview-of-computer-graphics/"/>
      <url>/2022/04/24/games101-01-overview-of-computer-graphics/</url>
      
        <content type="html"><![CDATA[<h2 id="What-is-Computer-Graphics"><a href="#What-is-Computer-Graphics" class="headerlink" title="What is Computer Graphics"></a>What is Computer Graphics</h2><blockquote><p>The use of computers to <strong>synthesize</strong> and <strong>manipulate</strong> visual information.</p></blockquote><p>计算机图形学不等于openGL，openGL只是实现计算机图形学操作的API。</p><h2 id="与Computer-Vision的区别"><a href="#与Computer-Vision的区别" class="headerlink" title="与Computer Vision的区别"></a>与Computer Vision的区别</h2><p>一切涉及需要<strong>猜测</strong>的研究都属于计算机视觉的范畴，需要分析和理解。</p><p><img src="/images/games101/01/diff.png" alt="计算机图形学与计算机视觉的区别"></p><p>参考资料：</p><ul><li><a href="https://www.tutorialspoint.com/dip/computer_vision_and_graphics.htm">Computer Vision and Computer Graphics</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-the-fields-of-Computer-Graphics-and-Computer-Vision">What is the difference between the fields of Computer Graphics and Computer Vision?</a></li></ul><h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><h3 id="Video-Games"><a href="#Video-Games" class="headerlink" title="Video Games"></a>Video Games</h3><ul><li>从技术角度来说，什么是好画面？<ul><li>画面是否足够亮——全局光照；</li></ul></li></ul><h3 id="Movies"><a href="#Movies" class="headerlink" title="Movies"></a>Movies</h3><ul><li>特效：special effects</li><li>特效是最简单的图形学应用，因为少见，或者缺乏直接的体验；</li><li>最难的是模拟真实的东西；</li></ul><h3 id="Animations"><a href="#Animations" class="headerlink" title="Animations"></a>Animations</h3><h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><ul><li>CAD: Computer-aided Design，宜家家居设计；</li></ul><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><h3 id="Virtual-Reality"><a href="#Virtual-Reality" class="headerlink" title="Virtual Reality"></a>Virtual Reality</h3><p>经常与增强现实 Augmented Reality共同出现。</p><ul><li>两者的区别<ul><li>VR：看不到现实；</li><li>AR：看到现实东西，但是还有一些新东西加入；</li></ul></li></ul><h3 id="Digital-illustration"><a href="#Digital-illustration" class="headerlink" title="Digital illustration"></a>Digital illustration</h3><h3 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h3><h3 id="Graphical-User-Interfaces"><a href="#Graphical-User-Interfaces" class="headerlink" title="Graphical User Interfaces"></a>Graphical User Interfaces</h3><h3 id="Typography"><a href="#Typography" class="headerlink" title="Typography"></a>Typography</h3><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><h3 id="Fundamental-intellectual-challenges"><a href="#Fundamental-intellectual-challenges" class="headerlink" title="Fundamental intellectual challenges"></a>Fundamental intellectual challenges</h3><ol><li>Creates and interacts with realistic virtual world;</li><li>Requires understanding of all aspects of physical world;</li><li>New computing methods, displays, technologies;</li></ol><h3 id="Technical-challenges"><a href="#Technical-challenges" class="headerlink" title="Technical challenges"></a>Technical challenges</h3><ol><li>Math of (perspective) projections, curves, surfaces;</li><li>Physics of lighting and shading;</li><li>Representing / operating shapes in 3D;</li><li>Animation / simulation;</li><li>3D graphics software programming and hardware;</li></ol><h2 id="主要研究内容"><a href="#主要研究内容" class="headerlink" title="主要研究内容"></a>主要研究内容</h2><h3 id="Rasterization"><a href="#Rasterization" class="headerlink" title="Rasterization"></a>Rasterization</h3><ul><li>Project geometry primitives (3D triangles / polygons) onto the screen;</li><li>Break projected primitives into fragments (pixels);</li><li>Gold standard in Video Games (Real-time Applications)  将3D形体显示在屏幕上，是所有游戏，即<strong>实时（每秒 30帧图片，30fps，否则叫离线）计算机图形学</strong>的主要应用。</li></ul><h3 id="Curves-and-Meshes"><a href="#Curves-and-Meshes" class="headerlink" title="Curves and Meshes"></a>Curves and Meshes</h3><p>How to represent geometry in Computer Graphics.</p><h3 id="Ray-Tracing"><a href="#Ray-Tracing" class="headerlink" title="Ray Tracing"></a>Ray Tracing</h3><ul><li>Shoot rays from the camera though each pixel<ul><li>Calculate intersection and shading;</li><li>Continue to bounce the rays till they hit light sources;</li></ul></li><li>Gold standard in Animations / Movies (Offline Applications)</li></ul><h3 id="Animation-Simulation"><a href="#Animation-Simulation" class="headerlink" title="Animation / Simulation"></a>Animation / Simulation</h3><ul><li>Key frame Animation</li><li>Mass-spring System</li></ul>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何入门计算机图形学：一份checklist</title>
      <link href="/2022/04/24/ru-he-ru-men-ji-suan-ji-tu-xing-xue-yi-fen-checklist/"/>
      <url>/2022/04/24/ru-he-ru-men-ji-suan-ji-tu-xing-xue-yi-fen-checklist/</url>
      
        <content type="html"><![CDATA[<h1 id="太长不看版"><a href="#太长不看版" class="headerlink" title="太长不看版"></a>太长不看版</h1><h2 id="不要"><a href="#不要" class="headerlink" title="不要"></a>不要</h2><ul><li>不要关注图形API：OpenGL、DirectX、Vulkan等只是API，而原理相同；</li><li>不要关注GPU还是CPU：它们只是效率不同；</li><li>不要关注游戏编程类：比如UE引擎等，这些只是上层的应用；</li><li>不要关注没有代码的论文：CG偏应用，单纯入门没必要；</li></ul><h2 id="要"><a href="#要" class="headerlink" title="要"></a>要</h2><ul><li>上课以建立理论脉络：CG一般包含Rendering/Geometry/Simulation</li><li>多写代码：CG偏应用，代码蕴含细节；</li><li>要边理论边实践编程；</li></ul><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><ul><li>从头造轮子：便于了解细节；</li><li>拆解小项目：明白工业级项目，学习其他人的经验；</li></ul><h1 id="技能要求"><a href="#技能要求" class="headerlink" title="技能要求"></a>技能要求</h1><h2 id="编程技能"><a href="#编程技能" class="headerlink" title="编程技能"></a>编程技能</h2><ul><li>C/C++：参考C++ Primer这本书，没有plus；</li></ul><h2 id="理论技能"><a href="#理论技能" class="headerlink" title="理论技能"></a>理论技能</h2><ul><li>微积分</li><li>线性代数<ul><li><a href="https://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra, 5th Edition</a></li><li><a href="http://immersivemath.com/ila/index.html">Immersive Math</a></li></ul></li></ul><h1 id="领域内容"><a href="#领域内容" class="headerlink" title="领域内容"></a>领域内容</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p>主要是建立对CG的整体认识，不要求多么深入，要求结构清晰，方便快速入门。</p><ul><li>书籍<ul><li>Fundamentals of Computer Graphics, Fourth Edition</li></ul></li><li>课程<ul><li><a href="http://games-cn.org/intro-graphics/">GAMES101:现代计算机图形学入门</a></li></ul></li></ul><h2 id="实时渲染"><a href="#实时渲染" class="headerlink" title="实时渲染"></a>实时渲染</h2><ul><li>书籍：Real-Time Rendering, Fourth Edition</li><li>课程<ul><li><a href="http://games-cn.org/games202/">Games202:高质量实时渲染 - 计算机图形学与混合现实研讨会</a></li></ul></li></ul><h2 id="几何"><a href="#几何" class="headerlink" title="几何"></a>几何</h2><ul><li><a href="https://www.bilibili.com/video/BV1B54y1B7Uc">数字几何处理-中国科学技术大学-傅孝明_哔哩哔哩_bilibili</a></li></ul><h2 id="动画"><a href="#动画" class="headerlink" title="动画"></a>动画</h2><ul><li><a href="https://www.bilibili.com/video/BV1ZK411H7Hc?from=search&seid=5195290021208890151">GAMES201：高级物理引擎实战指南2020_哔哩哔哩_bilibili</a></li></ul><h1 id="拆解项目"><a href="#拆解项目" class="headerlink" title="拆解项目"></a>拆解项目</h1><p>这些项目可用于拆解练习。</p><ul><li><a href="https://github.com/ssloy/tinyrenderer">软件光栅化渲染器——tinyrenderer</a></li><li><a href="https://github.com/skywind3000/mini3d">软件光栅化渲染器——mini3d</a></li><li><a href="https://github.com/topics/box2d-lite">物理引擎——box2d-lite</a></li><li><a href="https://github.com/mitsuba-renderer/mitsuba2">渲染系统——mitsuba2渲染器</a></li></ul><h1 id="其他技术资源"><a href="#其他技术资源" class="headerlink" title="其他技术资源"></a>其他技术资源</h1><h2 id="光栅技术"><a href="#光栅技术" class="headerlink" title="光栅技术"></a>光栅技术</h2><p>Peter Shirley（Fundamentals of Computer Graphics作者）的一系列关于光线追踪的文章<a href="https://raytracing.github.io/">Ray Tracing in One Weekend Series</a>。</p><ul><li><strong>Ray Tracing in One Weekend</strong></li><li><strong>Ray Tracing: The Next Week</strong></li><li><strong>Ray Tracing: The Rest of Your Life</strong></li></ul><h2 id="参考百科全书——PBR"><a href="#参考百科全书——PBR" class="headerlink" title="参考百科全书——PBR"></a>参考百科全书——PBR</h2><p><a href="https://www.pbrt.org/">Physically Based Rendering: From Theory to Implementation</a></p><p>该参考书还配有开源的<a href="https://github.com/mmp/pbrt-v3">工业级渲染器</a>以供使用和学习。</p><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><ol><li>学习GAMES-101，并学习C++实践+做作业；</li><li>利用Fundamentals of Computer Graphic作为补充内容；</li><li>在对应章节中，通过拆解项目和资源来深度理解知识；<ol><li>光追技术；</li><li>光栅化渲染；</li></ol></li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.zhihu.com/question/41468803">零基础如何学习计算机图形学？</a></p><p><a href="https://www.scratchapixel.com/">Scratchapixel</a></p><p><a href="https://www.zhihu.com/column/game-programming">浅墨的游戏编程</a></p><p><a href="https://developer.nvidia.com/gpugems/gpugems/contributors">Contributors</a></p><p><a href="https://learnopengl.com/Introduction">Introduction</a></p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022年开年返校被隔离了</title>
      <link href="/2022/02/09/2022-nian-kai-nian-fan-xiao-bei-ge-chi-liao/"/>
      <url>/2022/02/09/2022-nian-kai-nian-fan-xiao-bei-ge-chi-liao/</url>
      
        <content type="html"><![CDATA[<p>昨天返校了，本来有一个论文意见需要回复一下，今天一上午在回复论文的意见，准备今天完成意见回复后，就继续自己的毕业论文撰写，但是，天有不测风云，生活总是会跟你开一个玩笑。总是怕你太无聊，给你加入一些调味品，让你的生活充满丰富的色彩。</p><h1 id="诧异"><a href="#诧异" class="headerlink" title="诧异"></a>诧异</h1><p>中午收到一个短信，说我的杭州健康码变成了黄色，当时我一度有点懵，觉得是不是出现错误了，但是又是确实的信息，</p><p><img src="/images/%E9%9A%94%E7%A6%BB/message.png" alt="收到短信。。。"></p><p>虽然看到这个信息，但是仍然觉得不相信，此时想到的后果是：</p><ul><li>完了，可能要隔离；</li><li>信息要求报告，因此想能不能不报告，但是好像不行，会带来风险；</li></ul><p>总之，此时，就是上述两种心理的博弈，但是理性占据了上风，必须地报告，不然后果可能很大，不出事则已，一出事就是大事。</p><h1 id="懊悔"><a href="#懊悔" class="headerlink" title="懊悔"></a>懊悔</h1><p>想到上述的结果，第一时间就跟学校的负责人联系了，但是没有得到立即的反馈，可能他们也需要确定一下信息，于是我就等着。</p><p>但是，怎么想自己也不会是黄码，虽然从北京丰台区回来，但是不是中高风险的街道，想来也不会有问题，因此我还申诉了一下，结果还成功了，下午的时候，绿码回来了，但是行程码仍然有北京的痕迹，我后来想了一下，可能是因为在北京等车的时间超过了4个小时，有了痕迹，所以才被打上了黄码标签，真是充满了偶然性。</p><p>本来，我以为健康码变成了绿码，应该不会隔离了吧，但是下午得到的回复是去隔离，而且是14天，我天。。。，突然有一种自己没法按时毕业了的感觉，学院的老师的催促可能，在我看来就是让我延期毕业，那种对抗的心理很强烈，但是我忍住了，想用一个拖字诀。慢慢地，我觉得这样可能不对，因为无法解决问题，而且是躲避问题。要么立即隔离，要么承担后果，可能是无法活动，甚至学院可能有相应的惩罚措施等等，不管躲多久，迟早要面对这两个结果。</p><p>但是，我仍然不明白的是，国家已经公布了中低风险区，已经具体到街道，我觉得国家做的很好了，为什么地方仍要以严格管控为理由，扩大中高险地区范围呢？</p><p>同时也有一种懊悔的心情，如果自己不马上告诉老师，自己的健康码情况，等到健康码变绿，可能就不用报告和隔离了。</p><h1 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h1><p>今天面对的问题是个决策问题，其实是要找出一个最优的决策。</p><p>决策的目标是什么？通俗的说，早回来是为了抓紧时间赶论文毕业，这是我的目标。</p><p>本来一切顺利的话，就按照预定计划执行即可，但是突然有了意外情况——健康码变黄了，这个时候如何调整计划以适应预定的目标，有两个解决方案：</p><ul><li>瞒报<ul><li>最优的结果是谁都不知道，按照预定计划执行；</li><li>最坏的结果是，自己出现问题，被发现，然后被追溯信息，按原计划毕业时不可能了，甚至还会有额外的麻烦，甚至会影响为了的职业发展和规划。</li></ul></li><li>主动报告<ul><li>马上报告<ul><li>最坏的结果是集中隔离14天，在隔离场所写论文；</li><li>最好的结果，不用隔离，按照原计划执行；</li></ul></li><li>延迟报告<ul><li>最坏的结果，集中隔离14天，在隔离场所写论文；</li><li>最好的结果，不用隔离，按照原计划执行；</li></ul></li></ul></li></ul><p>从上述的分析中，发现瞒报肯定是不行的，最坏的结果是我无法承受的（在没有足够信息支撑的情况下做出这种决策）。</p><p>在主动报告中，马上和延迟报告的最好结果和最坏结果都是一样的，既然如此，但是延迟报告中，其发现最好结果的权重肯定要比马上报告大一些，因此这种情况下，目前信息的支撑下，延迟报告可能要好一些，因为得到的结果都是相同的。</p><p>总结来看，<strong>从决策角度来看，延迟报告更好一些，因为事态是在不断变化的，但是这种变化，并没有改变结果，只是改变了结果的分布。</strong></p><p>因此，应了那句话，</p><ul><li><strong>好事晚说不如早说</strong></li><li><strong>坏事早说不如晚说</strong></li></ul><h1 id="应对"><a href="#应对" class="headerlink" title="应对"></a>应对</h1><p>面对这种情况，可能有各种“如果”：</p><ul><li>如果，我当初没有从北京南回来。。。</li><li>如果，我当初没有停留超过4个小时。。。</li><li>如果，学校不是一刀切。。。</li><li>如果，我延迟报告。。。</li></ul><p><strong>没有那么多如果，事情已经发生了，原地懊悔3秒钟，就要想怎么应对的问题了，过去的事情永远无法改变。</strong></p><p><strong>你无法预测未来，就像你也无法预测股市一样，只能去应对。</strong>面对这种情况，可能需要：</p><ul><li>提前准备工作所需材料，做好准备；</li><li>合理安排自己的工作时间；</li><li>提前进行心理按摩，要在隔离点待14天；</li><li>保持身体健康，毕竟只在一个屋行动；</li></ul>]]></content>
      
      
      <categories>
          
          <category> 思考 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 思考 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2021年度总结：黎明前夜</title>
      <link href="/2022/02/01/2021-nian-du-zong-jie-li-ming-qian-ye/"/>
      <url>/2022/02/01/2021-nian-du-zong-jie-li-ming-qian-ye/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这些文字写在2021-2022年的除夕夜，2021年转眼间过去了。这一年感觉过得好快，疫情仍然影响着整个中国，甚至整个世界。所以，我将2021年的总结概括为：黎明前夜，不仅是对于我们生活的这个世界，也包括我们自己面对的情况，黎明前夜是怎样的状况？</p><ul><li>本质还是“夜”；</li><li>但是存在希望；</li><li>对于到达黎明的过程不会太长，但是这个过程中是最冷的；</li></ul><p>或许以后回看这些当时写下的文字时，也会体会到现在的心情吧。</p><h1 id="读书"><a href="#读书" class="headerlink" title="读书"></a>读书</h1><p>今年读了不少的闲书，不管是为了消遣时间还是提升认知，或者只是休息一下。虽然读了之后，发现好多东西都忘了，但是相比于没读，还是有意义的，不过今年学到了一个词“严肃学习”，其实用在读书上还是挺有用的，有些书可以在床上躺着读，有些只能在电脑边，边读边做笔记。这一点是我今年在读书时体会到的，还有一些听书的方式，我尝试了一下，发现并不适合我。</p><p>今年读的书好像有点杂，包括：</p><ul><li><p>《大空头》</p><p>  2020年投资市场的行情，实在是过于恐慌了，疫情的影响被极度放大了。之前看了《大空头》的电影，总有点觉得不过瘾，书确实能留出思考的时间，这本书对于08年那场风暴解释的很清楚，回头看，确是一个好的机会，如果子弹够的话。其中有很多专业术语，为了“专业”而专业，被包装成复杂的东西，显得“高大上”，其实全都是唬人的东西。回望今年，疫情引起的恐慌还在蔓延，但是仍然有一些机会，甚至有一些泡沫。</p></li><li><p>《我在美军航母上的8年》</p><p>  一个华裔士兵的美军服役记录，我觉得很真实，与其每天“厉害了，我的国”，不如老实了解一下，其他人都在干什么，整本书看完后，抛去华裔在海外的那些熟悉的经历，更多的是对于美军的专业性的认识。要像了解自己一样了解竞争者，不要走两个极端。</p></li><li><p>《周期》</p><p>  霍华德·马克斯对于周期的研究，投资中低买高卖是基本原则，但是实践起来是很困难的，如何识别当前位置的高低是一门艰深的学问，这本书尝试从不同角度回答这一问题，人类走极端的行为永远不会停止，而这些极端行为导致的结果总会回归正常，因此需要警惕“这次不一样的论调”，但是历史不会简单重复，但却遵从着类似的模式。这本书值得二刷。</p></li><li><p>《剧变：人类社会与国家危机的转折点》</p><p>  《枪炮、细菌与钢铁》同一作者——贾雷德·戴蒙德， 我挺喜欢这本书的，从国家历史的角度，分析个人的危机和国家危机的相似性，以及相应的处理策略，危机中永远蕴含着机会，像丘吉尔说的“永远不要浪费每一次危机”。总感觉作者和其他的人不太一样，看不出来他偏重哪个角度阐述，有融合了心理学、历史学、政治学还有自然科学等，不像枯燥的社科教材。</p></li><li><p>《创新者：一群技术狂人和鬼才程序员如何改变世界》</p><p>  这是写《乔布斯传》的作者写的，我觉得可以作为一个计算机专业学生的入门书籍，是极力推荐的，其中好多内容可以和大学学到的东西对应起来，当初学到的东西，全都是孤立的东西，现在这本书将所有内容都连起来了，从这一点上来看也是值得一读的。其中书中，没有如小标题所说那样，一定是技术推动，但无疑它扮演了主要的角色，还包括：家世、认知、文化、制度等。其中还充满着勾心斗角、官僚文化等等，总之，创新不是简单地完全由技术推动。</p></li><li><p>还有一本没有读完的《东京贫困女子》</p><p>  这本书看起来实在是太难受了，职场歧视、家庭破裂、性别歧视、性骚扰等等，困扰着女性们，但是这归根结底可能是多重因素导致的，一个发达国家中的高学历女性竟然沦落风尘，而且还不少，咋看咋觉得这个社会出现问题了，并不能全部如中国史家那样，只有有问题就归罪于女人。整本书，就不断将社会的黑暗面展现给你看，将负面情绪推到极点。后续，这本书会继续读下去。</p></li></ul><h1 id="财务规划"><a href="#财务规划" class="headerlink" title="财务规划"></a>财务规划</h1><p>今年也对自己的整个投资计划做了一个总结，因为也有帮老妈管理的部分。因为各种情况吧，今年的情况还算可以，但也是充满了魔幻，不管怎样这是第一次总结自己的投资成败，结果图表所示：</p><table><thead><tr><th>投资计划</th><th>收益率</th></tr></thead><tbody><tr><td>我的计划</td><td>14.87%</td></tr><tr><td>老妈的计划</td><td>5.75%</td></tr><tr><td>沪深300</td><td>-5.20%</td></tr><tr><td>中证500</td><td>15.58%</td></tr><tr><td>恒生指数</td><td>-14.08%</td></tr><tr><td>偏股混合基金</td><td>7.68%</td></tr></tbody></table><p>总的来说，结果还不错，其中第一次帮老妈管钱，之前也跟她明确了投资风格以稳健为主，因此投资了大量的债券和货币，所以基本没有回撤。虽然老妈说放心我，不在意投资啥，但是考虑到年纪和心理，还是以稳健为主，回家和老妈复盘了一下今年的成绩，得到认可，2022年可能需要加入一个增强的因素，来稍微提升一点收益率。</p><p>对于我自己，今年也在逐步建立自己的投资体系，目前已经有一个设计了，正在逐步实现，希望能够不断完善这个体系。而今年的成绩，有一些运气在里边，年初银行保险不太行，买了一些，所以收益还不错。在实践的过程中，逐步认识到投资是门技术+艺术的领域，很难完全从技术角度去理解，或者完全主观去看待，需要不断从实践中认识。</p><h1 id="自省和体会"><a href="#自省和体会" class="headerlink" title="自省和体会"></a>自省和体会</h1><p>今年的心情低落到极点，那种无力感，可能无法形容，我想极力挑出这种极端负面的情绪下，但是屡遭失败，这是自我调节的能力出现了问题，我想这种能力相比于技术能力、解决问题的问题更重要，因为学校里有些人，就是无法承受这种无力感，而选择了极端的行为，这是要避免的，现在我还不清楚如何锻炼这种能力，但是我意识到了这方面的缺乏，后续可能需要看一些书，或者相关材料学习，但是可能最好的还是自己从中体会并实践。</p><p>自我否定和怀疑，这几年来深深地陷入了自我否定的怪圈，觉得自己一文不值，什么都不会，什么都不行。后来反思，这与所在的环境密不可分，但是也有自己的原因，期望过高，并且没有对要做的事情进行量化，导致无法明确任务的结果，也就无法产生正反馈。但是今年通过参加了一些活动，觉得自己不是那么不堪，不管是从自己表现的结果来看，或者是他人的间接的评价来看。一个重要的原因，可能是做事的方法出现了问题，虽然做事有规划，但是没有明确的可量化的目标，或者阶段性的可以总结的成果，这是需要以后改正的。</p><p>今年的一个体会是，人们倾向于将一些表现突出的人神化为圣人，私以为这种倾向是有问题的，不管是专业领域还是日常生活，对于个人来说，这会让人迷信权威，没有独立思考的能力，对于被神化的人也没有好处，当人们把你捧得愈高，你会越焦虑，因为不敢丝毫出错误，只要有问题，你的所有之前的努力全都会被忽视，所有人都会背叛你，不管是娱乐明星还是专业人士。对于整个社会也存在隐患，人们不敢质疑，专业人士出一点问题就一棒子打死，那些当初最追捧你的，之后也是拆你墙最快的人。总之，对于我们自身来说，不要神化任何人，之前看到一个访问，一个诺贝尔经济学奖得主的投资成果，一塌糊涂，按理说没人比他更懂经济，但是结果就在那里。</p><h1 id="2022年目标"><a href="#2022年目标" class="headerlink" title="2022年目标"></a>2022年目标</h1><ul><li>很朴素和直接的一件事情——顺利毕业，拿到学位，这是今年的主要目标。</li><li>今年回家突然发现一件事情，老妈真的年纪大了，之前有意识到，但是没有几年这么明显，今年毕业回家后，要带老妈去至少两个地方游玩，并拍一些照片。</li><li>学习计算机图形学：完成闫令琪博士的<a href="http://games-cn.org/intro-graphics/">GAMES101:现代计算机图形学入门</a>课程和作业。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 年度总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 年度总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RotatE - 建模复杂关系的利器</title>
      <link href="/2021/11/20/rotate-jian-mo-fu-za-guan-xi-de-li-qi/"/>
      <url>/2021/11/20/rotate-jian-mo-fu-za-guan-xi-de-li-qi/</url>
      
        <content type="html"><![CDATA[<p>文章来源: <a href="https://arxiv.org/abs/1902.10197v1">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</a></p><h1 id="之前研究中的问题"><a href="#之前研究中的问题" class="headerlink" title="之前研究中的问题"></a>之前研究中的问题</h1><p>不能对三种关系同时进行建模和推理。</p><blockquote><p>None of existing models is capable of modeling and inferring all the above patterns: symmetry/antisymmetry, inversion, and composition。</p></blockquote><h3 id="例子说明"><a href="#例子说明" class="headerlink" title="例子说明"></a>例子说明</h3><ul><li>symmetry：婚姻，双向关系；</li><li>antisymmetry：孝顺，单向关系；</li><li>inversion：上位词和下位词，正向和反向关系；</li><li>composition：我母亲的丈夫是我父亲，关系组合计算；</li></ul><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><blockquote><p>Motivation is from Euler’s identity: a unitary complex number can be regarded as a rotation in the complex plane.</p></blockquote><p>$$<br>e^{i\theta} = \text{cos}\theta + i\text{sin}\theta<br>$$</p><p>即，任何一个复数都可以看作复平面上的一个旋转向量。</p><p><strong>RotatE model maps the entities and relations to the complex vector space and deﬁnes each relation as a rotation from the source entity to the target entity.</strong></p><p>形式化表达：</p><p>Given a triplet $(h, r, t)$ , we expect that $t = h \circ r$ , where $h, r, t \in \mathbb{C}^k $ are the embeddings, the modulus $| r_i | = 1$ and $\circ$ denotes the <code>Hadamard (element-wise) product</code>. Speciﬁcally, for each dimension in the complex space, we expect that:</p><p>$$<br>t_i = h_i r_i , \text{where} \ h_i , r_i , t_i \in \mathbb{C}^k \text{and} \ | r_i | = 1.<br>$$</p><p>说明：</p><ul><li><p><code>symmetric</code></p><p>  a relation $r$ is symmetric if and only if each element of its embedding $r_i$ , satisﬁes $r_i= e^{0/i\pi}= \pm1$ ;</p></li><li><p><code>inverse</code></p><p>  Two relations $r_1$ and $r_2$ are inverse if and only if their embeddings are <code>conjugates</code>: $r_2= \overline{r}_1$ ;</p></li><li><p><code>composition</code></p><p>  a relation $r_3= e^{i\theta_3} $  is a combination of other two relations $r_1= e^{i\theta_1}$ and $r_2= e^{i\theta_2}$ if and only if $r_3 = r_1 \circ r_2$ (i.e. $\theta_3 = \theta_1 + \theta_2$ ).</p></li></ul><h1 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h1><h2 id="Relations-Formal-deﬁnition"><a href="#Relations-Formal-deﬁnition" class="headerlink" title="Relations Formal deﬁnition"></a>Relations Formal deﬁnition</h2><p><strong>Deﬁnition 1</strong>. A relation $r$ is <code>symmetric</code> (<code>antisymmetric</code>) if $\forall x, y$ </p><p>$$ r(x, y) ⇒ r(y, x) \ ( r(x, y) ⇒ ¬r(y, x) ) $$</p><p>A clause with such form is a <code>symmetry</code> (<code>antisymmetry</code>) pattern.</p><p><strong>Deﬁnition 2</strong>. Relation $r_1$ is <code>inverse</code> to relation $r_2$ if $\forall x, y$ </p><p>$$r_2 (x, y) ⇒ r_1(y, x)$$ </p><p>A clause with such form is a <code>inversion</code> pattern.</p><p><strong>Deﬁnition 3</strong>. Relation $r_1$ is <code>composed</code> of relation $r_2$ and relation $r_3$ . if $\forall x, y, z$</p><p>$$r_2 (x, y) ∧ r_3 (y, z) ⇒ r_1 (x, z)$$ </p><p>A clause with such form is a <code>composition</code> pattern.</p><h2 id="建模思考过程"><a href="#建模思考过程" class="headerlink" title="建模思考过程"></a>建模思考过程</h2><p>我们希望达成以下目标：</p><ul><li>(非)对称关系<br>$$ r(x, y) ⇒ r(y, x) \ ( r(x, y) ⇒ ¬r(y, x))$$</li><li>逆向关系<br>$$r_2 (x, y) ⇒ r_1(y, x)$$</li><li>组合关系<br>$$r_2 (x, y) ∧ r_3 (y, z) ⇒ r_1 (x, z)$$</li></ul><p>那就要找到一个函数能够实现这三种关系的表示。</p><p>TransE能够对这三种关系同时建模吗？稍稍分析一下：</p><ul><li><p>对于<code>symmetric</code></p><p>  需要找到满足条件的关系的embeddings</p><p>  $$ r(x, y) ⇒ r(y, x)$$</p><p>  $$x + r_i = y$$</p><p>  $$y + r_j = x$$</p><p>  $$r_i = r_j$$</p><p>  则，只能 $r_i = r_j = 0$ ，所以不能表示 <code>symmetric</code>。</p></li></ul><ul><li><p>对于<code>antisymmetry</code></p><p>  $$ r(x, y) ⇒ ¬r(y, x) $$</p><p>  要求：</p><p>  $$x + r_i = y$$</p><p>  $$y + r_j = x$$</p><p>  $$r_i \ne r_j$$</p><p>  模型是能够满足要求的，只要保证：</p><p>  $$r_i + r_j = 0$$</p><p>  $$r_i \ne r_j$$</p><p>  至于建模的效果好不好，那是另外一回事。</p></li><li><p>对于<code>inversion</code></p><p>  需要满足：</p><p>  $$r_2 (x, y) ⇒ r_1(y, x)$$ </p><p>  从上述<strong>反对称</strong>的角度进一步看，在满足<strong>反对称</strong>的同时，它也就满足了<code>inversion</code>关系的表达。</p><p>  $$x + r_2 = y$$</p><p>  $$y + r_1 = x$$</p><p>  $$r_i \ne r_j$$</p><p>  只要满足以下条件就行：<br>  $$r_1 = - r_2$$</p></li><li><p>对于<code>composition</code></p><p>  需要满足：</p><p>  $$r_2 (x, y) ∧ r_3 (y, z) ⇒ r_1 (x, z)$$</p><p>  对于使用TransE来表示如下：</p><p>  $$x + r_2 = y$$</p><p>  $$y + r_3 = z$$</p><p>  $$x + r_1 = z$$</p><p>  对<code>TransE</code>的表示进行变形：</p><p>  $$x + r_2 + r_3 = x + r_1$$</p><p>  自然得到：</p><p>  $$r_2 + r_3 = r_1$$</p></li></ul><p>总结来看，<code>TransE</code>除了<strong>对称关系</strong>无法表达，其余关系均能表达，<strong>现在的问题是如何在TransE的基础上对对称关系建模，使得关系的embedding不全为0.</strong></p><p>那么要明白的是<code>TransE</code>为啥不能建模对称关系？因为在使用<strong>平移</strong>建模关系时，对应的是<strong>加法运算</strong>。那么换一种想法，加法不行，那么乘法是不是可以？</p><p>为了满足对称关系的要求，有如下的关系：</p><p>$$hr_i = t$$</p><p>$$tr_j = h$$</p><p>$$r_i = r_j$$</p><p>能够推导出以下关系：</p><p>$$tr_jr_i = t$$</p><p>$$r_jr_i = 1$$</p><p>$$r_ir_i = 1 = r_jr_j$$</p><p>上述可以看成是一个维度上的情况，当换到所有embedding上的维度时，这就自然引出了 <code>Hadmard (or element-wise) product</code>。</p><p>这就是当时使用<code>Hadmard (or element-wise) product</code>需要满足的条件，为了建模对称关系，要保证:</p><p>$$r_ir_i = 1 = r_jr_j$$</p><p>其实，这就是上文中要保证 </p><p>$$| r_i | = 1$$</p><p>的原因。</p><p>We map the head and tail entities $h$ , $t$ to the complex embeddings, i.e., $\mathbf{h}, \mathbf{t} \in \mathbb{C}^ k$ , then we <code>deﬁne the functional mapping</code> induced by each relation $\mathbf{r}$ as an element-wise rotation from the head entity $\mathbf{h}$ to the tail entity $\mathbf{t}$ .</p><p>这里，开始时不明白为什么选择 <code>Hadmard (or element-wise) product</code>，这个与欧拉公式是啥关系？因为原文用了 <code>define</code>，我觉得逻辑可能并不充分，可能是一种启发式的选择，只要它满足三种关系的形式化建模就行。</p><blockquote><p>其实这种思考方法，我仔细想了一下，其实是两种方式的区别，我一开始不明白，是因为没有严格的逻辑推导出要用Hadmard，所以它出来时，我一头雾水，这是一种从因到果的思考方式。但是这里换一种方法去思考，Hadmard的结果满足我们开始时的假设，即能够建模对称关系，因此我们选了它，这是一种由果及因的方式，我的感觉，在ML领域这种方式好像更常见，也是被数学系的人吐槽的原因。</p></blockquote><h2 id="用Hadmard-product验证关系约束"><a href="#用Hadmard-product验证关系约束" class="headerlink" title="用Hadmard product验证关系约束"></a>用Hadmard product验证关系约束</h2><p>验证三种关系如下：</p><ul><li><p>对于<code>symmetric</code></p><p>  需要找到满足条件的关系的embeddings</p><p>  $$ r(x, y) ⇒ r(y, x)$$</p><p>  $$\mathbf{h} \circ \mathbf{r_i} = \mathbf{t}$$</p><p>  $$\mathbf{t} \circ \mathbf{r_j} = \mathbf{h} $$</p><p>  $$\mathbf{r_i} = \mathbf{r_j} $$</p><p>  推导出：<br>  $$\mathbf{r_i} \circ \mathbf{r_i} = 1 = \mathbf{r_j} \circ \mathbf{r_j} $$<br>  符合。</p></li><li><p>对于<code>antisymmetry</code></p><p>  类似的，<br>  $$ r(x, y) ⇒ ¬r(y, x) $$</p><p>  推导出，<br>  $$\mathbf{r_i} \circ \mathbf{r_i} \ne 1 $$<br>  符合。</p></li><li><p>对于<code>inversion</code></p><p>  需要满足：</p><p>  $$r_2 (x, y) ⇒ r_1(y, x)$$ </p><p>  $$\mathbf{h} \circ \mathbf{r_i} = \mathbf{t}$$</p><p>  $$\mathbf{t} \circ \mathbf{r_j} = \mathbf{h} $$</p>  <!-- $$\mathbf{r_i} = \mathbf{r_j} $$ --><p>  推导出：<br>  $$\mathbf{r_i} \circ \mathbf{r_j} = 1 $$</p><p>  $$\mathbf{r_i} = \mathbf{r_j}^{-1} $$</p><p>  符合。</p></li><li><p>对于<code>composition</code></p><p>  需要满足：</p><p>  $$r_2 (x, y) ∧ r_3 (y, z) ⇒ r_1 (x, z)$$</p><p>  根据：</p><p>  $$\mathbf{x} \circ \mathbf{r_2} = \mathbf{y}$$</p><p>  $$\mathbf{y} \circ \mathbf{r_3} = \mathbf{z} $$</p><p>  $$\mathbf{x} \circ \mathbf{r_1} = \mathbf{z} $$</p><p>  推导出：<br>  $$\mathbf{x} \circ \mathbf{r_2} \circ \mathbf{r_3} = \mathbf{x} \circ \mathbf{r_1} $$</p><p>  $$\mathbf{r_2} \circ \mathbf{r_3} = \mathbf{r_1}$$</p><p>  完美得出。</p></li></ul><p>总结来看，<code>Hadmard product</code>能够完美建模三种关系，到此，我们的假设成立。</p><h2 id="与欧拉公式的关系"><a href="#与欧拉公式的关系" class="headerlink" title="与欧拉公式的关系"></a>与欧拉公式的关系</h2><p>用类比的方法来看，<code>TransE</code>是将所有的实体和关系映射到embedding space中，也就是实平面，使用的是<strong>向量加法运算</strong>，三者之间的关系可以使用<strong>平移</strong>这种操作来建立关联。</p><p>那么<strong>Hadmard product</strong>实现的是<strong>向量元素乘法运算</strong>，如果放到实平面中，这没办法对应一个操作，不管是用矩阵乘法实现的线性变换，还是加上平移的仿射变换，都没办法对应将两个同样长度的向量经过元素相乘得到同样长度的向量，因此在理论上说不过去。</p><p>那么，换到复平面上，会怎么样？</p><p>我们知道复平面上一个复数的表示方法有好几种：</p><ul><li>代数：$a+ib$，以 $(1,i)$ 为基的线性组合；</li><li>指数形式</li><li>极坐标</li><li>向量形式</li><li>矩阵乘法</li></ul><p>具体的内容可以参看<a href="https://zhuanlan.zhihu.com/p/85321120">这里</a>。</p><p>通过欧拉公式可以将指数形式和极坐标建立关系。</p><p>此时，复数的相乘可以看做矩阵变换：</p><p>$$z_1z_2 =(a+ib)(c+id) = (ac-bd)+(bc+ad)i$$</p><p><img src="/images/rotate/5.png" alt="复数乘法视为矩阵变换"></p><p>我们只要把embedding拆成实部和虚部，然后再利用复数的乘法进行计算，就同样能得到实部和虚部，即形式不变。</p><p>到此为止，已经弄明白几件事情了：</p><ul><li><code>TransE</code>的缺陷</li><li><code>RoratE</code>的改进想法</li><li><code>Hadmard product</code>满足三种关系的表达需求</li><li><code>Hadmard product</code>的计算可以看成是复平面中embedding的相乘</li></ul><p>还有一件事情，复平面中embedding的相乘和欧拉公式有啥关系？</p><p>我们使用了<code>Hadmard product</code>将一个head实体变成了tail实体，其中的一个操作用来衡量关系，在复空间中将一个实体变成另一个实体，使用的是乘法的方式，那就是指数了，而指数与复空间建立关系的途径，就是欧拉公式。</p><p>$$<br>e^{i\theta} = \text{cos}\theta + i\text{sin}\theta<br>$$</p><p>复空间中的每个数有指数形式和极坐标形式，这两者之间的关系也是欧拉公式。</p><p>从指数形式来看，将一个复数的embedding变换成另一个复数的embedding如下：</p><p>$$e^{i{\Theta}_h} e^{i{\Theta}_r} = e^{i{\Theta}_t}$$</p><p>用极坐标表示，就是将$e^{i{\Theta}_h}$ 逆时针方向旋转了角度 $\Theta_r$，变成了 $e^{i{\Theta}_t}$ 。</p><p>当将关系写成极坐标形式时，</p><!--$$\begin{pmatrix}    \text{cos}\theta & -\text{sin}\theta \\  \text{sin}\theta & \text{cos}\theta \end{pmatrix} $$--><p><img src="/images/rotate/math_1.png"></p><p>也就是如下所示：</p><!--$$\begin{pmatrix}    a & -b \\    b & a \end{pmatrix}\begin{pmatrix}    \text{cos}\theta & -\text{sin}\theta \\    \text{sin}\theta & \text{cos}\theta \end{pmatrix}$$ --><p><img src="/images/rotate/math_2.png"></p><p>这不就是线性变换里的旋转吗，也就是本文的名称的由来！</p><p>不明白的可以回顾一下<a href="https://blog.csdn.net/huangguohui_123/article/details/106000355">以前的知识</a>。</p><p>因为我们之前约束了关系的模长固定，为1，即 $|r_i| = 1$ 。</p><p>因此，在旋转过程中，head实体的模长不会受到影响，只是每一个维度下的复数的相位受到了影响。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>For each triple $(h, r, t)$ , we deﬁne the <code>distance function</code> of RotatE as:</p><p>$$d_r (\mathbf{h}, \mathbf{t}) = \Vert \mathbf{h} \circ \mathbf{r} − \mathbf{t} \Vert$$</p><h2 id="Loss函数"><a href="#Loss函数" class="headerlink" title="Loss函数"></a>Loss函数</h2><!--$$L=-\log \sigma \left( \gamma-d_r (\mathbf{h}, \mathbf{t}) \right) - \sum_{i=1}^{n} \frac{1}{k} \log \sigma\left (d_r \left( \mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)$$--><p><img src="/images/rotate/math_3.png"></p><ul><li>$\gamma$ : 和<code>TransE</code>中一样，是<code>margin</code>，视为超参数；</li><li>$\sigma$ : <code>sigmoid function</code></li><li>$(h_i , r, t_i )$ is the <code>i-th negative triplet</code>.</li><li>$k$ 为embedding dimension</li></ul><h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>原来的采样方式是使用<strong>均匀的方式</strong>替换head和tail：</p><ul><li><p>低效</p><p>  suffers the problem of inefﬁciency since many samples are obviously false as training goes on</p></li><li><p>没有足够的信息</p><p>  not provide any meaningful information.</p></li></ul><p>文中提出<code>self-adversarial negative sampling</code>，具体而言就是按照以下方式进行采样，</p><!--$$p\left(h_{j}^{\prime}, r, t_{j}^{\prime} \mid\left\{\left(h_{i}, r_{i}, t_{i}\right)\right\}\right)=\frac{\exp \alpha f_{r}\left(\mathbf{h}_{j}^{\prime}, \mathbf{t}_{j}^{\prime}\right)}{\sum_{i} \exp \alpha f_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)}$$--><p><img src="/images/rotate/math_4.png"></p><ul><li><p>$\alpha$ : temperature of sampling. </p><blockquote><p>Moreover, since the sampling procedure <strong>may be costly</strong>, we treat the above probability as the <strong>weight of the negative sample</strong>. Therefore, the ﬁnal negative sampling loss with self-adversarial training takes the following form:</p></blockquote><p>  <img src="/images/rotate/math_5.png"></p></li></ul><!--$$L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)-\sum_{i=1}^{n} p\left(h_{i}^{\prime}, r, t_{i}^{\prime}\right) \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)$$--><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><img src="/images/rotate/1.png" alt="数据集"></p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p><code>Filtered setting</code>: we rank test triples against all other candidate triples not appearing in the training, validation, or test set, where candidates are generated by corrupting subjects or objects: $(h^{‘} , r, t)$ or $(h, r, t^{‘} )$. </p></li><li><p><code>Mean Rank (MR)</code>, <code>Mean Reciprocal Rank (MRR)</code> and <code>Hits at N (H@N)</code> are standard evaluation measures for these datasets and are evaluated in our experiments.</p></li></ul><h2 id="超参设置"><a href="#超参设置" class="headerlink" title="超参设置"></a>超参设置</h2><ul><li><code>optimizer</code>: <code>Adam</code></li><li><code>search strategy</code>: ﬁne-tune the hyperparameters on the validation dataset with <code>grid search</code></li><li><code>embedding dimension</code>: 125, 250, 500, 1000</li><li><code>batch size</code>: 512, 1024, 2048 </li><li>self-adversarial <code>sampling temperature</code>: 0.5, 1.0 </li><li><code>ﬁxed margin</code> $\gamma$ :3, 6, 9, 12, 18, 24, 30. we ﬁnd that the ﬁxed margin $\gamma$ could prevent our model from over-ﬁtting.</li></ul><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>这里主要体现了在3个来源，4个数据集上的评估效果。</p><p><img src="/images/rotate/2.png" alt="在FB15k和WN18的结果"><br><img src="/images/rotate/3.png" alt="在FB15k-237和WN18RR的结果"><br><img src="/images/rotate/4.png" alt="在Countries的结果"></p><h1 id="结果总结"><a href="#结果总结" class="headerlink" title="结果总结"></a>结果总结</h1><p>本论文主要达成了三项目的：</p><ul><li><p>able to <strong>model and infer various relation patterns</strong> including: symmetry/antisymmetry, inversion, and composition.</p></li><li><p>propose a <strong>novel self-adversarial negative sampling technique</strong> for efﬁciently and effectively training the RotatE model.</p></li><li><p>the RotatE model is <strong>scalable to large knowledge graphs</strong> as it remains linear in both time and memory.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransE - 简单但有效的知识图谱表示学习方法</title>
      <link href="/2021/11/19/transe-jian-dan-dan-you-xiao-de-zhi-shi-tu-pu-biao-shi-xue-xi-fang-fa/"/>
      <url>/2021/11/19/transe-jian-dan-dan-you-xiao-de-zhi-shi-tu-pu-biao-shi-xue-xi-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>来自论文 NIPS 2013年的 <a href="https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html">Translating Embeddings for Modeling Multi-relational Data</a>。</p><p>这个方法开创了KGE中的一种思考方式，虽然简单，但是值得一读，因为有时候越简单的方法，取得的效果可能没有人们想得那么差。</p><h1 id="针对的问题"><a href="#针对的问题" class="headerlink" title="针对的问题"></a>针对的问题</h1><p>The problem of <code>embedding entities and relationships of multirelational data</code> in low-dimensional vector spaces.</p><h2 id="针对的研究对象"><a href="#针对的研究对象" class="headerlink" title="针对的研究对象"></a>针对的研究对象</h2><p><code>Multi-relational data</code> refers to directed graphs whose nodes correspond to <code>entities</code> and <code>edges</code> of the form <code>(head, label, tail)</code> , denoted $(h, l, t)$ , each of which indicates that there exists a relationship of name <code>label</code> between the entities <code>head</code> and <code>tail</code>.</p><h2 id="之前研究存在问题"><a href="#之前研究存在问题" class="headerlink" title="之前研究存在问题"></a>之前研究存在问题</h2><ul><li>表达能力上升以计算成本上升为代价</li></ul><blockquote><p>The greater expressivity of these models comes <code>at the expense of substantial increases in model complexity</code> which results in modeling assumptions that are <code>hard to interpret</code>, and in <code>higher computational costs</code>.</p></blockquote><ul><li>正则化方法难以设计导致过拟合</li></ul><blockquote><p>such approaches are potentially subject to either <code>overﬁtting</code> since <code>proper regularization of such high-capacity models is hard to design</code>, </p></blockquote><ul><li>非凸优化导致欠拟合</li></ul><blockquote><p>or <code>underﬁtting</code> due to the <code>non-convex optimization problems</code> with many local minima that need to be solved to train them.</p></blockquote><h1 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h1><p>模型希望达到的目标：</p><ul><li>easy to train,</li><li>contains a reduced number of parameters,</li><li>and can scale up to very large databases</li></ul><p>in complex and heterogeneous multi-relational domains simple yet appropriate modeling assumptions can <strong>lead to better trade-offs between accuracy and scalability</strong>.</p><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><p>Relationships are represented as <code>translations</code> in the <code>embedding space</code>: if $(h, l, t)$ holds, then the embedding of the tail entity $t$ should be close to the embedding of the head entity $h$ plus some vector that depends on the relationship $l$ .</p><p>模型的基本想法是<code>head</code>的向量表示 $h$ 与<code>relation</code>的向量表示 $r$ 之和与<code>tail</code>的向量表示  $t$ 越接近越好，即：</p><p>$$<br>h+r \approx t<br>$$</p><p>这里的“接近”可以使用<code>L1</code>或<code>L2</code>范数进行衡量，这也是一个可以调节的超参数。</p><h2 id="方法过程"><a href="#方法过程" class="headerlink" title="方法过程"></a>方法过程</h2><p><img src="/images/transe/1.png" alt="算法过程"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数是使用了负抽样的max-margin函数（原文为：<code>a margin-based ranking criterion</code>）。</p><p>$$<br>L(y, y^{\prime}) = \max(0, margin - y + y^{\prime})<br>$$</p><ul><li>$y$ 是正样本的得分；</li><li>$y^{\prime}$ 是负样本的得分。</li></ul><p>然后使损失函数值最小化，当这两个分数之间的差距大于<code>margin</code>的时候就可以了(我们会设置这个值，通常是1，这个是超参数)。</p><p>由于我们使用距离来表示得分，所以我们在公式中加上一个减号，知识表示的损失函数为：</p><p>$$<br>L(h,r,t) = \max(0, d_{pos} - d_{neg} + margin)<br>$$</p><p>其中，$d$ 是：</p><p>$$<br>d = \Vert h+r -t \Vert<br>$$</p><p>这是<code>L1</code>或<code>L2</code>范数。至于如何得到负样本，则是将head实体或tail实体替换为三元组中的随机实体，具体的方法可以参考 <a href="https://lifehit.cn/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-xun-lian-fang-fa/">知识图谱表示学习：训练方法</a>。</p><h2 id="需要调节的参数"><a href="#需要调节的参数" class="headerlink" title="需要调节的参数"></a>需要调节的参数</h2><p>对于模型本身来说，只需要确定：</p><ul><li>实体和关系的维度：<code>embedding size</code></li><li>衡量接近程度的指标：<code>L1</code>或者<code>L2</code></li><li><code>margin</code></li></ul><p>即可。</p><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><h2 id="使用的数据集"><a href="#使用的数据集" class="headerlink" title="使用的数据集"></a>使用的数据集</h2><p><img src="/images/transe/2.png" alt="使用的数据集"></p><h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><ul><li><p>评估指标</p><ul><li><code>mean rank</code></li><li><code>hits@10</code></li></ul></li><li><p>过滤真实三元组</p></li></ul><p>这部分可以参考 <a href="https://lifehit.cn/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-ping-gu-fang-fa/">知识图谱表示学习：评估方法</a>。</p><h2 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h2><p><img src="/images/transe/3.png" alt="Link prediction 评估结果"></p><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><ul><li><code>optimizer</code>: stochastic gradient descent</li><li><code>lr</code>: 0.001, 0.01, 0.1</li><li><code>margin</code>: 1, 2, 10</li><li><code>dimension k</code>: 20, 50</li><li><code>regularization</code>: L1, L2</li><li><code>epochs</code>: &lt;1000</li></ul><h1 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h1><ul><li>simplicity</li><li>只适合处理一对一的关系，不适合一对多/多对一/多对多的关系。<blockquote><p>举个例子，有两个三元组（中国科学院大学，地点，北京）和（颐和园，地点，北京），使用TransE进行表示的话会得到中国科学院大学的表示向量和颐和园的表示向量很接近，甚至完全相同。但是它们的亲密度实际上可能没有这么大。</p></blockquote></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这个方法从模型角度来看，很简单，但是要注意文章的目标是，取得预测性能和计算性能之间的平衡，也就说是比我强的没我快，比我快的没我强。</p><p>而且整个模型，很优美，没有任何繁琐的东西，是值得读的一篇入门的文章。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我还是重启了博客写作</title>
      <link href="/2021/11/18/wo-huan-shi-chong-qi-liao-bo-ke-xie-zuo/"/>
      <url>/2021/11/18/wo-huan-shi-chong-qi-liao-bo-ke-xie-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="以前的博客"><a href="#以前的博客" class="headerlink" title="以前的博客"></a>以前的博客</h1><p>博客的写作已经停了很久了，至于当初是如何开始的，已经有点记不清了，不过有一点可以明确的是：当初写博客一半是为了记录技术的积累，一半是有一定的功利性的，就是能够在找工作时写到简历上。最后找到的工作不错，不知道这个有没有起作用。</p><p>当初写博客时，由于是第一次，就直接在一个技术网站上开始了，这个网站，如果经常泡技术社区的人都会知道，CSDN，开始他提供了比较便捷的方式，你只要写就行了，所以对于新手还是挺友好的，类似的平台还有博客园，等等。</p><p>在这个平台上我也写作也慢慢稳定下来，截止到现在 2021年11月18日，目前的数据如下所示：</p><p><img src="/images/restart_blog/1.png" alt="以前的博客浏览数据1"><br><img src="/images/restart_blog/2.png" alt="以前的博客浏览数据2"></p><p>最后一篇文章是发布于2017年9月份，距今已经4年多了，然后我就停止了更新。说实话，这些数据是我没有想到的，因为当时是一边学技术，一边记录下自己的问题，没想到过得到很多人的反馈，虽然与很多的大佬不能比，但是还是有一些成就感的。在此，也为之前积极给与反馈的朋友表示感谢。</p><p>但是之后为啥停了呢？除了一些其他原因，主要有以下几个方面：</p><h2 id="CSDN的环境在变坏"><a href="#CSDN的环境在变坏" class="headerlink" title="CSDN的环境在变坏"></a>CSDN的环境在变坏</h2><p>不知道从什么时候开始，CSDN中的广告和写作的体验越来越不好了，当你看到辛辛苦苦写完的文章，旁边有一个让人不适的广告时，总觉得有点难受。</p><p>同时，还有一些技术上的原因，在写作的过程中，有一次，我发现写到一半的文章消失了，没有任何记录，这导致我对这个平台产生了不信任感，同时还包括整个平台技术质量的下降，之后我有任何问题，基本都是去Stack Overflow查找，除了一些软件安装上遇到的问题。</p><p>慢慢地，当你写的越多，会发现对这个平台的失望越多，而且沉没成本越高，不容易迁移。因此，希望，能有一个自己掌控的平台。</p><h2 id="专注于技术领域的限制"><a href="#专注于技术领域的限制" class="headerlink" title="专注于技术领域的限制"></a>专注于技术领域的限制</h2><p>之前写的博客，专注于纯技术领域，但是慢慢地，我发现，这种“自我设限”的方式，不利于自己的发展，因为人的思考是多方面的，因此，我希望从各个方面来记录个人的发展和对技术的思考，而不只是技术本身。</p><p>基于这种考虑，希望建立一种综合的博客体验，全面地记录技术、思考和阅读，以及自己的一些经历。</p><h1 id="为啥重启"><a href="#为啥重启" class="headerlink" title="为啥重启"></a>为啥重启</h1><p>重启博客的写作是不容易的，开始很难，但是坚持更难，不怕笑话，之前我尝试重启了一次，但是由于种种原因，最后没有坚持下去。但是，此时此刻，我觉得可能仍然没有准备好，但不是有一句话吗？<strong>做一件事最好的时刻是10年前，其次是现在。</strong> 纵然，现在仍然有很多的事情需要处理，但是我还是挤出时间，重建了这个博客平台。因为，我知道，有些东西，<strong>如果不现在记录下来，可能就永远没有记录的机会了。</strong></p><p>为啥要重启博客写作，说到底主要有两个方面原因：</p><h2 id="留下思考的legacy"><a href="#留下思考的legacy" class="headerlink" title="留下思考的legacy"></a>留下思考的legacy</h2><p>不像4年前那么功利性了，反倒是希望留下一些legacy，能够供以后来回忆，不管是技术，还是思考，阅读。</p><p>可能是年纪大了，学生时代的尾声慢慢来到了。有很多的内容输出，虽然也有一直在整理，但是都是躺在自己的Notion里，没有输出到公开的世界，可能都是自嗨，因此，也希望找一个出口，能够呈现自己的学习，思考和经历。</p><h2 id="促进学习和交流"><a href="#促进学习和交流" class="headerlink" title="促进学习和交流"></a>促进学习和交流</h2><p>最近看了很多学习过程方面的文章和书籍，深刻领会到一件事情：<strong>说不出来的知识，就是你还不懂的。</strong> 对于你能掌握的知识，最好是以简单和易懂的方式说出来，讲给别人听，如果别人懂了，那么你也懂了，如果别人不懂，其实你也不懂，因为每次学习一个新的知识，我们都倾向于骗自己，因为自己轻易就懂了，但是事实可能正好相反。</p><p>费曼学习方法，我在好多文章和书籍里都听到了这个方法，可以简单看<a href="https://learnku.com/articles/35137">这里</a>了解，便深入查看了一下，对于费曼这个人，大家可以去wiki查看，对于这种学习方法，简单来讲，即：</p><ul><li>学东西；</li><li>教给别人；</li><li>如果讲不清，回去重学；</li><li>再教，以把外行人教懂为目标；</li></ul><p>其中需要找到一个情景，来实现“教”这个步骤，我希望这个博客能充当这个媒介，当然不是传统的“教授”，而是作为一个将学到的知识进行总结、陈述和输出的平台，这样能让自己掌握的更加夯实。</p><h1 id="以后的发展"><a href="#以后的发展" class="headerlink" title="以后的发展"></a>以后的发展</h1><p>我也不知道未来会发展成什么样子，总之希望能坚持下去，我很钦佩阮一峰、陈皓等大神的持之以恒，或许这就是他们能走出一条对于技术人员来说，很成功的路。</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>在互联网的时代，我们是幸福的，这个博客平台的搭建，并没有消耗太多时间，因为有以下的技术支撑：</p><ul><li>Github</li><li>Hexo静态博客</li><li>Hexo主题 <a href="https://github.com/blinkfox/hexo-theme-matery">hexo-theme-matery</a></li><li>万网域名</li><li>Google Analytics</li></ul>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
            <tag> 写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱表示学习：评估方法</title>
      <link href="/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-ping-gu-fang-fa/"/>
      <url>/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-ping-gu-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>如何对知识图谱进行评估，其实知识图谱本身并不需要评估，因为其中的实体和关系已经确定，但是考虑到应用时的效率问题等，需要将它们表示成低维向量的形式，因此这种评估是针对表示为低维向量的知识图谱，来探求：低维向量的形式是否能完美代表之前的知识图谱。</p><h1 id="评估数据准备"><a href="#评估数据准备" class="headerlink" title="评估数据准备"></a>评估数据准备</h1><p>那么如何评估这种“完美程度”呢？我们知道在一些tabular数据中使用的模型，比如random forest，GBDT，来一组新的数据，因为有标记，我们预测新数据产生的输出，来与标记进行对比。模型学到的是不同类型数据之间的交互。这种情况下，是通过对新的数据进行prediction来评估模型的学习成果。</p><p>那么在知识图谱表示学习中，模型学到了什么？因为本质上是换了一种表示方法，所以模型学到的是针对同一事务的不同的表示形式，那么就要求在评估时，仍然有同一事务的存在，这么说有点抽象，举个例子：</p><table><thead><tr><th>$x_1$</th><th>$x_2$</th><th>$…$</th><th>$x_{i-1}$</th><th>$x_{i}$</th><th>$y$</th></tr></thead><tbody><tr><td>3</td><td>4</td><td>$…$</td><td>4</td><td>3</td><td>1</td></tr><tr><td>1</td><td>5</td><td>$…$</td><td>9</td><td>0</td><td>0</td></tr><tr><td>2</td><td>1</td><td>$…$</td><td>11</td><td>7</td><td>1</td></tr></tbody></table><p> 假设这是一个tabular数据的预测问题，例如：男生能否脱单。</p><p> 每个维度的向量$x_i$ 代表一种数据，比如：</p><ul><li>$x_1$ 代表你的学历；</li><li>$x_2$ 代表你的家庭背景；</li><li>等等，如此类推…</li></ul><p> 当进行评估时，你所需要的是再出现一个同样格式的数据，每一维的数据的意义和训练集的类似，比如：你不能说在评估时出现一个 $x_{i+1}$ 代表这个人是否会养猪，因为这种类型的数据不在训练集中。</p><p> 同样的，知识图谱表示学习中学到的是一个实体或者关系的低维向量表示，当训练集中出现的是: 高圆圆和Alex, 在评估时，你让我评估 新垣结衣和小约翰(均没有出现在训练集中)的关系。</p><p> 这一点是和之前的评估方法有所区别的。</p><p> 那么如何准备训练集和测试集呢？<strong>在评估数据准备时的一个原则：保证测试集中出现的实体和关系均在训练集中出现。</strong></p><blockquote><p>不同时出现，我评估谁去？谁知道小约翰和新垣结衣合不合适？但我们能评估高圆圆和Alex合不合适… </p></blockquote><h1 id="评估任务"><a href="#评估任务" class="headerlink" title="评估任务"></a>评估任务</h1><p>在KGE评估时，一般采取的是<code>Link prediction</code>任务，在准备时，可以采取的策略：</p><ul><li>在原始的知识图谱上随机删除一些link，然后在评估时预测这些link是否存在，这种情况下，需要计算预测的准确率；</li><li>还有一种方法是，将<code>link prediction</code>视为<code>learn to rank</code>任务，通<strong>过排序的指标进行评估，这种方式目前是主流的方法</strong>，下文主要阐述这种方法；</li></ul><p>对于这种方法，首先确定需要进行评估的数据集 $\mathcal{E}$ ，因为有时候进行评估的数据集可能并不是原始的完整的数据集，然后针对数据集中的三元组，进行两种类型的替换，如下：</p><ul><li><p>$(h, r, ?)$ ：取 $\mathcal{E}$ 中的所有实体，替换？，然后计算 $(h, r, t)$ 在其中的排序，越靠前越好；</p></li><li><p>$(?, r, t)$ ：取 $\mathcal{E}$ 中的所有实体，替换？，然后计算 $(h, r, t)$ 在其中的排序，越靠前越好；</p></li></ul><h1 id="评估指标-Rank-based-method"><a href="#评估指标-Rank-based-method" class="headerlink" title="评估指标(Rank-based method)"></a>评估指标(Rank-based method)</h1><h2 id="Hits-K"><a href="#Hits-K" class="headerlink" title="Hits@K"></a>Hits@K</h2><p>hits@k根据出现在排序列表中的前k个实体中，真实实体排序的位置进行评估。</p><p>具体计算方式如下：</p><p>$$<br>\text{hits@}k = \frac{1}{|\mathcal{I}|} \sum \limits_{r \in \mathcal{I}} \mathbb{I}[r \leq k]<br>$$</p><ul><li>$\mathcal{I}$ 表示一组排序的结果；</li><li>$|\mathcal{I}|$ 表示一组排序的结果的数量；</li><li>$\mathbb{I}$ 为指示函数；</li><li>$k$ 为界定的范围，常为1, 3, 5, 10</li><li>该值越接近1，说明学习的效果越好；</li><li>这里的<code>r</code>指的是<code>rank</code>;</li></ul><p>一个例子说明：</p><p>当有3个正样本用于生成负样本时，同时考虑替换head和tail，可以生成6个负样本，也就可以得到6个排名结果。</p><table><thead><tr><th>正样本</th><th>head负样本</th><th>tail负样本</th><th>head负样本排序</th><th>tail负样本排序</th></tr></thead><tbody><tr><td>$(h_1, r_1, t_1)$</td><td>$(h_1, r_1, ?)$</td><td>$(?, r_1, t_1)$</td><td>8</td><td>4</td></tr><tr><td>$(h_2, r_2, t_2)$</td><td>$(h_2, r_2, ?)$</td><td>$(?, r_2, t_2)$</td><td>2</td><td>1</td></tr><tr><td>$(h_3, r_3, t_3)$</td><td>$(h_3, r_3, ?)$</td><td>$(?, r_3, t_3)$</td><td>90</td><td>1</td></tr></tbody></table><p>则计算如下：</p><p>$$<br>hits@1 = \frac{1}{6} \times (0 + 0 + 0 + 1 + 0 + 1) = 0.3333<br>$$</p><p>$$<br>hits@3 = \frac{1}{6} \times (0 + 0 + 1 + 1 + 0 + 1) = 0.50<br>$$</p><p>$$<br>hits@5 = \frac{1}{6} \times (0 + 1 + 1 + 1 + 0 + 1) = 0.6667<br>$$</p><p>如果你实际计算时会发现一个问题，比如对于 $(h_1, r_1, ?)$，它的排序为8，但是由于比1， 3， 5都大，所以仍然记为0，但是它比 $(h_3, r_3, ?)$ 的排序为90，要好很多，但是对于 $rank &gt; k$ 的，该指标一视同仁，也是一个缺陷。</p><h2 id="Mean-Rank-MR"><a href="#Mean-Rank-MR" class="headerlink" title="Mean Rank(MR)"></a>Mean Rank(MR)</h2><p>正如名字体现的，是triples所有排序的算术平均值，这个值的范围是: [1, 所有负样本的数量]。</p><ul><li>1 表明是最理想的情况，所有的triples的排序都是1；</li><li>所有负样本的数量：最坏的情况，所有的排序都是最后一个；</li><li>这个指标越低越好；</li></ul><p>$$<br>\text{score} =\frac{1}{|\mathcal{I}|} \sum \limits_{r \in \mathcal{I}} r<br>$$</p><p>根据上述的6个负样本，计算如下：</p><p>$$<br>\text{MR} = \frac{1}{6} \times (8 + 4 + 2 + 1 + 90 + 1) = 17.6667<br>$$</p><p>从计算方式上来看，这个评估指标比<code>Hits@K</code>好的地方在于：</p><blockquote><p>it is sensitive to any model performance changes, not only what occurs under a certain cutoff and therefore reflects average performance.</p></blockquote><p>但是它的缺陷也很明显，虽然有一定的解释性，但是由于它的边界取决于负样本的数量，但是MR=10，对于负样本为20或者200000的模型，其性能指示含义完全不同。</p><h2 id="Mean-reciprocal-rank-MRR"><a href="#Mean-reciprocal-rank-MRR" class="headerlink" title="Mean reciprocal rank(MRR)"></a>Mean reciprocal rank(MRR)</h2><p>MRR是所有排名的倒数的算术平均值，具体计算如下：</p><p>$$<br>\text{score} =\frac{1}{|\mathcal{I}|} \sum_{r \in \mathcal{I}} r^{-1}<br>$$</p><p>根据上述的6个负样本，计算如下：</p><p>$$<br>\text{MR} = \frac{1}{6} \times (\frac{1}{8} + \frac{1}{4} + \frac{1}{2} + \frac{1}{1} + \frac{1}{90} + \frac{1}{1}) = 0.4810<br>$$</p><ul><li>这个指标在 $[0, 1]$ 之间;</li><li>是对MR的一种改进，使得可以对异常值不太敏感，这里指对高rank的异常值不敏感，但是去开始对低rank值敏感；</li></ul><p>同样的问题，对于一个模型，如果观察到MRR指标为0.01， 那么这个模型是好还是不好？</p><p>这种结论很难直接下，这个值意味着，当去掉outliers时，平均排名为 100(1/0.01) 左右，这个结果可能是好的，也可能是坏的，这取决于使用的负样本的数量。</p><ul><li>当使用了100万的负样本, 那么这个结果很好，因为在100万中排名100，是很好的；</li><li>当使用了负样本只有100个，说明test triples在跟corruptions一起排名时得到的结果很差；</li></ul><p>在真实的数据集上，应该仔细看看hits@k这个指标，之后再判断模型的好坏。</p><p>其中k的选择，应该取决于针对每个test triple生成的负样本的数量。</p><blockquote><p>其实主要的评估指标，包括：<code>MRR</code>, <code>hits@k</code>, 已经是经常使用的了，例如在：<a href="https://paperswithcode.com/task/link-prediction">paperswithcode</a>和<a href="https://www.jiqizhixin.com/sota/tech-task/90c2aa81-2fb1-4363-a379-53edc69b3898">机器之心</a>的SOTA中都是以这几种组作为benchmark的。</p></blockquote><h2 id="其他的指标"><a href="#其他的指标" class="headerlink" title="其他的指标"></a>其他的指标</h2><p>针对上述常用的指标，也有一些改进的衡量指标，比如：</p><ul><li>Inverse Geometric Mean Rank</li><li>Adjusted Mean Rank</li><li>Adjusted Mean Rank Index</li></ul><p>这些指标可以参考 <a href="https://pykeen.readthedocs.io/en/stable/tutorial/understanding_evaluation.html">其他衡量指标</a></p><h1 id="深入评估细节"><a href="#深入评估细节" class="headerlink" title="深入评估细节"></a>深入评估细节</h1><h2 id="如何计算rank"><a href="#如何计算rank" class="headerlink" title="如何计算rank?"></a>如何计算rank?</h2><p>上边我们默认已经知道rank，但是rank是如何计算的，不就是排序列表的索引吗？</p><p>对于一个模型，输出针对triples的分数如下：</p><table><thead><tr><th>Triples</th><th>Score</th><th>Rank(optimistic)</th><th>Rank(pessimistic)</th><th>Rank(realistic)</th></tr></thead><tbody><tr><td>(高圆圆，喜欢，XXX)</td><td>0.9628</td><td>1</td><td>1</td><td>1</td></tr><tr><td>(高圆圆，喜欢，Alex)</td><td>0.9405</td><td>2</td><td>3</td><td>2.5</td></tr><tr><td>(高圆圆，喜欢，赵又廷)</td><td>0.9405</td><td>3</td><td>2</td><td>2</td></tr><tr><td>(高圆圆，喜欢，小约翰)</td><td>0.0001</td><td>4</td><td>4</td><td>4</td></tr></tbody></table><p>但是会出现一个问题：如果模型针对不同的triples输出同样的分数时，如何计算rank？这时候有三种方式：</p><ul><li><code>optimistic</code>：乐观的方式，当正样本和其他的负样本有同样的分数时，则排名取第一个；</li><li><code>pessimistic</code>：悲观的方式，当正样本和其他的负样本有同样的分数时，则排名取最后一个；</li><li><code>realistic</code>：务实的方式，当正样本和其他的负样本有同样的分数时，则排名取<code>optimistic</code>和<code>pessimistic</code>的平均排名；</li></ul><h2 id="rank选边"><a href="#rank选边" class="headerlink" title="rank选边"></a>rank选边</h2><p>上述我们一直默认，当生成负样本时，是针对head和tail同时进行的，但是其实可以任选一边，即：</p><ul><li>只针对head生成负样本；</li><li>只针对tail生成负样本；</li></ul><p>但是一般情况下，我们都是针对两边同时生成负样本。</p><p>但是指针对以便进行负样本的生成是有意义的，因为这样可以评估模型对于head和tail的预测差异。</p><h2 id="过滤已知triples"><a href="#过滤已知triples" class="headerlink" title="过滤已知triples"></a>过滤已知triples</h2><p>这个很明显，当生成负样本时，我们发现，针对(高圆圆，喜欢，Alex)，替换head，得到新的负样本的head可以任意替换，那么，恰巧生成一个负样本 (孙艺珍，喜欢，Alex)，这就尴尬了，因为这是一个正样本，当评估时必然会得到很高的分数，导致得到排名很前的rank，这就导致(高圆圆，喜欢，Alex)的排名靠后了，因此为了正确地衡量模型的性能，因此需要过滤掉这些正确的样本，因为它会忽略这些一直的triples带来的负面影响。</p><p>但是这种filtering的使用时机是很重要的：<strong>一个原则是评估的结果不会影响到模型的训练过程</strong>。根据这个原则，不可以使用filtering：</p><ul><li>Early stopping：不使用test data进行正样本的过滤，以免泄露信息；</li><li>hyperparameter optimization：不使用test data进行正样本的过滤，以免泄露信息；</li></ul><h2 id="实体和关系限定"><a href="#实体和关系限定" class="headerlink" title="实体和关系限定"></a>实体和关系限定</h2><p>这个问题需要阐述的是，上边根据LCWA或者sLCWA，可以生成针对所有实体或者关系的负样本，但是依赖于具体的任务，我们不想这么做，比如：在医疗知识图谱中，每个实体会有额外的实体类型，比如:</p><p>(氯吡格雷，治疗，过敏)，氯吡格雷是一种药物，可以治疗过敏，但是实体中有二甲双胍，这也是一种药品，但是根据上述的假设，则可以生成负样本：(氯吡格雷，治疗，二甲双胍)，这显然是没有意义的，因为在不符合实际，因为这种情况下，当生成负样本时，我们一般希望规定，生成的负样本时符合实际的，比如：(氯吡格雷，治疗，过敏性鼻炎)。</p><p>这种情况下，需要完成两步工作：</p><ul><li>重新生成评估数据；</li><li>针对重新生成的评估数据，来计算衡量指标；</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法研究 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱表示学习：训练方法</title>
      <link href="/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-xun-lian-fang-fa/"/>
      <url>/2021/11/17/zhi-shi-tu-pu-biao-shi-xue-xi-xun-lian-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="前提假设"><a href="#前提假设" class="headerlink" title="前提假设"></a>前提假设</h1><h2 id="Open-World-Assumption-OWA"><a href="#Open-World-Assumption-OWA" class="headerlink" title="Open World Assumption(OWA)"></a>Open World Assumption(OWA)</h2><p>开放世界假设，其含义在于暗含的假定为是未知的事实而不是假的，这也是“开放”这个词的含义，这么理解起来有点抽象，一个例子：宇宙内有除了人类的外星人，这个描述在Open World Assumption条件下，不是假的，你只能说它是未知的，这个问题的答案是“开放”的。</p><h2 id="Closed-World-Assumption-CWA"><a href="#Closed-World-Assumption-CWA" class="headerlink" title="Closed World Assumption(CWA)"></a>Closed World Assumption(CWA)</h2><p>封闭世界假设，与开放对立，即：所有未知的都是假的。同样上述的例子：宇宙内有除了人类的外星人，这个描述在Closed World Assumption条件下，我们均认为它是错误的。</p><h2 id="Local-Closed-World-Assumption-LCWA"><a href="#Local-Closed-World-Assumption-LCWA" class="headerlink" title="Local Closed World Assumption(LCWA)"></a>Local Closed World Assumption(LCWA)</h2><p>这个是什么意思呢？与Closed World Assumption相关，这涉及到局部的情况。同样上述的例子：宇宙内有除了人类的外星人，比如：对于不同的人类，这个描述在Local Closed World Assumption条件下，不同的人可能有不同的回答，对于NASA或者国家航天局来说，他们可能知道但是不说，因此他们对这个的回答可能是正确的，而对于我们来说，一般来说是错误的，因为你肯定不知道。这种情况导致不同的人群对于同样的问题出现认知的不同，所以Closed World Assumption假设过于绝对了，因此引入了Local Closed World Assumption，即：对于所有未知的知识中的一部分认为是错误的（因为有人认为那是对的）。</p><h2 id="Stochastic-Local-Closed-World-Assumption-sLCWA"><a href="#Stochastic-Local-Closed-World-Assumption-sLCWA" class="headerlink" title="Stochastic Local Closed World Assumption(sLCWA)"></a>Stochastic Local Closed World Assumption(sLCWA)</h2><p>这个假设，是在Local Closed World Assumption基础之上构建的。其实这个词相比于上述三者使用的较少，<strong>它需要和具体的采样策略结合来看</strong>。因为人类的知识没有办法具体的统计出来，因此日常接触到的知识均可以视为Local Knowledge, 而基于Local Closed World Assumption，所有不属于Local Knowledge的知识都是错误的。Stochastic Local Closed World Assumption则告诉我们，如果在这些“错误”的知识中进行采样，来帮助训练。</p><h1 id="不同假设对于训练的影响"><a href="#不同假设对于训练的影响" class="headerlink" title="不同假设对于训练的影响"></a>不同假设对于训练的影响</h1><p>这种假设条件对于知识图谱嵌入的影响如下：</p><table><thead><tr><th>Assumptions</th><th>影响</th><th>采用情况</th><th>说明</th></tr></thead><tbody><tr><td>Open World Assumption</td><td>会导致欠拟合under-fitting，也即over-generalization)</td><td>一般不用</td><td>直观理解，当一个模型对于一个自己认知之外事务，不明确表达态度时，其实是一种“缺乏自信”的表现，也会对所有已知和未知的事务同样保持“中庸”。</td></tr><tr><td>Closed World Assumption</td><td>会导致over-fitting，即泛化程度很低</td><td>一般不用</td><td>这种假设太过绝对，只要我不知道，就是错误的，有点过于“自负”，这种模型除了自己知道的事务，对其他均不认可。</td></tr><tr><td>Local Closed World Assumption</td><td>根据已有的知识，生成一部分“假”知识</td><td>可用</td><td></td></tr><tr><td>Stochastic Local Closed World Assumption</td><td>根据已有的知识，生成一部分“假”知识，从这些假知识中进行采样</td><td>可用</td><td></td></tr></tbody></table><h1 id="如何生成“假”知识？"><a href="#如何生成“假”知识？" class="headerlink" title="如何生成“假”知识？"></a>如何生成“假”知识？</h1><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>来自于 <a href="https://pykeen.readthedocs.io/en/stable/reference/training.html">source</a>。</p><blockquote><p>Throughout the following explanations of training loops, we will assume the set of entities $\mathcal{E}$ , set of relations $\mathcal{R}$ , set of possible triples $\mathcal{T} = \mathcal{E} \times \mathcal{R} \times \mathcal{E}$. We stratify $\mathcal{T}$ into the disjoint union of positive triples $\mathcal{T^{+}} \subseteq \mathcal{T}$ and negative triples $\mathcal{T^{-}} \subseteq \mathcal{T}$ such that $\mathcal{T^{+}} \cap \mathcal{T^{-}} = \emptyset$ and $\mathcal{T^{+}} \cup \mathcal{T^{-}} = \mathcal{T}$ .</p></blockquote><h2 id="“朴素”的方法"><a href="#“朴素”的方法" class="headerlink" title="“朴素”的方法"></a>“朴素”的方法</h2><p>对于一个已有的知识图谱而言，根据Closed World Assumption，任何不在该知识图谱内的知识或者三元组都是假知识。</p><p>一种“朴素”的负样本的生成办法是：</p><p>对于三元组，可能的组合有以下几种：</p><ul><li>N(head): 代表head实体的种类数量；</li><li>N(rel): 代表relation的种类数量；</li><li>N(tail)：代表tail实体的种类数量；</li></ul><p>则可能的三元组的数量为：</p><p>$$<br>N(head) \times N(rel) \times N(tail)<br>$$</p><p>当删除知识图谱内的三元组时，就可以认为是所有的负样本。这种负样本的数量是巨大的。</p><p>一种可以解决方法是：采样。具体的采样方法有很多种，这里不赘述了。</p><p>但是有一种情况，我们的模型需要<strong>更容易地对真知识进行判断</strong>，举个例子：</p><p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTM15gYJOEGxQqr5jBsWUzpAw6RJjvCzus8MA&usqp=CAU" alt="高圆圆"></p><p>对比，高圆圆比娜扎漂亮，我们假设它是对的，当出现凤姐时，模型已经认识高圆圆，它能得出高圆圆比凤姐漂亮。但是当得到的负样本为：梅根福克斯与克劳馥时，你觉得模型对：</p><ul><li>高圆圆与凤姐</li><li>梅根福克斯与克劳馥</li></ul><p>哪个容易判断。</p><h2 id="LCWA下的负样本生成"><a href="#LCWA下的负样本生成" class="headerlink" title="LCWA下的负样本生成"></a>LCWA下的负样本生成</h2><p>基于上述的情况，在Local Closed World Assumption下，<strong>负样本的生成基于以下原则：任意替换三元组的任意一个位置，得到的负样本不属于知识图谱本身，即认为是负样本。</strong></p><p>根据这个原则，可以有三种类型的操作：</p><ul><li><p>head generation: $(h, r, t)$ -&gt; $(h, r, t_i)$;</p><p>   In this setting, for any triple $(h, r, t) \in \mathcal{K}$ that has been observed, a set<br>   $\mathcal{T^{-}} (h, r) $ of negative examples is created by considering all triples $(h, r, t_i) \notin \mathcal{K}$ as false.</p></li><li><p>relation generation: $(h, r, t)$ -&gt; $(h, r_i, t)$; </p><p>   In this setting, for any triple $(h, r, t) \in \mathcal{K}$ that has been observed, a set<br>   $\mathcal{T^{-}} (h, t) $ of negative examples is created by considering all triples $(h, r_i, t) \notin \mathcal{K}$ as false.</p></li><li><p>tail generation: $(h, r, t)$ -&gt; $(h_i, r, t)$;</p><p>   In this setting, for any triple $(h, r, t) \in \mathcal{K}$ that has been observed, a set<br>   $\mathcal{T^{-}} (r, t) $ of negative examples is created by considering all triples $(h_i, r, t) \notin \mathcal{K}$ as false.</p></li></ul><p>一般情况下，在Local Closed World Assumption下，很多的实现都只会考虑head generation和relation generation，不会考虑tail generation。</p><h2 id="sLCWA下的负样本生成"><a href="#sLCWA下的负样本生成" class="headerlink" title="sLCWA下的负样本生成"></a>sLCWA下的负样本生成</h2><p>从Local Closed World Assumption中的三种情况下进行的集合进行采样，即：</p><ul><li>$(h, r, t)$ -&gt; $(h, r, t_i)$</li><li>$(h, r, t)$ -&gt; $(h, r_i, t)$</li><li>$(h, r, t)$ -&gt; $(h_i, r, t)$</li></ul><p>但从实际来看，有时候只需要$(h, r, t)$ -&gt; $(h, r, t_i)$和$(h, r, t)$ -&gt; $(h_i, r, t)$，而没有考虑关系的替换，然后从这两者的集合中进行采样。</p><table><thead><tr><th>Assumptions</th><th>Local Closed World Assumption</th><th>Stochastic Local Closed World Assumption</th></tr></thead><tbody><tr><td>负样本来源</td><td>$(h, r, t)$ -&gt; $(h, r, t_i)$， $(h, r, t)$ -&gt; $(h, r_i, t)$</td><td>$(h, r, t)$ -&gt; $(h, r, t_i)$，$(h, r, t)$ -&gt; $(h_i, r, t)$</td></tr><tr><td>是否要采样</td><td>不需要</td><td>需要</td></tr><tr><td>过滤正样本</td><td>需要</td><td>需要</td></tr></tbody></table><h2 id="例子说明"><a href="#例子说明" class="headerlink" title="例子说明"></a>例子说明</h2><p>这个具体的帮助理解的例子来自于:<br><a href="https://arxiv.org/abs/2006.13365">Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework</a></p><p><img src="https://pykeen.readthedocs.io/en/stable/_images/training_approaches.png"></p><p>该图对比了负样本在：</p><ul><li>Local Closed World Assumption</li><li>Stochastic Local Closed World Assumption<br>两种生成策略。</li></ul><p>对于同一种关系 <code>works_at</code>, 红色部分是true triples。</p><ul><li>在LCWA生成的是深蓝色对应的负样本，它们都没在原始的知识图谱上。</li><li>在sLCWA生成的是浅蓝色对应的负样本，并从其中采样，它们都没在原始的知识图谱上。</li><li>黄色部分则不在负样本的考虑之列。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法研究 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读 - Convolutional 2D Knowledge Graph Embeddings</title>
      <link href="/2021/11/14/convkb/"/>
      <url>/2021/11/14/convkb/</url>
      
        <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>Link prediction是在知识图谱中预测实体之间的关系的任务，在查询扩展和语义关系预测中需要解决的，也是本论文致力于解决的。</p><p>在解决Link prediction, 其核心考虑因素在于：</p><blockquote><p>link predictors should scale in a manageable way with respect to <strong>both the number of parameters and computational costs</strong> to be applicable in real-world scenarios.</p></blockquote><p>同时需要考虑计算成本，还是考虑模型本身的预测性能，而参数数量是模型性能的一个指标。</p><p>之前方法的问题在于：</p><blockquote><p>Previous work on link prediction has <strong>focused on shallow, fast models</strong> which can scale to large knowledge graphs. However, these models <strong>learn less expressive features</strong> than deep, multi-layer models which potentially limits performance.</p></blockquote><p>简单来讲，之前的方法过于关注在大型知识图谱上进行快速的学习，这需要网络不太复杂。但是其架构学习到的特征不够丰富，性能不够强。</p><p>为了提高知识图谱上的预测性能，从而提高表现力，在不使用深层模型的前提下，只能提高embedding size，但是难以应用到大型知识图谱上。</p><p>换一种思路，使用深度模型，就可以减少embedding size，因为可以学到高层特征，但是在过往的模型中，深度模型中的使用的架构都是全连接层，因此会造成过拟合问题。</p><p>以上问题的一种解决办法是：</p><blockquote><p>use parameter efﬁcient, fast operators which can be composed into deep networks.</p></blockquote><p>因此，自然而然，卷积的操作就被引入了，因为以下特点：</p><ul><li>parameter efﬁcient</li><li>fast to compute: highly optimised GPU implementations</li></ul><p>本文提出来的模型：ConvE, a multi-layer convolutional network model for link prediction，这是一个基于卷积的多层架构。它的优点如下：</p><ul><li><strong>highly parameter efﬁcient</strong>: yielding the same performance as <strong>DistMult</strong> and <strong>R-GCN</strong> with 8x and 17x fewer parameters;</li><li><strong>particularly effective at modelling nodes with high indegree</strong>: common in highlyconnected, complex knowledge graphs such as Freebase and YAGO3;</li></ul><h1 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h1><h2 id="2-1-1D-vs-2D-Convolutions"><a href="#2-1-1D-vs-2D-Convolutions" class="headerlink" title="2.1 1D vs 2D Convolutions"></a>2.1 1D vs 2D Convolutions</h2><p>NLP任务中，多数使用1D卷积进行操作，即在文本的序列方向上进行卷积操作，本文使用2D卷积，不仅在文本序列方向进行操作，同时还在embedding的纵向上进行操作，这使用卷积操作捕捉到的交互信息更丰富。</p><p>这种操作的优点在于： increases the expressiveness of our model through additional points of interaction between embeddings.</p><p>这种操作如何理解？</p><p>当把1D的embedding 进行拼接时，你仍然需要得到1D的embedding，那么拼接的方法只能是在一维上，如下例：</p><p><img src="/images/conve/lex0.png"></p><p>其中假如你的卷积核k=3, 那么能捕捉到的交互只有最临近的a和b, 除非你讲卷积核的尺寸增加，这样才能捕捉到更多的交互。</p><p>当换到2D时，由于是在二维的方向上进行拼接和堆叠，因此其方式可以有多种，因此当卷积进行操作时，可以捕捉的信息可以是左右方向的，也可以是上下方向的，如下图：</p><p><img src="/images/conve/lex1.png"></p><p>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的提升交互次数:</p><p><img src="/images/conve/lex2.png"></p><p>因此在2D条件下，捕捉到的交互信息数量是不单单与卷积核有关，而且还与矩阵的尺寸有关。</p><h2 id="2-2-问题形式化"><a href="#2-2-问题形式化" class="headerlink" title="2.2 问题形式化"></a>2.2 问题形式化</h2><p>link prediction 可以认为是 a pointwise learning to rank problem。具体而言，对每个输入的三元组 triples: $x = (s, r, o)$, 目标是 learning a scoring function $\psi(x)$, 其结果正比于x为true的likelihood。</p><h2 id="2-3-Neural-Link-Predictors"><a href="#2-3-Neural-Link-Predictors" class="headerlink" title="2.3 Neural Link Predictors"></a>2.3 Neural Link Predictors</h2><p>这个组件是干嘛的，深度学习中总有一些为了唬人提出的名词，其实这个predictors就是一个多层神经网络，包括：</p><ul><li>encoding component: 对 $x = (s, r, o)$, 该部分将subject和object映射为embeddings, $e_s$, $e_o$; </li><li>scoring component: 使用 $\psi_r$ 对embeddings评分，即: $\psi(s, r, o) = \psi_r(e_s, e_o)$ </li></ul><p>一些经常使用的典型predictors如下: </p><p><img src="/images/conve/1.png" alt="经常使用的典型predictors"></p><h2 id="2-4-模型主要内容"><a href="#2-4-模型主要内容" class="headerlink" title="2.4 模型主要内容"></a>2.4 模型主要内容</h2><h3 id="2-4-1-Scoring-function"><a href="#2-4-1-Scoring-function" class="headerlink" title="2.4.1 Scoring function"></a>2.4.1 Scoring function</h3><p>scoring function定义如下:<br>$$<br>\psi_{r}\left(\mathbf{e}_s, \mathbf{e}_o\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_s} ; \overline{\mathbf{r}_r}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_o<br>$$</p><p>一些符号如下:</p><ul><li><p>$\mathbf{e}_s, \mathbf{e}_o$ 分别代表头实体和尾实体的Embedding；</p></li><li><p>$\overline{\mathbf{e}_s}, \overline{\mathbf{r}_r}$ 分别代表Reshape后的头实体和关系向量，这种操作如下：如果 $\mathbf{e}_s, \mathbf{r}_r \in \mathbb{R}^k$, 那么 $\overline{\mathbf{e}_s}, \overline{\mathbf{r}_r} \in \mathbb{R}^{k_w \times k_h} $, 则 $k = k_w  k_h$</p></li><li><p>$\omega$ 代表卷积核；</p></li><li><p>$\mathbf{W}$ 代表投影矩阵；</p></li></ul><h3 id="2-4-2-模型架构"><a href="#2-4-2-模型架构" class="headerlink" title="2.4.2 模型架构"></a>2.4.2 模型架构</h3><p><img src="/images/conve/2.png" alt="ConvE"></p><p><code>ConvE</code>的整个训练过程如下.</p><ul><li>先通过Embedding的方式分别获得头实体表示 $\mathbf{e}_s$ 和关系表示 $\mathbf{r}_r$ ；</li><li>将头实体和关系表示先<code>Concat</code>起来, 然后将其<code>Reshape</code>到某一种尺寸, 此时头实体和关系的表示记为 $\left[ \overline{\mathbf{e}_s} ; \overline{\mathbf{r}_r} \right]$；</li><li>接着利用卷积抽取Reshape后的二维向量, 也就是对头实体和关系的交互信息进行捕捉；</li><li>利用卷积(可以是任意数量的卷积核)抽取完信息后, 将所有的特征打平成一个一维向量；</li><li>通过投影矩阵 $\mathbf{W}$ 投影到一个中间层中，输出的尺寸与embedding size相同，以便与尾实体表示 $\mathbf{e}_o$ 做内积, 获得相似度, 即Logits；</li><li>这种方式通过内积来比较所获向量与尾实体的相似度, 越相似得分越高.</li><li>然后将Logits经过 $\sigma$ 函数, 得到每个实体的概率：  $$p=\sigma(\psi_r\left(\mathbf{e}_s, \mathbf{e}_o \right))$$</li></ul><p>优化时的损失函数采用BCE(binary cross-entropy loss)：</p><p>$$\mathcal{L}(p, t)=-\frac{1}{N} \sum_i\left(t_i \cdot \log \left(p_i \right)+\left(1-t_i \right) \cdot \log \left(1-p_i \right)\right)$$</p><p>$t$ 是尾实体的one-hot vector. 对于和输入的 $(s, r, ?)$ 匹配的位置为1，其余为0.</p><h3 id="2-4-3-训练tips"><a href="#2-4-3-训练tips" class="headerlink" title="2.4.3 训练tips"></a>2.4.3 训练tips</h3><h4 id="2-4-3-1-基本tips"><a href="#2-4-3-1-基本tips" class="headerlink" title="2.4.3.1 基本tips"></a>2.4.3.1 基本tips</h4><ul><li>rectiﬁed linear units: as the non-linearity $f$, 加快训练；</li><li>batch normalization: after each layer to stabilise</li><li>regularise: dropout</li><li>optimiser: Adam</li><li>label smoothing: to lessen overﬁtting due to saturation of output non-linearities at the labels</li></ul><h4 id="2-4-3-2-加速评估tips"><a href="#2-4-3-2-加速评估tips" class="headerlink" title="2.4.3.2 加速评估tips"></a>2.4.3.2 加速评估tips</h4><p>卷积操作消耗大量时间</p><blockquote><p>convolution consumes about 75-90% of the total computation time, thus it is important to minimise the number of convolution operations to speed up computation</p></blockquote><p>思路1：增加batch size加速，但是CNN会使得GPU的内存超过限制；</p><p>解决办法：1-N scoring</p><p>ConvE最后的输出, 能获得对所有实体相关的Logits, 这样就能<strong>对所有的尾实体同时打分</strong>, <strong>而不用考虑采样的问题</strong>. 在原文中这种打分方式被称为<strong>1-N Scoring</strong>.</p><p>过去评估时，需要采样负样本，进行1-1评估。现在这种方式能极大地加快Evaluation的速度, 因为负采样只能对单一的三元组打分, 而这种方式能同时对所有的尾实体同时打分。这种思想能够应用于所有的1-1 scoring Model.</p><p>这种方式其实本质上利用GPU并行执行的特点，在架构上将训练和评估同时考虑，通过将平衡计算性能和收敛速度，来使得评估过程加快。</p><h1 id="3-实验评估"><a href="#3-实验评估" class="headerlink" title="3. 实验评估"></a>3. 实验评估</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><table><thead><tr><th>数据集</th><th>来源</th><th>关系</th><th>实体</th><th>三元组</th><th>说明</th></tr></thead><tbody><tr><td>WN18</td><td>a subset of WordNet</td><td>18</td><td>40943</td><td>151442</td><td>consist of hyponym and hypernym relations and, for such a reason, WN18 tends to follow a strictly hierarchical structure. 用WN18RR替代。</td></tr><tr><td>FB15k</td><td>a subset of Freebase</td><td>1345</td><td>14951</td><td>——</td><td>A large fraction of content in this knowledge graph describes facts about movies, actors, awards, sports, and sport teams. 用FB15k-237替代。</td></tr><tr><td>YAGO3-10</td><td>a subset of YAGO3</td><td>37</td><td>123182</td><td>entities which have a minimum of 10 relations each</td><td>Most of the triples deal with descriptive attributes of people, such as citizenship, gender, and profession.</td></tr><tr><td>Countries</td><td></td><td></td><td></td><td></td><td>a benchmark dataset that is useful to evaluate a model’s ability to learn long-range dependencies between entities and relations. It consists of three sub-tasks which increase in difﬁculty in a step-wise fashion, where the minimum pathlength to ﬁnd a solution increases from 2 to 4.</td></tr></tbody></table><h2 id="3-2-超参"><a href="#3-2-超参" class="headerlink" title="3.2 超参"></a>3.2 超参</h2><ul><li><p>选择方法: <code>grid search</code> according to the <code>mean reciprocal rank (MRR)</code> on the validation set</p></li><li><p>选定范围:</p><ul><li><code>embedding dropout</code>: 0.0, 0.1, 0.2</li><li><code>feature map dropout</code>: 0.0, 0.1, 0.2, 0.3</li><li><code>projection layer dropout</code>: 0.0, 0.1, 0.3, 0.5</li><li><code>embedding size</code>: 100, 200</li><li><code>batch size</code>: 64, 128, 256</li><li><code>learning rate</code>: 0.001, 0.003</li><li><code>label smoothing</code>: 0.0, 0.1, 0.2, 0.3</li></ul></li><li><p>最佳参数:</p><ul><li>WN18, YAGO3-10 and FB15k<ul><li><code>embedding dropout</code>: 0.2</li><li><code>feature map dropout</code>: 0.2</li><li><code>projection layer dropout</code>: 0.3</li><li><code>embedding size</code>: 200</li><li><code>batch size</code>: 128</li><li><code>learning rate</code>: 0.001</li><li><code>label smoothing</code>: 0.1</li></ul></li><li>Countries dataset<ul><li><code>embedding dropout</code>: 0.3</li><li><code>hidden dropout</code>: 0.5</li><li><code>label smoothing</code>: 0</li></ul></li><li><code>early stopping</code> according to the <code>mean reciprocal rank</code> (WN18, FB15k, YAGO3-10) and <code>AUC-PR</code> (Countries) statistics on the validation set</li></ul></li></ul><h2 id="3-3-结果"><a href="#3-3-结果" class="headerlink" title="3.3 结果"></a>3.3 结果</h2><p>实验中进行评估时，有几件注意事项：</p><ul><li><p>由于数据出现leakage，因此使用了rule-based method来识别逆向关系作为对照，同时在数据集包括有无逆向关系)中进行评估；</p></li><li><p>使用了<code>filtered setting</code>;</p><blockquote><p>Rank test triples against all other candidate triples not appearing in the training, validation, or test set.</p></blockquote><blockquote><p>Candidates are obtained by permuting either the subject or the object of a test triple with all entities in the knowledge graph.</p></blockquote></li></ul><h3 id="3-3-1-从参数效率看"><a href="#3-3-1-从参数效率看" class="headerlink" title="3.3.1 从参数效率看"></a>3.3.1 从参数效率看</h3><p><img src="/images/conve/3.png" alt="参数数量和性能(ConvE vs. DistMult)"></p><h3 id="3-3-2-结果-含有逆向关系"><a href="#3-3-2-结果-含有逆向关系" class="headerlink" title="3.3.2 结果(含有逆向关系)"></a>3.3.2 结果(含有逆向关系)</h3><p><img src="/images/conve/4.png" alt="结果(含有逆向关系)"></p><h3 id="3-3-3-结果-不含逆向关系"><a href="#3-3-3-结果-不含逆向关系" class="headerlink" title="3.3.3 结果(不含逆向关系)"></a>3.3.3 结果(不含逆向关系)</h3><p>这里将数据集中存在逆向关系的三元组全部删除了，来避免leakage造成的负面影响。</p><p><img src="/images/conve/5.png" alt="结果(不含逆向关系)1"></p><p><img src="/images/conve/6.png" alt="结果(不含逆向关系)2"></p><h2 id="3-4-分析"><a href="#3-4-分析" class="headerlink" title="3.4 分析"></a>3.4 分析</h2><h3 id="3-4-1-消融实验"><a href="#3-4-1-消融实验" class="headerlink" title="3.4.1 消融实验"></a>3.4.1 消融实验</h3><p>为了查看，哪部分组件在整个架构中的作用最重要，消融实验显示：</p><p><img src="/images/conve/7.png" alt="消融实验结果"></p><ul><li>hidden dropout的影响最大;</li><li>但是label smoothing的影响几乎可以忽略；</li></ul><h3 id="3-4-2-从图的结构分析优点"><a href="#3-4-2-从图的结构分析优点" class="headerlink" title="3.4.2 从图的结构分析优点"></a>3.4.2 从图的结构分析优点</h3><h4 id="3-4-2-1-假设1"><a href="#3-4-2-1-假设1" class="headerlink" title="3.4.2.1 假设1"></a>3.4.2.1 假设1</h4><p>datasets contain nodes with very high relation-speciﬁc indegree时，<code>ConvE</code>效果更好，而indegree较小时，一些模型足以应对<code>DistMult</code>。</p><blockquote><p>Our hypothesis is that deeper models, that is, models that learn multiple layers of features, like ConvE, have an advantage over shallow models, like DistMult, to capture all these constraints.</p></blockquote><ul><li>验证：通过将数据中indegree中的过大或者过小的node删除，然后分别使用ConvE和DistMult进行实验，验证了上述假设。</li></ul><h4 id="3-4-2-2-假设2"><a href="#3-4-2-2-假设2" class="headerlink" title="3.4.2.2 假设2"></a>3.4.2.2 假设2</h4><p>平均PageRank越高的graph，ConvE的性能相比于<code>DistMult</code>越好；</p><blockquote><p>This gives additional evidence that models that are deeper have an advantage when modelling nodes with high (recursive) indegree.</p></blockquote><ul><li>验证：通过计算各个数据集的平均pagerank值，然后计算pagerank值与(convE - DistMult)差值计算相关性，验证了上述假设，这个假设2与假设1其实可以看成等价。</li></ul><h1 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4. 代码实现"></a>4. 代码实现</h1><ul><li><p><a href="https://github.com/TimDettmers/ConvE">原文实现</a></p></li><li><p><a href="https://github.com/Accenture/AmpliGraph">AmpliGraph</a></p></li></ul><h1 id="5-问题"><a href="#5-问题" class="headerlink" title="5. 问题"></a>5. 问题</h1><ol><li>与CV中CNN的应用对比，模型仍是浅层，未来会增加深度的卷积模型。</li><li>2D卷积的解释；</li><li>如何更多地捕捉embedding之间的交互，例如通过增加大型结构；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 表示学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《美国陷阱》中阿尔斯通被肢解背后的经济战</title>
      <link href="/2021/11/13/mei-guo-xian-jing-zhong-a-er-si-tong-bei-zhi-jie-bei-hou-de-jing-ji-zhan/"/>
      <url>/2021/11/13/mei-guo-xian-jing-zhong-a-er-si-tong-bei-zhi-jie-bei-hou-de-jing-ji-zhan/</url>
      
        <content type="html"><![CDATA[<p>疫情在家，除了完成正常的工作学习外，由于没啥好的娱乐项目，读书是一种廉价的消遣方式。正巧遇到一本书《美国陷阱》，其中的情节和阴谋论恐怕电影都拍不出来，甚至可以当成悬疑小说来读。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/1.png" alt="《美国陷阱》"></p><h1 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1. 背景介绍"></a>1. 背景介绍</h1><p>最近几年，美国加大对中国的打压力度，尤其是科技领域，其中以华为最为大家所熟悉。那么最为人所知伎俩即，扣押孟晚舟（任正非的女儿和华为的高管）。大家肯定跌破眼镜，这种流氓式的手段，对于被称为“世界最大的民主国家”的美国，怎么干得出来？</p><p>当然，理由还是要有的，不然无法说服世界舆论，其实估计也没人信。因为这种套路也不是第一次了，本书讲得就是同样的套路，应用到另一个公司的另一个人身上。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/2.png" alt="两起事件的对比"></p><p>而本书，讲得就是阿尔斯通的故事，而且是由当事人弗雷德里克-皮耶鲁齐自己陈述的。因此对于我们具有极大的参考价值。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/3.png" alt="这位老哥比孟晚舟惨得多"></p><h1 id="2-阿尔斯通与中国"><a href="#2-阿尔斯通与中国" class="headerlink" title="2. 阿尔斯通与中国"></a>2. 阿尔斯通与中国</h1><p>阿尔斯通是法国一家有战略意义的工业巨头，关于它的内容，大家可以在网上搜索，我这里不过多阐述，大家认识到这是一家工业巨头就可以了，法国的核电站都是它来维护的，舰艇的燃气轮机也是它提供的。</p><p>而他与中国的渊源也是颇深，三峡水利工程、向家坝水利工程等好多项目，都有它的参与，<strong>甚至书中还披露，当然是阿尔斯通的认罪协议中，阿尔斯通曾经为参与台北地铁项目，而贿赂相关负责人。</strong></p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/4.png" alt="阿尔斯通"></p><h1 id="3-书中主要内容"><a href="#3-书中主要内容" class="headerlink" title="3. 书中主要内容"></a>3. 书中主要内容</h1><p>为了大家能够明白书中的主要线索，我首先用图的方式表达其中涉及到的参与人和机构。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/5.png" alt="当事人和相关关系"></p><ol><li>首先，美国司法部质控阿尔斯通违反《反腐败法》，逮捕作者，也就是皮耶鲁齐，希望他认罪，在阿尔斯通内部充当“间谍”，以便掌控该企业的内部信息，作者拒绝，被监禁；</li><li>陆续有其他阿尔斯通的高管被逮捕，公司CEO惶恐，希望摆脱诉讼和入狱；</li><li>通用电气介入，摆出了可以帮助阿尔斯通摆脱当前困局的慈善面目，公司CEO为脱罪，达成秘密协议（推测，当事人都矢口否认，但作者强烈暗示这种行为），将阿尔斯通能源部门出售给通用电气（是该公司最有价值的资产）；</li><li>法国政府对这种行为保持暧昧态度，后来知晓，通用电气和阿尔斯通花费将近3亿欧元进行公关，还有一层原因，马克龙（现任法国总统，时任经济部长）相信阿尔斯通CEO与美国司法部、通用电气有交易，但是没有证据；同时受限于大西洋主义以及对美国强大实力的深深恐惧；</li><li>“肢解”交易达成，作者被判入狱30个月。</li></ol><p>从上述过程中，从头到尾，<strong>作者的角色和所处的地位，相较于整个事件，简直微不足道！所以，实际上他当了一个背锅侠和替罪羊！</strong></p><h1 id="4-带给我们的思考和启示"><a href="#4-带给我们的思考和启示" class="headerlink" title="4. 带给我们的思考和启示"></a>4. 带给我们的思考和启示</h1><p><strong>第一点</strong>，战争的形式已经发生变化了，从之前枪炮相加，血肉横飞到网络世界的攻防，可能并没有人死亡，但是损失可能更大。<strong>这本书中提到的有两点很值得注意，一个是舆论公关战，一个就是法律战。</strong></p><p>通用电气和阿尔斯通为促成收购，在媒体和网络上，大肆说明该收购带来的好处，比如增加就业岗位，帮助阿尔斯通摆脱诉讼，同时游说法国政界和企业界的精英。结果，一个曾经在戴高乐将军领导下致力于独立自主的法国居然在外人收购自己的核心资产时，居然沉默是“金”，简直可怕！想想我们这个社会，这样的人会在少数吗？</p><p>法律战，也即本事件的起因，违反《反海外腐败法》。这个法是个什么东西呢？简单来说，美国之外国家的公司，如果使用了与美国相关的工具，则全部收受到该法律的管辖。是不是，有点治外法权的意味呢！比如，在我们国家犯罪，却用美国的法律诉讼，是不是有点滑稽，这种域外法的行使，使得美国的“全球警察”地位坐实了。而且，还让人产生错觉，好像是我错了（虽然在本书中提到的阿尔斯通确实有腐败行为）！</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/6.png" alt="《反海外腐败法》"></p><p><strong>第二点</strong>，我想引述书中的一句话，</p><blockquote><p>我们不能上当受骗。不管谁当美国总统，无论他是民主党人还是共和党人，华盛顿都会维护少数工业巨头的利益</p></blockquote><p>有人曾经，觉得美国民主党上台，对我们可能会温和一点（可能法国人也是这么想的），还是放弃这种幻想吧！这是体制决定的，不是简单的人事变动能够改变的，美国政府和企业巨头的纠缠不断的联系导致“旋转门”的不断上演，所以我们还是做好准备吧！</p><p><strong>第三点</strong>，“人权”和“自由”这种过于抽象的东西，每个人都有自己的理解，比如说那个只喝矿泉水的归国人员，它的解释就为“特权”。<strong>而且每个人的理解都会倾向于自己的利益</strong>，比如作者在狱中的遭遇，美国的监狱的确是没有“人权”，你能相信监狱是私人的，所以当然是需要盈利的，“人权”多费钱啊，与资本主义的核心目标——攫取利益不符啊，当事情与他们的利益不符时，就会有这样的论调。当人权在敌对国家遭到威胁时，这时候美国这个道德楷模就出来了。</p><p><img src="/images/%E7%BE%8E%E5%9B%BD%E9%99%B7%E9%98%B1/7.png" alt="你品，你细品！"></p><p>第四点，爱国主义，这种东西在人们的脑海中是正面的东西，我也倾向于这样认为。但是有一点在书中提及，声称“自己生于斯，长于斯”的人，向大家彰显自己的爱国举动，真是是所谓的“爱国人士”吗，或许阿尔斯通CEO柏珂龙的话有助于我们思考，</p><blockquote><p>我是法国择优选拔人才体制的产物。借此机会，我想说，我的父母都是移民。自从在阿尔斯通任职后，我在法国创造了将近1.5万个就业岗位，对此我感到非常自豪。每个人都必须为法国就业做出一份贡献。我已经努力做出了微薄的贡献。</p></blockquote><p>不要忘记，就是他私下与通用电气密谋，肢解了自己服务的“工业明珠”！</p><p>在这本书中，还有很多值得人们思考的东西，包括友情、爱情、劳资关系、糖衣炮弹、爱国主义等等，建议大家去看一下，会收获不一样的东西！其中，对个人来说，最想提及的一点是，找一个好妻子真的是很重要的事情，尤其是像这位老哥遭遇监禁这样的遭遇时，更能体现这一点。</p><h1 id="5-Q-amp-A"><a href="#5-Q-amp-A" class="headerlink" title="5. Q&amp;A"></a>5. Q&amp;A</h1><p>看本书时，我一直不明白的有几个问题，后来弄明白了，在这里列出来同大家分享。</p><p><strong>1) 美国为啥可以管理国外的犯罪？怎么管？</strong></p><p>这与美国在国际上巨大的影响力和超强的实力有关，比如：做国际贸易需要美元结算，没有美元的话没办法进行交易，大家伙都不认，这也是我们一直推进人民币国际化的原因之一。因此，你只要涉及到美元交易，就归美国管，因为美元是美国发行的，具体怎么管，国际贸易会有清算，主要的清算中心在美国，通俗点讲，不听话的话，你就没办法收到货款，也没办法进口。这里，顺便提一句，你觉得Facebook的Libra能成功吗？</p><p>还有一点，除了中国外，世界主要的信息技术公司都是美国公司，所以只要你用了Gmail、Facebook等就都和美国有关系了。</p><p>其实，立什么法不重要，重要的是要有执行能力，这是美国厉害的地方。比如，有一天，你说全世界都得听我的，人家以为你有病呢，但是有人不听时，你有办法治人家，这时大家才服你，最起码表面上服你！</p><p><strong>2) 大西洋主义 VS 亲欧洲主义</strong></p><p>简单来讲，大西洋主义，就是西欧和北美互相合作，保护共同的安全和价值观，但是被认为是美国操纵欧洲的工具，尤其是对于盟友，美国的小动作也在不断的情况下。亲欧洲主义，强调欧洲一体化，摆脱美国的影响。</p>]]></content>
      
      
      <categories>
          
          <category> 读书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济 </tag>
            
            <tag> 政治 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
